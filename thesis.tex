% -*- latex -*- This is a LaTeX document.
% $Id: thesis.tex,v 1.70 1999-08-21 03:57:28 cananian Exp $
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt,titlepage,twoside]{article}
\usepackage[light,first,bottomafter]{draftcopy}
% fonts
\usepackage{beton}\usepackage{euler}
%\usepackage{pdffonts}\usepackage{euler}
% useful stuff
\usepackage[section,plain]{algorithm} % algorithm environment,\listofalgorithms
\usepackage{amsthm} % proof environment
\usepackage{amstext} % the \text command for math mode (replaces \mbox)
\usepackage{varioref}\vrefwarning % \vref command
% common definitions
\usepackage{comdef}
\newcommand*{\figscale}{1.0}

% Meet symbol
\newcommand{\meet}{\ensuremath{\sqcap}}

% number tables and figures according to section
\let\oldsection\section
\renewcommand{\section}{\setcounter{figure}{0}\setcounter{table}{0}\oldsection}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\renewcommand{\thetable}{\thesection.\arabic{table}}

% double-space
\linespread{1.2} %one-and-a-half spacing is 1.3

\title{Static Single Information Form\\
\small Or, chiseled on stone this'll last 4000 years}
\author{C. Scott Ananian}
\date{\today \\ $ $Revision: 1.70 $ $}

\begin{document}
\bibliographystyle{abbrv}

\pagestyle{empty}
\maketitle\cleardoublepage
\pagestyle{myheadings}\markboth{$ $Revision: 1.70 $ $}{$ $Revision: 1.70 $ $}
\pagenumbering{roman}
\tableofcontents\listoffigures\listoftables\listofalgorithms\cleardoublepage
\pagenumbering{arabic}

\section{Introduction}
The selection of a compiler internal respresentation from the many
littering the academic literature may at times seem a black art.  Each
IR is accompanied by papers trumpeting its superiority for one purpose
or another, and each IR feels compelled to introduce new terms and
structures into the jargon.  A pragmatic compiler writer will often
pick randomly from the plethora available, basing the decision on ease
of implementation or understandibility without realizing the extent to
which the IR influences the structure of the compiler.  Similarly, the
theorist will derive proofs based upon intermediate representations
(and languages) that are amenable to mathematical methods, without
regard to current implementation practices.%
\fixme{This paragraph will be revised to be less offensive, eventually.}

This paper introduces yet another intermediate
representation to the literature:  Static Single Information (SSI) form.
This IR is the core of the FLEX compiler project, which is primarily
investigating intelligent compilation techniques for distributed
systems.  This thesis, in presenting the IR,
attempts to keep both the mathematician and the programmer in mind.  
SSI form has both a rigorous mathematical semantics and a factored
form which aids efficient implementation of advanced analyses.
I believe that it best straddles the gap between dataflow-oriented,
graph-structured, and control-flow driven IRs, while maintaining the
sparsity needed to achieve practical efficiency.  The construction
algorithms are linear in the size of the program.

Our discussion of the Static Single Information form will be at times
tied to the source language of the FLEX compiler, Java.  Unlike many
abstract IRs, the choices made in the design of SSI form have been
dictated by the necessities of compiling a real-world imperative
language.  Java, however, has several theoretical properties that make
mathematical analysis more tractable.  In particular, we
will mention here Java's strict constraints on pointer variables.
Pointers in C can be abused in many ways that Java disallows.

Ultimately, the choice of compiler internal representation is fundamental.
Advances in IRs translate into advances in compilers.  SSI form
represents a clean and simple unification of many extant ideas, and
our hope is that it will allow the FLEX compiler to achieve a similar
integration of practical implementation and mathematical elegance.

\section{Context and goals}
Strong et al.\ \cite{strong58}\footnote{Attribution by 
Aho \cite{aho88:dragon}.} first advocated the use of compiler
intermediate representations in a 1958 committee report.  Their
idealistic ``universal intermediate language'' was called UNCOL.
Thirty years later, the Static Single Assignment (SSA) form was
introduced by  Alpern, Rosen, Wegman and Zadeck
as a tool for efficient optimization in a pair of POPL
papers \cite{alpern88:ssa,rosen88:gvn}, and three years after that Cytron
and Ferrante joined Rosen, Wegman, and Zadeck in explaining how to
compute SSA form efficiently in what has since become the 
``canonical'' SSA paper \cite{cytron89:ssa}.  Johnson and Pingali
\cite{johnson93:dfg} trace the development of SSA form back to Shapiro
and Saint in \cite{shapiro70:ssa}, while Havlak \cite{havlak94:isa}
views \phifunction{s} as descendants of the ``birthpoints'' introduced
in \cite{reif81:sym}.

Despite industry adoption of SSA form in production compilers
\cite{chow97:ssapre,chow96:hssa}, academic research into alternative
representations continues.
Recent proposals have included Value Dependence Graphs
\cite{weise94:vdg}, Program Dependence Webs \cite{ballance90:pdw},
the Program Structure Tree \cite{johnson94:pst},
DJ graphs \cite{sreedhar96:dj}, and Depedence Flow Graphs
\cite{johnson93:dfg}.

In comparison to these representations, the dominant characteristics of
our Static Single Information form may be summarized as follows:
\begin{itemize}
\item It names information units.
\item It is complete.
\item It is simple.
\item It is efficient.
\item It has no explicit control dependencies.
\item It allows both forward and reverse dataflow analyses.
\end{itemize}
SSI form is used as an IR for the FLEX compiler for the Java
programming language, which informs some of these design decisions.
The FLEX compiler does deep analysis and will support
hardware/software co-design.  SSI addresses these needs, concentrating
on analysis rather than optimization.  We will address each design
point in turn.

\textbf{It names information units.}  SSA form (which we will describe
further in section \ref{sec:ssa}) assigns unique names to unique \emph{static
values} of a variable.  However, it ignores the value information
which may be added to a variable at program branch points.  SSI form
renames at branch points which allows us to associate unique names
with unique \emph{information} about static values.  For example, a
program may test the value of an integer against zero before using it
as a divisor.  After the branch on the tested predicate, it is
possible to make statements about values (regarding equality or
inequality to zero) which were impossible to make previously.  SSI
form allows us to take advantage of this additional information.

\textbf{It is complete.}\label{sec:complete}
By this we mean that there exists an
executable semantics for the IR that does not require the use of
information external to the IR.  The original SSA form---and most
derivatives---require use of the original program control flow graph
during analysis, translation, or direct execution.  In fact,
\phifunction{s} are intimately tied with the precise input edge
structure of the control flow graph, and switch nodes (where control
flow splits) are undecipherable without referring to the control flow
graph.

In practice, this seems not a great disadvantage---it merely forces us to
maintain a mapping of SSA statements to nodes (equivalently, basic
blocks) of the original control flow graph.  But maintaining this
correspondence complicates editing the IR.  Also, it complicates the
interpretation of the program as a set of simultaneous equations,
which SSI form will allow us to do.  Finally, explicit control flow
may limit the available parallelism of the program.

\ssiplus, as it will be presented in section \ref{sec:ssiplus},
overcomes these difficulties and presents a \emph{complete}
representation of program meaning as a set of simultaneous equations,
without resort to graph information.

\textbf{It is simple.}  A bestiary of new $\phi$-like functions have
been introduced in the past decade, including
$\mu$-, $\gamma$-, and $\eta$-functions in \cite{ballance90:pdw,tu95:gssa},
$\psi$- and $\pi$-functions in \cite{lee99:parssa},
interprocedural \phifunction{s} in \cite{liao99:issa},
$\mu$- and $\chi$-functions in \cite{chow96:hssa},
$\mu$- and $\eta$-functions in \cite{gerlek95:inductssa},\footnote{Compare to
\cite{ballance90:pdw,tu95:gssa}.} and $\Lambda$-functions in \cite{lo98:ssu},
among others.\footnote{Maybe.
Actually I've named all the ones I know about.  Except for TGSSA,
an early form of which apparently used $\mu$- and $\nu$-functions (mentioned in
\cite{weise94:vdg}).}
Some of these are orthogonal to our work---the techniques of
\cite{lee99:parssa} can be used to extend SSI form to explicitly
parallel source languages, and those of \cite{chow96:hssa} to
languages with local variable aliasing (absent in Java).  Our goal is
to achieve minimal conceptual complexity in SSI form; that is, to
introduce the minimum set of $\phi$-like functions necessary to
represent the ``interesting'' properties of the compiled program.

\textbf{It is efficient.}  Construction of SSI form should be fast,
and space requirements should be reasonable.  The original SSA
algorithms required $O(E+V_{SSA}|{DF}|+NV_{SSA})$ time.  This bound
was dominated by the time and space required to construct the
dominance frontier, as $|{DF}|$, the size of the dominance frontier,
could be $O(N^2)$ for common cases.  Taking the dominant term, we will
abbreviate the time complexity of the Cytron's SSA-construction
algorithm as $O(N^2 V)$.

Our algorithms do not require the construction of a dominance
frontier---building on recent work on efficient SSA construction in
this regard---and run in so-called ``linear'' time: $O(EV)$.  A more
detailed analysis will be given later, but suffice for now to say that
close attention has been paid to the run-time of our construction and
analysis algorithms.\footnote{Dhamdhere \cite{dhamdhere92:large} quite
correctly states that Cytron's original algorithm has a worst-case
time bound of $O(N^3)$.  This is also true for our algorithms.
However, these worst-case time bounds are not tight; we will present
experimental evidence that run times on real programs are $O(N)$.}
       
%Construction should be fast, and
%space should be reasonable.\footnote{Replace with actual numbers.}

\textbf{All explicit control dependencies are eliminated.}
Some researchers (including \cite{appel:modern} and
\cite{pingali97:apt}) view control-dependence as a fundamental
property of the CFG, and \cite{ballance90:pdw,appel:modern} suggest
that accurate knowledge of control-dependence relations is the sole
key to automatic parallelization.  Often, incomplete intermediate
representations\footnote{See page \pageref{sec:complete} for our
definition of ``completeness'' in an IR.} are augmented with
control-dependence edges to express proper program semantics---see
\cite{johnson93:dfg} on DFGs and \cite{weise94:vdg} on VDGs, for
example.

Unfortunately, explicit control-flow edges tend to serialize
computation more than strictly necessary.  Figure~\vref{fig:ctrldep},
for example, contains two parallel loops which would be serialized by
the explicit control dependency between them.  Prior work often
focused on fine-grain intra-loop parallelism and ignored this coarser
inter-loop parallelism.\footnote{We discuss the dataflow-architecture
work of Traub \cite{traub86:ttda} in particular in section
\ref{sec:ssiplus}.} Our objective in this work is to fully utilize
coarse parallelism by removing source-language control-dependency artifacts.

\textbf{It is efficient for both forward and backward dataflow analyses.}
It is often observed that traditional SSA form cannot handle backward
dataflow analysis.  Johnson and Pingali note this, and suggest
\emph{anticipatability} as an example of a backwards dataflow analysis
where their dependence flow graph representation betters SSA form
\cite{johnson93:dfg}. Lo et al.\ suggest the use of an ``SSU'' form to
address much the same issue \cite{lo98:ssu}.  There are in fact many
analyses where both use and definition information is utilized, and
where dataflow in both forward and reverse directions occurs.  SSI
form is able to handle both of these cases, as we demonstrate in
section \ref{sec:bidirectional}.

\section{Static Single Assignment form}\label{sec:ssa}
Static Single Information (SSI) form derives many features from Static
Single Assignment (SSA) form, as described by Cytron in
\cite{cytron89:ssa}.  We will review SSA form to provide context for
our definition of SSI form in section~\ref{sec:ssi}.

\subsection{Definition of SSA form}

\textbf{Define SSA form here.}\fixme{Write this.}

Figure~\vref{fig:tossa} illustrates the conversion to SSA form.
\begin{myfigure}
\begin{center}
\input{Figures/THex1base} \vline\ \input{Figures/THex1ssa}
\end{center}
\caption
[A simple program and its single assignment version.]
{A simple program (left) and its single assignment version (right).
\label{fig:tossa}}
\end{myfigure}

\subsection{Minimal and pruned SSA forms}
Cytron defines a minimal SSA form in \cite{cytron91:ssa}.  His minimal
form is defined to use the smallest number of \phifunction{s} such
that the following three conditions hold:
\begin{enumerate}
\item If two nonnull paths $X \pathplus Z$ and $Y \pathplus Z$
converge at a node $Z$, and nodes $X$ and $Y$ contain assignments to
[a variable] $V$ (in the original program), then a trivial
\phifunction{} $V \leftarrow \phi(V, \ldots, V)$ has been inserted at
$Z$ (in the new program). \label{criteria1}
\item Each mention of $V$ in the original program or in an inserted
\phifunction{} has been replaced by a mention of a new variable $V_i$,
leaving the new program in SSA form.
\item Along any control flow path, consider any use of a variable $V$
(in the original program) and the corresponding use of $V_i$ (in the
new program).  Then $V$ and $V_i$ have the same value.
\end{enumerate}
Of these criteria, the first %\ref{criteria1}
is the most important.
%The other two can be summarized by noting that in proper SSA form,
%there is exactly one reaching definition for a variable $V$ at every
%non-$\phi$ use of $V$.
The SSA form in figure~\vref{fig:tossa} is minimal.

A variation
on minimal SSA form, called \emph{pruned} form \cite{ferrante91:pruned},
avoids placing \phifunction{s} which define variables which are never used.
In most cases, the more regular properties of minimal SSA form
outweigh the pruned form's slight increase in space efficiency.
Figure~\vref{fig:prunedssa} compares minimal and pruned SSA form for
our example program.\fixme{Mention \cite{ferrante91:pruned}, which
contains definition and algorithm for pruned SSA.}
\begin{myfigure}
\begin{center}
\input{Figures/THex1ssa} \vline\ \input{Figures/THex1ssaPr}
\end{center}
\caption[Minimal and pruned SSA forms.]
{Minimal (left) and pruned (right) SSA forms.}
\label{fig:prunedssa}
\end{myfigure}

\section{Static Single Information form}\label{sec:ssi}

SSI form extends SSA form to achieve symmetry for both forward and
reverse dataflow.   SSI form recognizes that information about
variables is generated at branches and generates new names at these
points.  This provides us with a one-to-one mapping between variable
names and information about those variables which is independent of
control-flow graph context.  Analyses can then associate information
with variable names and propagate this information efficiently and
directly both with and against the dataflow direction.

\subsection{Definition of SSI form}
Building SSI form involves adding pseudo-assignments for a variable $V$:
\begin{enumerate}
\item[$(\phi)$] at a control-flow merge when disjoint paths from a
conditional branch come together and at least one of the paths
contains a definition of $V$; and
\item[$(\sigma)$] at locations where control-flow splits and at least
one of the disjoint paths from the split uses the value of $V$.
\end{enumerate}

Figure~\vref{fig:tossi} compares the SSA and SSI forms for our example program.
% add text here about how SSI is better.
\begin{myfigure}
\begin{center}
\input{Figures/THex1ssa} \vline\ \input{Figures/THex1ssi}
\end{center}
\caption[A comparison of SSA and SSI forms.]
{A comparison of SSA (left) and SSI (right) forms.}
\label{fig:tossi}
\end{myfigure}

\subsection{Minimal and pruned SSI forms}
\emph{Minimal} and \emph{pruned} SSI forms can be defined which
parallel their SSA counterparts.  \emph{Minimal} SSI form would have the
smallest number of \phisigfunction{s} such that:
\begin{enumerate}
\item If two nonnull paths $X \pathplus Z$ and $Y \pathplus Z$
exist having only the node $Z$ where they converge in common,
and nodes $X$ and $Y$ contain either assignments to a variable $V$ in the
original program or a \phisigfunction[or] for $V$ in the new program,
then a \phifunction{} for $V$ has been inserted at $Z$ in the new program.
\item If two nonnull paths $Z \pathplus X$ and $Z \pathplus Y$
exist having only the node $Z$ where they diverge in common,
and nodes $X$ and $Y$ contain either uses of a variable $V$ in the
original program or a \phisigfunction[or] for $V$ in the new program,
then a \sigfunction{} for $V$ has been inserted at $Z$ in the new program.
\item There is exactly one reaching definition of $V$ at every
non-$\phi$ use of $V$ in the new program.
\item \textbf{FIXME: what's the equivalent condition for \sigfunction{s}?}
% has something to do with cycle-equivalence.
\end{enumerate}

A \emph{pruned} SSI form would be the minimal form without any unused
definitions, that is, \phisigfunction[or]{s} after which there are no
subsequent uses of any of the variables defined on the left-hand side,
except in other \phisigfunction[or]{s}.\footnote{An even more
compact SSI form may be produced by removing \sigfunction{s} for which
there are uses for \emph{exactly one} of the variables on the
left-hand side, but it does not seem profitable to split this
particular hair.}

Figure~\vref{fig:prunedssi} compares minimal and pruned SSI form for
our example program.
\begin{myfigure}
\begin{center}
\input{Figures/THex1ssi} \vline\ \input{Figures/THex1ssiPr}
\end{center}
\caption[Minimal and pruned SSI forms.]
{Minimal (left) and pruned (right) SSI forms.}
\label{fig:prunedssi}
\end{myfigure}

\subsection{Fast construction of SSI form}
Our construction algorithm begins with a program structure tree of
single-entry single-exit (SESE) regions, constructed as described by
Johnson, Pearson, and Pingali \cite{johnson94:pst}.  We will review
the algorithms involved, as their published descriptions
\cite{johnson93:sese} contain a number of errors.

We begin with a few definitions from \cite{johnson94:pst}.
\begin{definition}
Edges $a$ and $b$ are said to be \newterm{edge cycle-equivalent} in a
graph iff every cycle containing $a$ contains $b$, and vice-versa.
Similarly, two nodes are said to be \newterm{node cycle-equivalent} iff
every cycle containing one of the nodes also contains the other.
\end{definition}
\begin{definition}
A \newterm{SESE region} in a graph $G$ is an ordered edge pair
$\tuple{a,b}$ of distinct control flow edges $a$ and $b$ where
\begin{tightenum}
\item $a$ dominates $b$,
\item $b$ postdominates $a$, and
\item every cycle containing $a$ also contains $b$ and vice-versa.
\end{tightenum}
Edges $a$ and $b$ are called the \newterm{entry} and \newterm{exit} edges,
respectively.
\end{definition}
\begin{definition}
A SESE region $\tuple{a,b}$ is \newterm{canonical} provided
\begin{tightenum}
\item $b$ dominates $b'$ for any SESE region $\tuple{a,b'}$, and
\item $a$ postdominates $a'$ for any SESE region $\tuple{a',b}$.
\end{tightenum}
\end{definition}

We will give time bounds in terms of $N$ and $E$, the number of nodes
and edges of the control-flow graph, respectively.
Placement of \phisigfunction{s}
is also dependent on $V$, the number of variables in the program.
Since SSI renaming increases the number of variables, we will use
$V_0$ and $V_{SSI}$ to indicate the number of variables in the
original program and SSI form, respectively.

Note that $V$ is $O(N)$ at most, since our representation only allows
a constant number of variable definitions per node.  Typically $V_0$
will be much smaller than $N$, but $V_{SSI}$ need not be.  Also $E$
may be as large as $O(N^2)$, but in most control-flow graphs is $O(N)$
instead, as node arities are typically limited by a constant.

\subsubsection{Cycle-equivalency}
\newcommand{\cyceq}{\equiv_{cq}}%{\equiv_{CYC}}
The identification of SESE regions begins by computing the
cycle-equivalency of the edges in the program control flow graph.  The
cycle-equivalency algorithm works on undirected graphs, so we prepare the
directed control flow graph $G$ as follows:
\begin{enumerate}
\item \textbf{Add an edge from END to START in $\mathbf{G}$.} It is common
practice to add an edge from START to END in order to root the control
dependence graph at START \cite{cytron89:ssa}.  However in this case
the edge serves to make the control flow graph into a single strongly
connected component, and for this reason the direction of the edge is
from END to START.
\item \textbf{Create an equivalent undirected graph.}  Johnson et al.\
prove that the node expansion illustrated in figure~\vref{fig:CQundir}
results in an undirected graph with the same cycle-equivalency
properties as the original directed graph.  More precisely, nodes $a$
and $b$ in directed graph $G$ are cycle-equivalent if and only if
nodes $a'$ and $b'$ are cycle-equivalent in transformed undirected
graph $G'$.  The nodes $n_i$ and $n_o$ generated by the expansion are
termed \emph{not representative}; the node $n'$ in $G'$ is said to be
\emph{representative} of node $n$ in $G$.  Obviously, this
correspondence must be recorded during the transformation so we may
properly attribute the cycle-equivalency properties of $n'$ to $n$
later.
\begin{myfigure}
\begin{center}
\renewcommand*{\figscale}{0.5}\input{Figures/THundir}
\end{center}
\caption{Transformation from directed to undirected graph
	 (from \cite{johnson93:sese}).}
\label{fig:CQundir}
\end{myfigure}
\item \textbf{Perform a pre-order numbering of nodes in $\mathbf{G'}$.}
This is done with a simple depth-first search of $G'$.  When we visit
a node $a_i$ or $a_o$, we prefer to visit $a'$ before any other
neighbor.  This ensures that representative nodes are interior nodes
in the DFS spanning tree. The START node is numbered 0, and succeeding
nodes in the traversal get increasing numbers.  Thus low-numbered
nodes are closest to START and we will call them ``highest'' in the
DFS spanning tree.
\end{enumerate}

\begin{myfigure}\small\input{Figures/THcqdata}
\caption{Datatypes and operations for the cycle-equivalency algorithm.}
\label{fig:CQdata}\end{myfigure}

\begin{myfigure}\small\linespread{0.75}\input{Figures/THcqalg}
\caption{The cycle-equivalency algorithm
	 (corrected from \cite{johnson93:sese}).}
\label{fig:CQalg}\end{myfigure}

The above steps form an undirected graph $G'$ from the control-flow
graph $G$.  The remainder of the cycle-equivalency algorithm is
presented in figure~\vref{fig:CQalg}, with the above procedure
corresponding to the statement \code{G':=Preprocess(G)}.  The
algorithm has been corrected from the published version in
\cite{johnson93:sese}; in addition it has been extended to compute
both node and edge equivalencies (in effect, merging the algorithm of
\cite{johnson94:pst}).  Lines modified from the presentation in
\cite{johnson93:sese} are indicated in the figure with a vertical bar
in the left margin.  The datatype \code{BracketList} and the node
and edge properties used in the algorithm are described in
figure~\vref{fig:CQdata}.  The interested reader is encouraged to consult
\cite{johnson93:sese} for additional detail on these data structures
and representations.%
\fixme{Um, change to compute \emph{edge}
equivalency as well as \emph{node} equivalency.  Merge algorithms from
\cite{johnson93:sese} and \cite{johnson94:pst}.}
Figure~\vref{fig:CQex} shows cycle-equivalent regions in a simple
control-flow graph.  We use the notation
$\tuple{a,b}\cyceq\tuple{c,d}$ to indicate that the CFG edge from node
$a$ to node $b$ is edge cycle-equivalent to the edge from node $c$ to
node $d$.

\begin{myfigure}\centering
\vertcenter{\input{Figures/THcqex2}}
$\begin{array}[c]{cc}
\tuple{\text{START},1}\cyceq\tuple{16,\text{END}}\\
\tuple{1,2}\cyceq\tuple{8,16}\\
\tuple{2,3}\cyceq\tuple{3,4}\cyceq\tuple{7,8}\\
\tuple{4,5}\cyceq\tuple{5,7}\\
\tuple{4,6}\cyceq\tuple{6,7}\\
\tuple{1,9}\cyceq\tuple{9,10}\cyceq\tuple{14,15}\cyceq\tuple{15,16}\\
\tuple{10,11}\cyceq\tuple{11,13}\\
\end{array}$
\caption{Control flow graph and cycle-equivalent edges.}
\label{fig:CQex}\end{myfigure}

Calculating cycle-equivalent regions is based on a single reverse
depth-first traversal of $G$, so as long as all datatype operations in
figure~\ref{fig:CQdata} can be completed in constant time (and
\cite{johnson93:sese} shows how to do so), this computation is $O(E)$.

\subsubsection{SESE regions and the program structure tree}
Johnson, Pearson, and Pingali show how to construct a tree structure
of nested SESE regions from the cycle-equivalency information in
\cite{johnson94:pst}.  The cycle-equivalent regions are sorted by
dominance using a simple depth-first traversal of the graph, and then
canonical SESE regions are found by taking adjacent pairs of
edges from the cycle-equivalence classes.  Another depth-first search
of the CFG suffices to obtain to nesting of these regions,
which is represented in a data structure called the 
\emph{program structure tree}.
The algorithm and data structures required are presented in figures
\ref{fig:SESEdata} and \ref{fig:SESEalg}.  Figure~\vref{fig:SESEex}
shows the SESE regions on the left and program structure tree on
the right for the example of figure~\vref{fig:CQex}.%
\footnote{In addition, the regions ${c,d,e}$ and ${f,g}$ are
\emph{sequentially composed} \cite{johnson94:pst}.  Does this matter?}

\begin{myfigure}\small\input{Figures/THsesedata}
\caption{Datatypes and operations used in construction of the PST.}
\label{fig:SESEdata}\end{myfigure}

\begin{myfigure}\small\linespread{0.75}\input{Figures/THsesealg}
\caption{Computing nested SESE regions and the PST.}
\label{fig:SESEalg}\end{myfigure}

\begin{myfigure}\centering
\vertcenter{\renewcommand*{\figscale}{0.5}\input{Figures/THcqex}}\hspace{1cm}
\vertcenter{\renewcommand*{\figscale}{0.7}\input{Figures/THpst}}
\caption{SESE regions and PST for the CFG of
         figure~\ref{fig:CQex} (from \cite{johnson94:pst}).}
\label{fig:SESEex}\end{myfigure}

The time complexity for constructing the PST is easily seen to be
$O(E)$. The algorithm presented in figure~\vref{fig:SESEalg} begins
with a depth first traversal of $G$ to construct an ordered edge list
for each cycle-equivalent region; the traversal is $O(E)$ and the
list-append operation can be done in constant time.  We then iterate
through the cycle-equivalence classes and the edge lists of each
constructing SESE regions.  No edge can be on more than one list, so
this step is $O(E)$.  Finally, we do a final $O(E)$ depth-first
traversal of $G$, performing the constant-time operations {\tt append}
and {\tt LinkRegion}.  All steps are $O(E)$ and their sequential
composition is also $O(E)$.

\subsubsection{Placing \phisigfunction{s}}
As with the presentation of SSA form in \cite{cytron91:ssa}, we split
construction of SSI form into two parts: placing \phisigfunction{s}
and renaming variables.  The placement algorithm runs in $O(N V_0)$
time, and is presented in figure~\vref{fig:SSIalg}.  No new node
properties or datatypes are required; however, it is parameterized on
a function called \code{MaybeLive}.  The purpose of this function is
to generate pruned or minimal SSI form by suppressing unnecessary
\phisigfunction{s}.  However, it need not be precise.  If
\code{MaybeLive} returns a conservative value, the generated SSI
form will have too many \phisigfunction{s}.  This can be
post-processed to remove the excess; section \ref{sec:unusedcode} will
discuss this further.\footnote{Note that equivalent results could be
obtained by adding a \phifunction{} for every variable at every merge
and a \sigfunction{} for every variable at every split, and
post-processing.  In fact the same time bounds ($O(N V_0)$) would be
obtained.  There is a large practical difference in actual runtime,
however, which motivates our choice of approach.}  The remainder of
this section will be devoted to a correctness proof of the algorithm
of figure~\ref{fig:SSIalg}.

\begin{myalgorithm}\small%\linespread{0.75}
\input{Figures/THssialg}
\caption{Placing \phisigfunction{s}.}\label{fig:SSIalg}
\end{myalgorithm}

\begin{lemma}
If a \phifunction{} (\sigfunction{}) for a variable $v$ is needed in
an SESE region, there is at least one definition (use) of $v$ within
the SESE.
\end{lemma}
\begin{proof}
By definition X a \phifunction{} for $v$ is needed at some node $Z$
iff there exist paths $X \pathplus Z$ and $Y \pathplus Z$ having no
nodes but $Z$ in common where $X$ and $Y$ contain definitions for $v$
(where a \phisigfunction[or] for $v$ is also considered a definition).
Choose any such paths.  Let us assume that both $X$ and $Y$ are
outside the SESE containing $Z$.  As there is only one entrance edge
into the SESE, the paths $X \pathplus Z$ and $Y \pathplus Z$ must
contain some node in common other than $Z$.  But this contradicts our
choice of $X$ and $Y$.  Therefore at least one of $X$ and $Y$ must be
inside the SESE.

Likewise, by definition Y a \sigfunction{} for $v$ is needed at some
node $Z$ iff there exist paths $Z \pathplus X$ and $Z \pathplus Y$
having no nodes but $Z$ in commong where $X$ and $Y$ contain uses of
$v$ (where a \phisigfunction[or] for $v$ is also considered a use).
By argument paralleling the above, at least one of $X$ and $Y$ must be
in the SESE containing $Z$ because there is only one exit edge.
\end{proof}

More stuff here.

\begin{theorem}
The algorithm in figure~\ref{fig:SSIalg} is correct.
\end{theorem}
\begin{proof}
Direct from the preceding lemmas.
\end{proof}

My algorithm is correct.\fixme{Prove this.}
Rough sketch:
First theorem: a merge for v must be in the same SESE region as a
definition of v.
Proof: merge at first point in common between two defs of v and a use
of v (Iterate). If both defs outside the region, then must join
outside, as there is only one entry edge to the SESE region.  If both
defs inside a nested region, then must join inside, as only one exit
edge from nested region.  So merge must be in same region as def--but
one or both defs can be in a nested region of this region.  Hence we
can skip regions that contain no defs of v. Also collapsing nested
regions to single nodes does not mess with the path.  Generalize to
sigmas, too.

Iterating, a merge node which becomes a def requiring another merge is
in what relation to the new merge?  If other def is also in this
region, then new merge must be, too, because only one exit node.
New merge can't be in child region, as only one entrance?

Want to show that algorithm produced exactly minimal form or exactly
pruned form.  Need to define minimal/pruned forms more closely, first.

\subsubsection{Computing liveness}
We can compute pruned SSI form if we have access to liveness
information.
We get this for free in our compiler framework.  There's also an
$O(E+N^2)$ algorithm.  I hope to do better using the properties of the
PST.  I want an $O(E V_0)$ algorithm, which means solving the
one-variable case in $O(E)$.  Argh.
\fixme{Either do this or punt it.  Our infrastructure actually gives
us decent liveness info straight from the stack state during translation, so we
don't need a fast liveness algorithm; we get what we need `for free'.}

\subsubsection{Variable renaming}
Um, my current (SSA-inspired) algorithm requires dominance
information, which we've done away with in general.  Need to rewrite
to use the PST structure instead, which I'm pretty sure I can do.
I have to prove that the results are correct, though, which may be harder.%
\fixme{Do this.}

\subsubsection{Pruning SSI form}\label{sec:unusedcode}
The SSI algorithm can be run using any conservative approximation to
the liveness information
(including the function $\text{\bf Live}(v)=\text{\tt true}$) if
unused code elimination%
\footnote{We follow \cite{wegman91:scc} in distinguishing
\emph{unreachable code elimination}, which removes code that can never
be executed, from \emph{unused code elimination}, which deletes
sections of code whose results are never used.  Both are often called
``dead code elimination'' in the literature.}  is performed to remove
the extra \phisigfunction{s} added.  Figures \ref{fig:deaddata} and
\ref{fig:deadalg} present an algorithm to identify unused code in
$O(N V_{SSI})$ time, after which a simple $O(N)$ pass suffices to remove it.
The complexity analysis is simple: nodes and variables are visited at
most once, raising their value in the analysis lattive from
\emph{unused} to \emph{used}.  Nodes marked \emph{used} are never
visted.  So \code{MarkNodeUseful} is invoked at most $N$ times, and
\code{MarkVarUseful} is invoked at most $V_{SSI}$ times.  Each call to
\code{MarkNodeUseful} examines each variable used by the node; a
node may use at most every variable, taking $O(N V_{SSI})$ time.  Each call
to \code{MarkVarUseful} examines at most one node (the single
definition node for the variable, if it exists) and in constant time
pushes at most one node on to the worklist for a total of $O(V_{SSI})$ time.
So the total run time of \code{FindUseful} is
$O(N V_{SSI}+V_{SSI})=O(N V_{SSI})$, and
in fact it is typically much less.\footnote{If the number of instruction
operands and \phisigfunction{} arities are limited by a
constant, we get a time bound of $O(N)$.}  The total runtime for SSI
placement and subsequent pruning, including the time to construct the
PST, is $O(EV_0 + NV_{SSI})$.

\begin{myfigure}\small
\input{Figures/THdeaddata}
\caption{Datatypes and operations used in unused code elimination.}
\label{fig:deaddata}
\end{myfigure}

\begin{myfigure}\small\linespread{0.75}
\input{Figures/THdeadalg}
\caption{Identifying unused code using SSI form.}
\label{fig:deadalg}
\end{myfigure}

\subsubsection{Discussion}
Note that our algorithm for placing \phisigfunction{s} in
SSI form is
\emph{pessimistic}; that is, we at first assume every node in the
control-flow graph with input arity larger than one requires a
\phifunction{} for every variable and every node with out-arity larger
than one requires a \sigfunction{} for every variable, and then use
the PST and liveness information to determine safe places to
\emph{omit} \phisigfunction[or]{s}.  Most SSA construction
algorithms, by contrast, are \emph{optimistic}; they assume no
\phisigfunction[or]{s} are needed and attempt to determine where
they are provably necessary.  In my experience, optimistic algorithms tend to
have poor time bounds because of the possibility of input graphs like
figure~\vref{fig:evil}.  Proving that all but two nodes require
\phisigfunction[and/or]{s} for the variable $a$ in this example seems to
inherently require $O(N)$ passes over the graph; each pass can prove
that \phisigfunction[or]{s} are required for only those nodes adjacent to
nodes tagged in the previous pass.  Starting with the circled node, the
\phisigfunction{s} spread one node left on each pass. On the other hand,
an pessimistic algorithm assumes the correct answer at the start, fails
to show that any \phisigfunction[or]{s} can be removed, and
terminates in one pass.\fixme{Are we \emph{sure} similar worst cases
don't exist for the pessimistic algorithm?}

\begin{myfigure}[t]
\centering\renewcommand*{\figscale}{0.25}\input{Figures/evil}
\caption{A worst-case CFG for ``optimistic'' algorithms.}
\label{fig:evil}
\end{myfigure}

\subsection{Space complexity of SSI form}
\begin{myfigure}%[t]
\input{Figures/phisig}
\caption{Statistical analysis to determine typical values for $V_{SSI}$.}
\label{fig:phisigdata}
\end{myfigure}
See figure \vref{fig:phisigdata}.%
\fixme{Import discussion from appendix here.}

\section{Uses and applications of SSI}
\textbf{Discuss predicated analysis, anticipatibility, PRE, etc.
	Also bitwidth analysis.}%
\fixme{Write this.
Use the techniques of \cite{kennedy98:strength} to do loop-invariant
code motion, strength reduction, and partial-redundancy-elimination
using SSI form?}

\subsection{Sparse Predicated Typed Constant (SPTC) Propagation}
Sparse Predicated Typed Constant Propagation is a powerful analysis tool which
derives its efficiency from SSI form.  It is built on Wegman and
Zadeck's Sparse Conditional Constant (SCC) algorithm
\cite{wegman91:scc} and removes unnecessary array-bounds and
null-pointer checks, computes variable types, and performs
floating-point- and string-constant-propagation in addition to the
integer constant propagation of standard SCC.\footnote{It also washes
the dishes and does the laundry.  It does not do windows$^{\text{TM}}$.}

We will describe this algorithm incrementally, beginning with the
standard SCC constant-propagation algorithm for review.
Wegman and Zadeck's algorithm operates on a program in SSA form; we will
call this SCC/SSA to differentiate it from SCC/SSI, using the SSI
form, which we will describe in section \ref{sec:sccssi}. Section
\vref{sec:bitwidth} will discuss an extension to SPTC which does
\newterm{bit-width analysis}.

\subsubsection{Wegman and Zadeck's SCC/SSA algorithm}
\begin{myfigure}
\centering\renewcommand*{\figscale}{0.5}\input{Figures/THlat1}
\caption{Three-level value lattice and two-level executability lattice for SCC.}
\label{fig:scclat1}
\end{myfigure}
\begin{table}\centering
$\begin{array}{|l|cccc|} \hline
\meet & \bot & c & d (\not= c) & \top \\ \hline
\bot  & \bot & c & d  & \top \\
c     &   c  & c &\top & \top \\
\top  & \top &\top&\top& \top \\ \hline
\end{array}%
\quad\quad%
\begin{array}{|l|ccc|}\hline
\oplus& \bot &   d  & \top \\ \hline
\bot  & \bot &   d  & \top \\
c     &   c  &c\oplus d& \top \\
\top  & \top & \top & \top \\ \hline
\end{array}$
\caption{Meet and binary operation rules for SCC.}
\label{tab:sccmeet1}
\end{table}
\begin{myalgorithm}\small
\input{Figures/THsccalg1}
\caption{SCC algorithm for SSA form.}\label{alg:scc}
\end{myalgorithm}
\begin{myalgorithm}\small
\input{Figures/THsccalg2}
\caption{SCC algorithm for SSA form, cont.}\label{alg:scc2}
\end{myalgorithm}
The SCC algorithm works on a simple three-level value lattice
associated with variable definition points and a two-level
executability lattice associated with flow-graph edges.  These
lattices are shown in figure \vref{fig:scclat1}.  Associating a lattice
value with a definition point is a conservative statement that, for
all possible program paths, the value of that variable has a certain
property.  The value lattice is, formally, $\domain{Int}_\bot^\top$;
the lattice value $\bot$ signifies that no information
about the value is known, the lattice value $\top$ indicates that it
is possible that the variable has more than one dynamic value, and the
other lattice entries (corresponding to integer constants and occuping
a flat space between $\top$ and $\bot$) indicate that the variable can
be proven to have a single constant value in all runs of the program.%
\footnote{Note that we follow the $\top$ and $\bot$ conventions used
in semantics and abstract interpretation; authors in dataflow analysis
(including Wegman and Zadeck in their SCC paper \cite{wegman91:scc})
often use contrary definitions, letting $\top$ mean undefined and
$\bot$ indicate overdefinition.  As section \ref{sec:semantics} will
discuss the semantics of \ssiplus at length, we thought it best to
adhere to one set of definitions consistently, instead of switching
mid-paper.}
Similarly, the executability lattice indicates whether it is possible
that the control flow edge is traversed in some execution of the
program (marked ``executable''), or if it can be proven that the edge
is never traversed in any valid program path (marked ``not
executable'').  The algorithm works with SSA form, and is presented
as algorithm \ref{alg:scc}.  Binary operations on lattice values and
combination at $\phi$-nodes follow the rules in
table~\ref{tab:sccmeet1}; notice that the meet operation ($\meet$) is
simply the least upper bound on the lattice.
The time complexity of SCC/SSA can be found
easily: the procedure \code{RaiseE} puts each node on the $W_n$
worklist at most once, and \code{RaiseV} puts a variable on the $W_v$
worklist at most $D-1$ times, where $D$ is the maximum lattice depth.
The \code{Visit} procedure can thus be invoked a maximum of $N$ times
by line~\ref{line:visitWn} of the \code{Analyze} procedure of
algorithm~\ref{alg:scc}, and a maximum of $U_{SSA}(D-1)$ times by
line~\ref{line:visitWv}, where $U_{SSA}$ is the number of variable
\newterm{uses} in the SSA representation of the program.  The lattice
depth $D$ is the constant 3 in this version of the algorithm, so it
drops out of the expression.  The \code{RaiseE} procedure itself is
called at most $E$ times.  The time complexity is thus
$O(E+N+U_{SSA}(D-1))$ which simplifies to $O(E+U_{SSA})$.

\subsubsection{SCC/SSI: predication using \sigfunction{s}.}\label{sec:sccssi}
\begin{myfigure}%
\begin{samplecode}[2]%
foo = f();        & \subvar{foo}{0} = f();\\
if (foo == 1)     & if (\subvar{foo}{0} == 1) \\
                  & $\tuple{\subvar{foo}{1},\subvar{foo}{2}}$ =
                    $  \sigma(\subvar{foo}{0})$ \\
\>bar = foo + 1;  & \>\subvar{bar}{0} = \subvar{foo}{2} + 1;\\
else              & else \\
\>bar = 2;        & \>\subvar{bar}{1} = 2;\\
                  & \subvar{bar}{2} =
                    $  \phi(\subvar{bar}{0},\subvar{bar}{1})$%\\
\end{samplecode}%
\caption{A simple constant-propagation example.}
\label{fig:ssa_vs_ssi}
\end{myfigure}
\begin{myalgorithm}\small
\input{Figures/THsccssi}
\caption{A revised \code{Visit} procedure for SCC/SSI.}\label{alg:sccssi}
\end{myalgorithm}
Porting the SCC algorithm from SSA to SSI form immediately increases
the number of constants we can find.  A simple example is shown in
figure \ref{fig:ssa_vs_ssi}: the version of the program on the right
is in SSI form, and SCC/SSI---unlike SCC/SSA---can
determine that \code{\subvar{foo}{2}} is a constant with value 1
(although nothing can be said about the value of
\code{\subvar{foo}{0}} or \code{\subvar{foo}{1}}) and therefore that
\code{\subvar{bar}{0}}, \code{\subvar{bar}{1}}, and
\code{\subvar{bar}{2}} are constants with the value 2.
SSI form creates a new name for \code{bar} at the conditional branch
to indicate that more information about its value is known.

Only the \code{Visit} procedure must be updated for SCC/SSI: lattice
update rules for \sigfunction{s} must be added.
Algorithm~\ref{alg:sccssi} shows a new \code{Visit} procedure for the
two-level integer constant lattice of Wegman and Zadeck's SCC/SSA;
with this restricted value set only integer equality tests tap the
algorithm's full power.  The utility of SCC/SSI's \newterm{predicated
analysis} will become more evident as the value lattice is extended to
cover more constant types.

The time complexity of the updated algorithm is identical to that of
SCC/SSA: $O(E+U_{SSA})$, by the same argument as before.

\subsubsection{Extending the value domain}
\begin{myfigure}
\centering\renewcommand*{\figscale}{0.5}\input{Figures/THlat2}
\caption{SCC value lattice extended to Java primitive value domain.}
\label{fig:scclat2}
\end{myfigure}
The first simple extension of the SCC value lattice enables us to
represent floating-point and other values.  For this work, we extended
the domain to cover the full type system of Java bytecode
\cite{gosling95:bytecode}; the extended lattice is presented in figure
\ref{fig:scclat2}.
The figure also introduces the abbreviated lattice notation we will use
through the following sections; it is understood that the
lattice entry labelled ``int'' stands for a finite-but-large set of
incomparable lattice elements, consisting (in this case) of the
members of the Java \code{int} integer type.
%\footnote{As the Java \code{int}
%type is a proper subset of the Java \code{long} integer type, we
%simplify our lattice by representing all integers as elements of
%\code{long}. Overflows are handled correctly after analysis.  Although
%the mechanics are slightly different, the same principle holds for
%the Java \code{double} and \code{float} types, and we similarly
%combine them in our lattice.}
Java \code{int}s are 32 bits long, so the ``int'' entry abbreviates
$2^{32}$ lattice elements.  Similarly, the ``double'' entry encodes not
the infinite domain of real numbers, but the domain spanned by the
Java \code{double} type which has fewer than $2^{64}$
members.\footnote{In IEEE-standard floating-point, some possible bit
patterns are not valid number encodings.}  The
Java \code{String} type is also included, to allow simple constant
string coalescing to be performed.  The propagation algorithm over
this lattice is a trivial modification to algorithm \ref{alg:sccssi}, and
will be omitted for brevity.  In the next sections, the ``int'' and ``long''
entries in this lattice will be summarized as ``Integer Constant'',
the ``float'' and ``double'' entries as ``Floating-point Constant'',
and the ``String'' entry as ``String Constant''.  As the lattice is
still only three levels deep, the asymptotic runtime complexity is
identical to that of the previous algorithm.

\subsubsection{Type analysis}
\begin{myfigure}[p]
\centering\renewcommand*{\figscale}{0.5}\input{Figures/THlat3}
\caption{SCC value lattice extended with type information.}
\label{fig:scclat3}
\end{myfigure}
\begin{myfigure}[p]
\centering\renewcommand*{\figscale}{0.5}\input{Figures/THlat4}
\caption{``Typed'' category of figure \ref{fig:scclat3} shown expanded.}
\label{fig:scclat4}
\end{myfigure}
\begin{myfigure}%
\begin{eqnarray*}
\code{int}\oplus\code{int}&=&\code{int}\\
\code{long}\oplus\{\code{int},\code{long}\}&=&\code{long}\\
\code{float}\oplus\{\code{int},\code{long},\code{float}\}&=&\code{float}\\
\code{double}\oplus\{\code{int},\code{long},\code{float},\code{double}\}&=&\code{double}\\
\code{String}\oplus\{\code{int},\code{long},\code{float},\code{double},\code{Object},\ldots\} &=& \code{String}
\end{eqnarray*}%
\caption{Java typing rules for binary operations.}
\label{fig:scc_typed_binop}
\end{myfigure}
\begin{table}%[t]
\begin{tabular}{|l|l|c|c|c|}\hline
Hierarchy & Source language & Classes & Avg. depth & Max. depth \\ \hline
FLEX infrastructure & Java  &   550   &    1.9     &     5      \\
\code{javac} compiler & Java&   304   &    2.8     &     7      \\
NeXTStep 3.2$^\dag$& Objective-C & 488 &   3.5     &     8      \\
Objectworks 4.1$^\dag$&Smalltalk & 774 &   4.4     &    10      \\ \hline
\end{tabular}
{\small$\dag$ indicates data obtained from Muthukrishnan and M\"uller
 \cite{muthukrishnan96:ch}.}
\caption{Class hierarchy statistics for several large O-O projects.}
\label{tab:chstats}
\end{table}
\begin{myalgorithm}\small
\input{Figures/THscctyped}
\caption{\code{Visit} procedure for typed SCC/SSI.}
\label{alg:scctyped}
\end{myalgorithm}
In figure \ref{fig:scclat3} we extend the lattice to compute Java type
information.  The new lattice entry marked ``Typed'' is actually
forest-structured as shown in figure \ref{fig:scclat4}; it is as deep
as the class hierarchy, and the roots and leaves are all comparable to
$\top$ and $\bot$.  Only the \code{Visit} procedure must be modified;
the new procedure is given as algorithm~\ref{alg:scctyped}.
Because the lattice $L$ is
deeper, the asymptotic runtime complexity is now $O(E+U_{SSA}D_c)$
where $D_c$ is the maximum depth of the class hierarchy.  
To form an estimate of the magnitude of $D_c$, table \ref{tab:chstats}
compares class hierarchy statistics for several large
object-oriented projects in various source languages. Our FLEX
compiler infrastructure, as a typical Java example, has an average
class depth of 1.91.\footnote{Measured August 2, 1999; the
infrastructure is under continuing development.}
In a forced example, of course, one can make the class depth $O(N)$;
however, one can infer from the data given that in real code the $D_c$
term is not likely to make the algorithm significantly non-linear.

A brief word on the roots of the hierarchy forest in figure
\ref{fig:scclat4} is called for: Java has both a class hierarchy,
rooted at \code{java.lang.Object}, and several primitive types, which
we will also use as roots.  The primitive types include
\code{int}, \code{long}, \code{float}, and
\code{double}.\footnote{In the type system our infrastructure uses
(which is borrowed from Java bytecode) the \code{char},
\code{boolean}, \code{short} and \code{byte} types are folded into
\code{int}.}  Integer constants in the lattice are comparable to and
less than the \code{int} or \code{long} type; floating-point constants
are likewise comparable to and less than either \code{float} or
\code{double}.  String constants are comparable to and less than the
\code{java.lang.String} non-primitive class type.

The \code{void} type, which is the type of the expression \code{null},
is also a primitive type in Java; however we wish to keep $x \meet y$
identical to $\bigsqcup_L\{x, y\}$ (the least upper bound of $x$ and
$y$) while satisfying the Java typing rule that $\code{null} \meet x = x$
when $x$ is a non-primitive type and not a constant.  This requires
putting \code{void} comparable to but less than every non-primitive
leaf in the class hierarchy lattice.\fixme{Not shown in the figure.}

The Java class hierarchy also includes \newterm{interfaces}, which are
the means by which Java implements multiple inheritance.  Base
interface classes (which do not extend other interfaces) are additional
roots in the hierarchy forest, although no examples of this are shown
in figure \ref{fig:scclat4}.

Since untypeable variables are generally forbidden, no operation
should ever raise a lattice value above ``Typed'' to $\top$.  The
otherwise-unnecessary $\top$ element is retained to indicate error
conditions.

Finally, note that the ability to represent \code{null} as the
\code{void} type in the lattice begins to allow us to address
null-pointer checks, although because $\code{null} \meet x =x$ for
non-primitive types we can only reason about variables which can be
proven to be null, not those which might be proven to be non-null
(which is the more useful case).  The next section will provide a more
satisfactory treatment.

\subsubsection{Addressing array-bounds and null-pointer checks}
\begin{myfigure}
\centering\renewcommand*{\figscale}{0.5}\input{Figures/THlat5}
\caption{The SPTC value lattice.}
\label{fig:scclat5}
\end{myfigure}
\begin{myfigure}
\[\begin{array}{l}
\forall C \in \text{Class},\:
  C_{\text{non-null}} \sqsubset C_{\text{possibly-null}}\\
\forall C \in \text{Class}_{\text{non-null}},\:
  \bigsqcup_L \{\code{void},C\} \in \text{Class}_{\text{possibly-null}}\\
\forall C \in \text{Class}_{\text{possibly-null}},\:
  \code{void} \sqsubset C\\
\forall C \in \text{Class}_{\text{non-null}},\:
  \tuple{\code{void},C}\notin\:\sqsubseteq\\
\end{array}\]%
Let $A(C, n)$ be a
function to turn a lattice entry representing a non-null array class
type $C$ into the lattice entry representing a said array class with
known integer constant length $n$.  Then for any non-null array class
$C$ and integers $i$ and $j$,
\[\begin{array}{l}
A(C, i) \sqsubset C\\
\tuple{A(C, i), A(C, j)} \in\:\sqsubseteq \text{ if and only if } i=j\\
\end{array}\]%
\caption{SPTC value lattice inequalities.}\label{fig:sptc_rules}
\end{myfigure}
\begin{myalgorithm}\small
\input{Figures/THsptc}
\caption{\code{Visit} procedure outline for SPTC.}
\label{alg:sptc}
\end{myalgorithm}
\begin{myfigure}
\begin{samplecode}
x = 5 + 6;\\
do \{\\
\>y = new int[x];\\
\>z = x-1;\\
\>if (0 <= z \&\& z < y.length)\\
\>\>y[z] = 0;\\
\>else\\
\>\>x--;\\
\} while (P);\\
\end{samplecode}
\caption{An example illustrating the power of combined analysis.}
\label{fig:combined}
\end{myfigure}
At this point, we can expand the value lattice a final time to allow
elimination of unnecessary array-bounds and null-pointer checks, based
on our constant-propagation algorithm.  The new lattice is shown in
figure \ref{fig:scclat5}; we have split the ``Typed'' lattice entry to
allow distinguishing between non-null and possibly-null
values,\footnote{Values which are always-null were discussed in the
previous section; they are identified as having primitive type \code{void}.}
and added a lattice level for arrays of known constant length.  
Some formal definition of the new value lattice can be found in
figure \ref{fig:sptc_rules}; the meet rule is still the least upper
bound on the lattice.  Modifications to the \code{Visit} procedure are
outlined in algorithm~\ref{alg:sptc}.
Notice that we exploit the pre-existing integer-constant propagation to
identify constant-length arrays, and that our integrated approach
allows one-pass optimization of the program in figure \ref{fig:combined}.

Note that the variable renaming performed by the SSI form at
control-flow splits is essential in allowing SPTC to do array-bounds
and null-pointer check elimination.  Some quantitative measure of the
utility of SPTC is given as table \ref{tab:sptc_numbers}.

\subsection{Bit-width analysis}\label{sec:bitwidth}
\begin{myfigure}[t]
\centering\renewcommand*{\figscale}{0.6}\input{Figures/THlattice}
\caption{Lattice for constant/type/bitwidth analysis.}
\label{fig:lattice}
\end{myfigure}

\section{An executable representation}\label{sec:ssiplus}
The Static Single Information (SSI) form, as presented in the first
half of this thesis,
requires control-flow graph information in order to be executable. We
would like to have a demand-driven operational semantics for SSI form
that does not require control-flow information; thus freeing us to
more flexibly reorder execution.

In particular, we would like a representation that eliminates
unnecessary control dependencies such as exist in the program of
figure~\vref{fig:ctrldep}.  A control-flow graph for this program, as
it is written, will explicitly specify that no assignments to
\code{B[]} will take place until all elements of \code{A[]} have
been assigned; that is, the second loop will be
\emph{control-dependent} on the first.  We would like to remove this
control dependence in order to provide greater parallelism---in this
case, to allow the assignments to \code{A[]} and \code{B[]} to
take place in parallel, if possible.\footnote{Note that Arvind's
dataflow compiler \cite{traub86:ttda} looked for exactly the opposite type of
parallelism.  By concentrating on intra-loop dependencies, as he did, you get
fine-grain parallelism suitable for VLIW-type machine with many
functional units.  We try to remove dependencies \emph{between} loops,
which gets us coarser parallelism that does not require as many
functional units to take advantage of.  This should be in the paper
body, but I'm not quite sure where a discussion of the differences
between Arvind's work and mine belongs.}

\begin{myfigure}[t]
\begin{samplecode}
for (int i=0; i<10; i++)\\
\>A[i] = x;\\
for (int j=0; j<10; j++)\\
\>B[j] = y;\\
\end{samplecode}
\caption[An example of unnecessary control dependence.]
{An example of unnecessary control dependence: the second loop
is \emph{control-dependent} on the first and so assignments to
\code{A[]} and \code{B[]} cannot take place in parallel.}
\label{fig:ctrldep}
\end{myfigure}

In addition, an executable representation allows us to apply the
techniques of abstract interpretation \cite{idunno}.  Although abstract
interpretation may be applied to the original SSI form using
information extracted from the control flow graph, as in \cite{foo}
and \cite{bar}, an executable SSI form allows more concise (and thus,
more easily derived and verified) interpretation
algorithms.\fixme{I'm still working on understanding abstract
interpretation techniques.  It may be that we can do stuff with
\ssiplus\ that is plain impossible with \ssizero\ (instead of merely
more difficult); if so, I'll mention that here, of course.
\textbf{ADDENDUM:} seems we might be able to \emph{prove} properties
of \ssiplus\ more easily than \ssizero; still working on this.}

The modifications outlined here will extend SSI form in order to
provide a useful and descriptive operational semantics.  We will call
the extended form \ssiplus.  For clarity, SSI form as originally
presented we will call \ssizero.  We will describe algorithms to
contruct \ssiplus{} efficiently,\fixme{I will make this more precise
as soon as I get the math done; construction should be $O(N)$, and space
$O(N^2)$, but $\Theta(N)$.} and illustrate analyses and
optimizations using the form.\fixme{maybe something here like: we
will also show how to apply the techniques of abstract interpretation
to \ssiplus?}

\subsection{Discussion}
\textbf{Why an executable representation is desirable.  Also,
deficiencies in \ssizero{} form.}

\textbf{A couple of paragraphs here on why an executable
representation is desirable.  Many some of the information above
should be moved down here.}

\textbf{A couple paragraphs describing deficiencies in \ssizero{} form.
Structure question: I'd like to describe the fixes \ssiplus{} proposes
right next to the bits saying what's broken in \ssizero{}; but the way
the outline is laid out currently, the fixes are described later.  I'm
going to try it the way the outline has it for now.}

\subsection{Definition}
\begin{itemize}
\item \textbf{\xifunction{s}.}
\item \textbf{Triggered constants and CALLs.}
\end{itemize}

\subsection{Semantics}\label{sec:semantics}
We will base the operational semantics of \ssiplus\ on a demand-driven
dataflow model.  We will define both a cycle-oriented semantics and an
event-driven semantics, which (incidentally) correspond to synchronous
and asynchronous hardware models.

Following the lead of Pingali \cite{pingali90:dfg}, we present Plotkin-style
semantics \cite{plotkin81:opsem} in which \emph{configurations} are
rewritten instead of programs.  The configurations represent program
state and transitions correspond to steps in program execution.  The
set of valid transitions is generated from the program text.

The semantics operate over a lifted value domain
$\domain{V}=\domain{Int}_\bot$. When some variable
$t = \bot_\domainsm{V}$ we say it is
\emph{undefined}; conversely $t\succ\bot_\domainsm{V}$ indicates that the
variable is \emph{defined}.  ``Store'' metavariables $S_x$ are not
explicitly handled by the semantics, but the extension is trivial with
an appropriate redefinition of the value domain $\domain{V}$.  Floating-point
and other types are also trivial extensions.  The
metavariables $c$ and $v$ stand for elements of $\domain{V}$.

We also define a domain of \emph{variable names},
$\domain{Nam}=\set{n_0,n_1,\ldots}$.  The metavariables $t$ and $P$ stand for
elements in $\domain{Nam}$, although $P$ will be reserved for naming branch predicates.

A fixed set of ``built-in'' operators, \textbf{op}, is defined,
of type $\domain{V}^* \to \domain{V}$.  If any operator argument is $\bot$, the
result is also $\bot$.  Constants are implemented as a special case of
the general operator rule: an \textbf{op} producing a constant has a
single input (which does not affect the output) called its \emph{trigger}.

\subsubsection{Cycle-oriented semantics}
\begin{myfigure}[t]
\begin{transitions}
t=\mathbf{op}(t_1,\ldots,t_n):
& \trule{\rho[t]=\bot \wedge 
         \left(
          \rho[t_1]\succ\bot \wedge \ldots \wedge \rho[t_n]\succ\bot
	 \right)}
	{\rho \to \rho[t \mapsto
		         \mathbf{op}(\rho[t_1],\ldots,\rho[t_n])] } \\

t=\phi(t_1,\ldots,t_n):
& \trule{\rho[t]=\bot \wedge
         \rho[t_j]\succ\bot \wedge
         \text{all other}\,\rho[t_1],\ldots,\rho[t_n]=\bot}
        {\rho \to \rho[t \mapsto \rho[t_j]] } \\

\tuple{t_1,\ldots,t_n}=\sigma(P,t):
& \myarray{r}{
  \trule{\rho[P]=v\succ\bot \wedge
         \rho[t_{v-1}]=\bot \wedge
         \rho[t]\succ\bot
         %\text{ where } (0\leq v\leq n-1)
	}
	{\rho \to \rho[t_{v-1} \mapsto \rho[t]] }
  \quad \hfill \\ \footnotesize \mbox{where } (0\leq v\leq n-1) }
 \\

\footnotesize % latex complains, but does the right thing.
\xivec{t_1,\ldots,t_n}{t_{n+1},\ldots,t_{m}}=\xi(\xivec{t'_1,\ldots,t'_n}{t'_{n+1},\ldots,t'_m}):
& \myarray{r}{
  \trule{\rho[t_j]=\bot \wedge
         \rho[t'_j]\succ\bot %\mbox{ where } (1\le j\le n)}
	}
        {\rho \to \rho[t_j \mapsto \rho[t'_j]] }
  \qquad\qquad\qquad \hfill \\ \footnotesize \mbox{where } (1\le j\le n)}
 \\

\footnotesize % latex complains, but does the right thing.
\xivec{t_1,\ldots,t_n}{t_{n+1},\ldots,t_{m}}=\xi(\xivec{t'_1,\ldots,t'_n}{t'_{n+1},\ldots,t'_m}):
& \footnotesize % latex complains, but does the right thing.
  \trule{\rho[t'_{n+1}]\succ\bot \wedge \ldots \wedge \rho[t'_m]\succ\bot}
        {\myarray{r @{} l}{
         \rho \to
%	 \hfill \\ \: % line-break
         \rho_\emptyset &
                [t_1 \mapsto \rho[t_1]]\ldots[t_n \mapsto \rho[t_n]]
		\hfill \\ & \quad % line-break
                [t_{n+1} \mapsto \rho[t'_{n+1}]]\ldots[t_m \mapsto \rho[t'_m]]
		} } \\
\end{transitions}
\caption{Cycle-oriented transition rules for \ssiplus.}
\label{fig:cyclesemantics}
\end{myfigure}

In the cycle-oriented semantics, configurations consist of an
\emph{environment}, $\rho$, which maps
names in $\domain{Nam}$ to values in $\domain{V}$.

\begin{definition}~\\*[-1\baselineskip]
\begin{enumerate}
\item An \emph{environment} $\rho: \domain{N} \to \domain{V}$ is a
finite function---its domain $\domain{N} \subseteq \domain{Nam}$ is
finite.  The notation $\rho[t\mapsto c]$ represents an environment
identical to $\rho$ except for name $t$ which is mapped to $c$.
\item The null environment $\rho_\emptyset$ maps every $t\in\domain{N}$ to
$\bot_\domainsm{V}$.
\item A \emph{configuration} consists of an environment.  The initial
configuration is $\rho_\emptyset$, that is, all names in $\domain{N}$
are mapped to $\bot_\domainsm{V}$.\fixme{START CONDITION.}
\end{enumerate}
\end{definition}

Figure~\vref{fig:cyclesemantics} shows the cycle-oriented transition
rules for \ssiplus\ form.  The left column consists of definitions and
the right column shows a precondition on top of the line, and a
transition below the line.  If the definition in the left column is
present in the \ssiplus\ form and the precondition on top of the line
is satisfied, then the transition shown below the line can be performed.

\textbf{EXPLAIN THE RATIONALE BEHIND THE RULES HERE.}

\subsubsection{Event-driven semantics}
\begin{myfigure}[t]\small
\begin{transitions}
t=\mathbf{op}(t_1,\ldots,t_n):
& \tuple{E[t_1=v_1]\ldots[t_n=v_n],S} \to \tuple{E[t=\mathbf{op}(v_1,\ldots,v_n)],S}\\

t=\phi(t_1,\ldots,t_n):
& \tuple{E[t_i=v],S} \to \tuple{E[t=v],S}\\

\tuple{t_1,\ldots,t_n}=\sigma(P,t):
& \tuple{E[t=v][P=i],S} \to \tuple{E[t_i=v],S}\\

\xivec{t_1,\ldots,t_n}{t_{n+1},\ldots,t_m}=\xi_K(\xivec{t'_1,\ldots,t'_n}{t'_{n+1},\ldots,t'_m}):
& \myarray{r}{
  \tuple{E[t'_i=v],S} \to \hfill\\\quad\quad\quad\quad\quad
  \tuple{E[t_i=v],S[K\mapsto S[K]\cup\tuple{t_i,v}]}\\
   \text{where }1\le i \le n\\
%  \mbox{where }K\mbox{ is a unique constant corresponding to}\\
%  \mbox{this \ssiplus\ statement}\\
  }\\

\xivec{t_1,\ldots,t_n}{t_{n+1},\ldots,t_m}=\xi_K(\xivec{t'_1,\ldots,t'_n}{t'_{n+1},\ldots,t'_m}):
& \trule{S[K]=\left\{\tuple{t_1,v_1},\ldots,\tuple{t_n,v_n}\right\}}
  {\myarray{r}{
   \tuple{E[t'_{n+1}=v_{n+1}]\ldots[t'_m=v_m],S} \to\quad\quad\quad\quad\\
   \tuple{E[t_1=v_1]\ldots[t_m=v_m],S}\\
%   \mbox{where }S[K]=\bigcup_{i=1}^n \{\tuple{t_i,v_i}\}\\
%   \mbox{where }S[K]=\left\{\tuple{t_1,v_1},\ldots,\tuple{t_n,v_n}\right\}\\
  } }
\end{transitions}
\caption[Event-driven transition rules for \ssiplus.]
{Event-driven transition rules for \ssiplus.  Note the
unfortunate synchronization in the last rule. $K$ is a
statement-identifier constant which is unique for each source \xifunction.}
\label{fig:eventsemantics}
\end{myfigure}

In the event-driven semantics, configurations consist of an
\emph{event set} and an \emph{invariant store}.  The event set
$E$ contains definitions of the form $t=c$,
and the invariant store is a mapping from numbered \xifunction{s} in
the source \ssiplus\ form to a set of tuples representing saved values
for loop invariants.

We define the following domains:
\begin{itemize}
\item $\domain{Evt} = \domain{Nam} \times \domain{V}$ is the event
domain.  An event consists of a name-value pair.  The metavariable $e$
stands for elements of $\domain{Evt}$.
\item $\domain{Xif} \subset \domain{Int}$ is used to number
\xifunction{s} in the source \ssiplus\ form.  There is some mapping
function which relates \xifunction{s} to unique elements of
$\domain{Xif}$.  The metavariable $K$ stands for an element in
$\domain{Xif}$.
\end{itemize}

A formal definition of our configuration domain is now possible:
\begin{definition}~\\*[-1\baselineskip]
\begin{enumerate}
\item An \emph{event set} $E:\domain{Evt}^*$.
The notation $E[t=c]$ represents an event set
identical to $E$ except that it contains the event $\tuple{t,c}$.  We
say a name $t$ is \emph{defined} if $\tuple{t,v} \in E$ for some $v$.
For all $\tuple{t_1,v_1},\tuple{t_2,v_2} \in E$, $t_1$ and $t_2$
differ.  This is equivalent to saying that no name $t$ is multiply
defined in an event set.  This constraint is enforced by the
transition rules, not by the definition of $E$.
\item An \emph{invariant store} $S: \domain{Xif} \to
\domain{Evt}^*$ is a finite mapping from \xifunction{s} to event sets.
\item A \emph{configuration} is a tuple
$\tuple{E, S}:\domain{Evt}^* \times (\domain{Xif}\to\domain{Evt}^*)$ consisting
of an event set and an invariant store.  The initial
configuration is
$\tuple{\{\}_{\domainsm{Evt}},
        []_{\domainsm{Xif} \to \domainsm{Evt}^*}}$
that is, it consists of an empty event set%
\footnote{Read, ``who cares about Maria'' \cite{marinov99}.}
and an empty mapping for the invariant store.%
\fixme{start condition.}
\end{enumerate}
\end{definition}

Figure~\vref{fig:eventsemantics} shows the event-driven transition
rules for \ssiplus\ form.  As before, the left column consists of
definitions and the right column shows an optional precondition above
a line, and a transition.  If the definition in the left column is
present in the \ssiplus\ form and the precondition (if any) above the
line is satisfied, then the transition can be performed.  Note that
most transitions remove some event from the event set $E$, replacing
it with a new event.  The invariant store $S$ stores
the values of loop invariants for regeneration at each loop iteration.

\textbf{MORE DESCRIPTION OF EVENT-DRIVEN SEMANTICS HERE.}

\subsection{Construction algorithms}
\textbf{Presentation of algorithms; complexity analysis.}
\footnote{Basic algorithm for generating \xifunction{s}: the top
tuple is found with a depth-first search of the source
control-flow graph, and the bottom tuple can be determined by a
traversal of the same SESE-region tree which generates \ssizero\
form.}
\footnote{Constant trigger is found via traversal of the SESE-region tree.}

\subsection{Uses and applications}
\subsubsection{Analysis and optimization.}
\subsubsection{Abstract Interpretation.}
\subsubsection{Hardware compilation.}\label{sec:hardware}
The observant reader may have noticed that the two 
operational semantics given in section \ref{sec:semantics} closely
resemble circuit implementations for the program according to
synchronous and asynchronous design methodologies.  In fact,
\ssiplus{} was designed specifically to facilitate rendering a
high-level program into hardware.  The two semantics differ primarily
on how cyclic dependencies (i.e. loops) are handled.

In this section, we will describe how to translate \ssiplus{} to
hardware, glossing over details of how the store is handled, which is
outside the scope of this work.  Upon concluding this section, it will
be obvious how to generate hardware for simple functional constructs
using \ssiplus.

\textbf{Er, at this point I will do what I just said I would do.  I'm
working on generating the figures to illustrate this section.}

\section{Results}
\textbf{Numbers, numbers, and more numbers.}

\section{Future work}
\textbf{Get a PhD.}\footnote{This involves finishing section
\ref{sec:hardware} by describing how memory is handled in hardware
compilation.  Answer: using lots and lots of sophisticated analysis,
that's how.}

\section{Conclusions}
\textbf{Scott is done.}

\newpage
\bibliography{harpoon}
\newpage
\appendix
\section{Common errors of IR research papers}
For some reason, the field of compiler intermediate representations
seems to attract more than its share of village idiots.
\begin{enumerate}
\item Quote: ``The number of $\phi$-nodes needed remains linear''
\cite{sreedhar95:lintime}.  Often \cite{cytron91:ssa} is cited in
support of this.  What Cytron \emph{actually} said was ``For the
programs we tested, the plot in Figure~21 shows that the number of
\phifunction{s} is also linear in the size of the original program.''
In other words, this is not a theoretical or hard bound, this is an
\emph{experimental} result.  By the same experimental technique that
showed the run-time of his algorithm to be linear ``in
practice''---despite the fact that nested \code{repeat-until} loops
yielded quadratic performance.  Wegman and Zadeck get points in
\cite{wegman91:scc} for stating ``in theory the size can be $O(EV)$,
but empirical evidence indicates that the work required to compute the
SSA graph is linear in the program size,'' which is rigorously correct.
\item Time bounds of Cytron's SSA algorithm stated as being $O(N^2)$.
Chow et al.\ \cite{chow97:ssapre} claim ``the $\phi$-insertion step is
$\Omega(v^2)$\ldots'' (where they use $v$ to refer to what we call
$N$), and further ``[but] there are linear-time SSA $\phi$-placement
algorithms that can be used to lower it to $O(e)$.''  Wrong, wrong,
wrong.  Cytron's algorithm (which Chow et al.\ use) is $O(N^2 V)$ and
the faster algorithms are $O(EV)$.  The $V$ cannot be omitted; $V$ can
be $O(N)$ (one definition per node).  Dhamdhere et al.\
\cite{dhamdhere92:large} blame this common error on an early paper by
Cytron et at.\ \cite{cytron89:ssa}, noting that this was corrected in
\cite{cytron91:ssa}.  As the error was published in POPL and the
correction in TOPLAS, one may understand the widespread confusion.  A
closely related sin is referring to an SSA algorithm as ``linear,''
since the SSA form itself may be cubic in the size of the input
program.  Chow et al.\ \cite{chow97:ssapre} do this, too.  Usually
what happens is a redefinition of the ``size of the program'' until
the desired algorithm can be called ``linear.''  Sreedhar and Gao
\cite{sreedhar95:lintime} do this in two ways: first, they restrict
their algorithm to ``constructing a single SEG,'' which they can do in
$O(E)$, instead of contructing the SEGs for all variables (otherwise
known as SSA form), which takes them $O(EV)$.  Then they redefine
``linear'' to be in terms of $E$ (which could be $N^2$), instead of in
terms of $N$.  Thus they manage to call an $O(N^3)$ algorithm ($E=N^2$
and $V=N$) ``linear.''  Cytron et al.\ \cite{cytron91:ssa} win the
most points by blatently declaring a variable $R$ to be ``the
largest'' of $N$, $E$, $V$, and a few other metrics.  They then spin
you in circles to distract you from the $O(R^3)$ figure they cite
(which may be as high as $O(N^6)$ if you've defined $R=E$!) before
landing at ``the entire translation process is effectively $O(R)$''!
Since the algorithm they are describing takes time equivalent to
$O(E+V_{SSA}|{DF}|+NV_{SSA})$ their conclusion is surprising, to say
the least.\footnote{Their justification, term by term: $O(E)$ is
$O(R)$.  Placing \phifunction{s}, which may take as much as
$V_{SSA}|{DF}|$ time, is linear because their empirical results say
so.  The total mentions of variables in the final program, which could
have $V_{SSA}$ variables mentioned in each of $N$ statements, is
$O(N)$---again, because that's the way it looks ``in practice.''  So,
everything is ``linear.''}
\end{enumerate}

\section{Canonical and derived definitions of the empty set}
\begin{enumerate}
\item \textbf{``Who cares about Maria?''} (Canonical) \cite{marinov99}
\item \textbf{``Who cares about Martin and Maria?''} (Radu Revised)
\item \textbf{``Who cares about \code{bash}?''} (Found on FLEX web page)
\end{enumerate}

\end{document}
