% -*- latex -*- This is a LaTeX document.
% $Id: thesis.tex,v 1.13 1999-06-06 21:36:17 cananian Exp $
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[12pt,notitlepage,twoside]{article}
%\documentclass[12pt,notitlepage,twocolumn,twoside]{article}
\usepackage{comdef}

% the name of the project
\newcommand{\magic}{\textsc{Magic}}

% double-space
\renewcommand{\baselinestretch}{1.2}

\title{Static Single Information Form}
\author{C. Scott Ananian}
\date{\today \\ $ $Revision: 1.13 $ $}

\begin{document}
\pagestyle{myheadings}\markboth{$ $Revision: 1.13 $ $}{$ $Revision: 1.13 $ $}
\bibliographystyle{abbrv}

\maketitle

\section{Introduction}
The selection of a compiler internal respresentation from the many
littering the academic literature may at times seem a black art.  Each
IR is accompanied by papers trumpeting its superiority for one purpose
or another, and each IR feels compelled to introduce new terms and
structures into the jargon.  A pragmatic compiler writer will often
pick randomly from the plethora available, basing the decision on ease
of implementation or understandibility without realizing the extent to
which the IR influences the structure of the compiler.  Similarly, the
theorist will derive proofs based upon intermediate representations
(and languages) that are amenable to mathematical methods, without
regard to current implementation practices.%
\footnote{This paragraph will be revised to be less offensive, eventually.}

This paper introduces yet another intermediate
representation to the literature:  Static Single Information (SSI) form.
This IR is the core of the FLEX compiler project, which is
investigating intelligent compilation techniques for distributed
systems among other things.  This thesis, in presenting the IR,
attempts to keep both the mathematician and the programmer in mind.  
SSI form has both a rigorous mathematical semantics and a factored
form which aids efficient implementation of advanced analyses.
I believe that it best straddles the gap between dataflow-oriented,
graph-structured, and control-flow driven IRs, while maintaining the
sparsity needed to achieve practical efficiency.  The construction
algorithms are linear in the size of the program.

Our discussion of the Static Single Information form will be at times
tied to the source language of the FLEX compiler, Java.  Unlike many
abstract IRs, the choices made in the design of SSI form have been
dictated by the necessities of compiling a real-world imperative
language.  Java, however, has several theoretical properties that make
mathematical analysis more tractable.  In particular, we
will mention here Java's strict constraints on pointer variables.
Pointers in C can be abused in many ways that Java disallows.

Ultimately, the choice of compiler internal representation is fundamental.
Advances in IRs translate into advances in compilers.  SSI form
represents a clean and simple unification of many extant ideas, and
our hope is that it will allow the FLEX compiler to achieve a similar
integration of practical implementation and mathematical elegance.

\section{Intermediate Representations}
The first paper directly addressing an IR as a separate feature of a
compiler was Foo and Bar in \cite{foobar}.\footnote{FIXME}
The Static Single Assignment (SSA) form introduced by Shapiro and
Saint in \cite{shapiro70:ssa} is the foundation of most of the
work we will discuss.  Alpern, Rosen, Wegman and Zadeck reintroduced
SSA form as a tool for efficient optimization in a pair of POPL
papers \cite{alpern88:ssa,rosen88:gvn}, and three years later Cytron
and Ferrante joined Rosen, Wegman, and Zadeck in explaining how to
compute SSA form efficiently in what has since become the 
``canonical'' SSA paper \cite{cytron89:ssa}.

Despite industry adoption of SSA form in production compilers
\cite{chow96:hssa,chow97:ssapre}, academic research into alternative
representations continues.
Recent proposals have included Value Dependence Graphs
\cite{weise94:vdg}, Program Dependence Webs \cite{ballance90:pdw},
the Program Structure Tree \cite{johnson94:pst},
DJ graphs \cite{sreedhar96:dj}, and Depedence Flow Graphs
\cite{johnson93:dfg}.

In comparison to these representations, the dominant characteristics of
our Static Single Information form may be summarized as follows:
% EXPAND THESE BULLETS.  MORE INFORMATION.
\begin{itemize}
\item It is complete.  There exists an executable semantics
for the IR that does not require the use of information external to
the IR.
\item It is simple.  The number of added constructs is kept to a
minimum.  Many SSA-form variants spawn multitudes of new
constructs; for example, Gated SSA form \cite{ballance90:pdw,tu95:gssa}
has four different \phifunction{} variants.\footnote{$\phi$-functions,
$\mu$-functions, $\gamma$-functions, and $\eta$-functions.}
\item It is compact and efficient.  Construction should be fast, and
space should be reasonable.\footnote{Replace with actual numbers.}
\item All explicit control dependencies are eliminated.  Many IRs
require explicit control flow edges that serialize computation more
than is strictly necessary.\footnote{Explain better.}
\item Efficient for both forward and backward dataflow analyses.
\end{itemize}

\textbf{Transition goes here: we are going to describe SSA form for
background on SSI form.}

\subsection{Static Single Assignment Form}
Static Single Assignment (SSA) form, introduced by Cytron in
\cite{cytron89:ssa}\ldots

\textbf{Define SSA form here.}

Figure \ref{fig:tossa} illustrates the conversion to SSA form.
\begin{figure}\label{fig:tossa}
\begin{center}
\input{Figures/THex1base} \vline\ \input{Figures/THex1ssa}
\end{center}
\caption{\phifunction{s} are added to the program on the left to
produce the SSA-form IR on the right.}
\end{figure}

\subsection{Minimal and pruned SSA forms}
Cytron defines a minimal SSA form in \cite{cytron91:ssa}.  His minimal
form is defined to use the smallest number of \phifunction{s} such
that the following three conditions hold:
\begin{enumerate}
\item If two nonnull paths $X \pathplus Z$ and $Y \pathplus Z$
converge at a node $Z$, and nodes $X$ and $Y$ contain assignments to
[a variable] $V$ (in the original program), then a trivial
\phifunction{} $V \leftarrow \phi(V, \ldots, V)$ has been inserted at
$Z$ (in the new program). \label{criteria1}
\item Each mention of $V$ in the original program or in an inserted
\phifunction{} has been replaced by a mention of a new variable $V_i$,
leaving the new program in SSA form.
\item Along any control flow path, consider any use of a variable $V$
(in the original program) and the corresponding use of $V_i$ (in the
new program).  Then $V$ and $V_i$ have the same value.
\end{enumerate}
Of these criteria, the first %\ref{criteria1}
is the most important.
%The other two can be summarized by noting that in proper SSA form,
%there is exactly one reaching definition for a variable $V$ at every
%non-$\phi$ use of $V$.
The SSA form in figure \ref{fig:tossa} is minimal.

A variation
on minimal SSA form, called \emph{pruned} form \cite{ferrante91:pruned},
avoids placing \phifunction{s} which define variables which are never used.
In most cases, the more regular properties of minimal SSA form
outweigh the pruned form's slight increase in space efficiency.
Figure \ref{fig:prunedssa} compares minimal and pruned SSA form for
our example program.
\begin{figure}\label{fig:prunedssa}
\begin{center}
\input{Figures/THex1ssa} \vline\ \input{Figures/THex1ssaPr}
\end{center}
\caption{Minimal SSA form on the left; pruned SSA form on the right.}
\end{figure}

\section{Static Single Information Form}

SSI form extends SSA form to achieve symmetry for both forward and
reverse dataflow.   SSI form recognizes that information about
variables is generated at branches and generates new names at these
points.  This provides us with a one-to-one mapping between variable
names and information about those variables which is independent of
control-flow graph context.  Analyses can then associate information
with variable names and propagate this information efficiently and
directly both with and against the dataflow direction.

\subsection{Definition of SSI form}
Building SSI form involves adding pseudo-assignments for a variable $V$:
\begin{enumerate}
\item[$(\phi)$] at a control-flow merge when disjoint paths from a
conditional branch come together and at least one of the paths
contains a definition of $V$; and
\item[$(\sigma)$] at locations where control-flow splits and at least
one of the disjoint paths from the split uses the value of $V$.
\end{enumerate}

Figure \ref{fig:tossi} compares the SSA and SSI forms for our example program.
% add text here about how SSI is better.
\begin{figure}\label{fig:tossi}
\begin{center}
\input{Figures/THex1ssa} \vline\ \input{Figures/THex1ssi}
\end{center}
\caption{SSA form on the left; SSI form on the right.}
\end{figure}

\subsection{Minimal and pruned SSI forms}
\emph{Minimal} and \emph{pruned} SSI forms can be defined which
parallel their SSA counterparts.  \emph{Minimal} SSI form would have the
smallest number of $\phi$- and \sigfunction{s} such that:
\begin{enumerate}
\item If two nonnull paths $X \pathplus Z$ and $Y \pathplus Z$
exist having only the node $Z$ where they converge in common,
and nodes $X$ and $Y$ contain either assignments to a variable $V$ in the
original program or a $\phi$- or \sigfunction{} for $V$ in the new program,
then a \phifunction{} for $V$ has been inserted at $Z$ in the new program.
\item If two nonnull paths $Z \pathplus X$ and $Z \pathplus Y$
exist having only the node $Z$ where they diverge in common,
and nodes $X$ and $Y$ contain either uses of a variable $V$ in the
original program or a $\phi$- or \sigfunction{} for $V$ in the new program,
then a \sigfunction{} for $V$ has been inserted at $Z$ in the new program.
\item There is exactly one reaching definition of $V$ at every
non-$\phi$ use of $V$ in the new program.
\item \textbf{FIXME: what's the equivalent condition for \sigfunction{s}?}
% has something to do with cycle equivalence.
\end{enumerate}

A \emph{pruned} SSI form would be the minimal form without any unused
definitions, that is, $\phi$ or \sigfunction{s} after which there are no
subsequent uses of any of the variables defined on the left-hand side,
except in other $\phi$ or \sigfunction{s}.\footnote{An even more
compact SSI form may be produced by removing \sigfunction{s} for which
there are uses for \emph{exactly one} of the variables on the
left-hand side, but it does not seem profitable to split this
particular hair.}

Figure \ref{fig:prunedssi} compares minimal and pruned SSI form for
our example program.
\begin{figure}\label{fig:prunedssi}
\begin{center}
\input{Figures/THex1ssi} \vline\ \input{Figures/THex1ssiPr}
\end{center}
\caption{Minimal SSI form on the left; pruned SSI form on the right.}
\end{figure}

\subsection{Theory and algorithms}
\textbf{Discuss construction algorithms here.}

\subsection{Uses and applications}
\textbf{Discuss predicated analysis, anticipatibility, PRE, etc.}

\section{An executable representation}
The Static Single Information (SSI) form, as presented in the first
half of this thesis,
requires control-flow graph information in order to be executable. We
would like to have a demand-driven operational semantics for SSI form
that does not require control-flow information; thus freeing us to
more flexibly reorder execution.

In particular, we would like a representation that eliminates
unnecessary control dependencies such as exist in the program of
figure \ref{fig:ctrldep}.  A control-flow graph for this program, as
it is written, will explicitly specify that no assignments to
\texttt{B[]} will take place until all elements of \texttt{A[]} have
been assigned; that is, the second loop will be
\emph{control-dependent} on the first.  We would like to remove this
control dependence in order to provide greater parallelism---in this
case, to allow the assignments to \texttt{A[]} and \texttt{B[]} to
take place in parallel, if possible.\footnote{Note that Arvind's
dataflow compiler looked for exactly the opposite type of
parallelism.  By concentrating on intra-loop dependencies, as he did, you get
fine-grain parallelism suitable for VLIW-type machine with many
functional units.  We try to remove dependencies \emph{between} loops,
which gets us coarser parallelism that does not require as many
functional units to take advantage of.  This should be in the paper
body, but I'm not quite sure where a discussion of the differences
between Arvind's work and mine belongs.}

\begin{figure}[t]
\begin{samplecode}
for (int i=0; i<10; i++)\\
\>A[i] = x;\\
for (int j=0; j<10; j++)\\
\>B[j] = y;\\
\end{samplecode}
\caption{An example of unnecessary control dependence: the second loop
is \emph{control-dependent} on the first and so assignments to
\texttt{A[]} and \texttt{B[]} cannot take place in parallel.}
\label{fig:ctrldep}
\end{figure}

In addition, an executable representation allows us to apply the
techniques of abstract interpretation \cite{idunno}.  Although abstract
interpretation may be applied to the original SSI form using
information extracted from the control flow graph, as in \cite{foo}
and \cite{bar}, an executable SSI form allows more concise (and thus,
more easily derived and verified) interpretation
algorithms.\footnote{i'm still working on understanding abstract
interpretation techniques.  it may be that we can do stuff with
\ssiplus\ that is plain impossible with \ssizero\ (instead of merely
more difficult); if so, i'll mention that here, of course.}

The modifications outlined here will extend SSI form in order to
provide a useful and descriptive operational semantics.  We will call
the extended form \ssiplus.  For clarity, SSI form as originally
presented we will call \ssizero.  We will describe algorithms to
contruct \ssiplus{} efficiently,\footnote{i will make this more precise
as soon as I get the math done; construction should be $O(N)$, and space
$O(N^2)$, but $\Theta(N)$.} and illustrate analyses and
optimizations using the form.\footnote{maybe something here like: we
will also show how to apply the techniques of abstract interpretation
to \ssiplus?}

\subsection{Discussion}
\textbf{Why an executable representation is desirable.  Also,
deficiencies in \ssizero{} form.}

\textbf{A couple of paragraphs here on why an executable
representation is desirable.  Many some of the information above
should be moved down here.}

\textbf{A couple paragraphs describing deficiencies in \ssizero{} form.
Structure question: I'd like to describe the fixes \ssiplus{} proposes
right next to the bits saying what's broken in \ssizero{}; but the way
the outline is laid out currently, the fixes are described later.  I'm
going to try it the way the outline has it for now.}

\subsection{Definition}
\begin{itemize}
\item \textbf{\xifunction{s}.}
\item \textbf{Triggered constants.}
\end{itemize}

\subsection{Semantics}\label{sec:semantics}
We will base the operational semantics of \ssiplus\ on a demand-driven
dataflow model.  We will define both a cycle-oriented semantics and an
event-driven semantics, which (incidentally) correspond to synchronous
and asynchronous hardware models.

Following the lead of Pingali \cite{pingali90:dfg}, we present Plotkin-style
semantics \cite{plotkin81:opsem} in which \emph{configurations} are
rewritten instead of programs.  The configurations represent program
state and transitions correspond to steps in program execution.  The
set of valid transitions is generated from the program text.

The semantics operate over a lifted value domain
$V=\mathit{Int}_\bot$. When some variable $t = \bot_V$ we say it is
\emph{undefined}; conversely $t\succ\bot_V$ indicates that the
variable is \emph{defined}.  ``Store'' metavariables $S_x$ are not
explicitly handled by the semantics, but the extension is trivial with
an appropriate redefinition of the value domain $V$.  Floating-point
and other types are also trivial extensions.  The
metavariables $c$ and $v$ stand for elements of $V$.

We also define a domain of \emph{variable names},
$\mathit{Nam}=\set{n_0,n_1,\ldots}$.  The metavariables $t$ and $P$ stand for
elements in $\mathit{Nam}$, although $P$ will be reserved for naming branch predicates.

\textbf{Neither of these semantics incorporate constant triggers, yet.}

\subsubsection{Cycle-oriented semantics}
\begin{figure}[t]
\begin{transitions}
t=c,\,c\:\mbox{a constant}:
& \trule{\rho[t]=\bot \wedge t\notin D}
        {\tuple{\rho, D} \to
         \tuple{\rho[t \mapsto c],D}} \\

t=\mathbf{op}(t_1,\ldots,t_n):
& \trule{\rho[t]=\bot \wedge 
         \left(
          \rho[t_1]\succ\bot \wedge \ldots \wedge \rho[t_n]\succ\bot
	 \right)}
	{\tuple{\rho,D} \to
         \tuple{\rho[t \mapsto
                     \mathbf{op}(\rho[t_1],\ldots,\rho[t_n])],
                D \cup \set{t_1,\ldots,t_n} } } \\

t=\phi(t_1,\ldots,t_n):
& \trule{\rho[t]=\bot \wedge
         \rho[t_j]\succ\bot \wedge
         \mbox{all other}\,\rho[t_1],\ldots,\rho[t_n]=\bot}
        {\tuple{\rho,D} \to
         \tuple{\rho[t \mapsto \rho[t_j]],
                D \cup \set{t_j} } } \\

\tuple{t_1,\ldots,t_n}=\sigma(P,t):
& \trule{\rho[P]=v\succ\bot \wedge
         \rho[t_{v-1}]=\bot \wedge
         \rho[t]\succ\bot
         \mbox{ where } (0\leq v\leq n-1)}
	{\tuple{\rho,D} \to
         \tuple{\rho[t_{v-1} \mapsto \rho[t]],
                D \cup \set{t} } } \\

\footnotesize % latex complains, but does the right thing.
\xivec{t_1,\ldots,t_n}{t_{n+1},\ldots,t_{m}}=\xi(\xivec{t'_1,\ldots,t'_n}{t'_{n+1},\ldots,t'_m}):
& \trule{\rho[t_j]=\bot \wedge
         \rho[t'_j]\succ\bot \mbox{ where } (1\le j\le n)}
        {\tuple{\rho,D} \to
         \tuple{\rho[t_j \mapsto \rho[t'_j]], D \cup \set{t'_j} } } \\

\footnotesize % latex complains, but does the right thing.
\xivec{t_1,\ldots,t_n}{t_{n+1},\ldots,t_{m}}=\xi(\xivec{t'_1,\ldots,t'_n}{t'_{n+1},\ldots,t'_m}):
& \footnotesize % latex complains, but does the right thing.
  \trule{\rho[t'_{n+1}]\succ\bot \wedge \ldots \wedge \rho[t'_m]\succ\bot}
        {\myarray{r}{
         \tuple{\rho,D} \to
	 \hfill \\ \: % line-break
         \tuple{\rho_\emptyset
                [t_1 \mapsto \rho[t_1]]\ldots[t_n \mapsto \rho[t_n]]
%               \right.\right.\\ \left.\left. % line-break
                [t_{n+1} \mapsto \rho[t'_{n+1}]]\ldots[t_m \mapsto \rho[t'_m]],
                \right.\\ \left. % line-break
	        D \cup \set{t_1,\ldots,t_n,t'_{n+1},\ldots,t'_m} } } }\\
\end{transitions}
\caption{Cycle-oriented transition rules for \ssiplus.}
\label{fig:cyclesemantics}
\end{figure}

In the cycle-oriented semantics, configurations consist of an
\emph{environment} and a \emph{done set}.  The environment $\rho$ maps
names in $\mathit{Nam}$ to values in $V$; the done set $D$ is used for constant
value bookkeeping.

\begin{definition}~\\*[-1\baselineskip]
\begin{enumerate}
\item An \emph{environment} $\rho: N \to V$ is a finite function ---
its domain $N \subset \mathit{Nam}$ is finite.  The \emph{done set}
$D$ is a subset of $N$.
The notation $\rho[t\mapsto c]$ represents an environment
identical to $\rho$ except for name $t$ which is mapped to $c$.
\item The null environment $\rho_\emptyset$ maps every $t\in N$ to
$\bot_V$.
\item A \emph{configuration} is a pair $\tuple{\rho,D}$ consisting of
an environment and a done set.  The initial configuration is
$\tuple{\rho_\emptyset, \{\}_{\mathit{Nam}}}$ ---
that is, all names in $N$ are
mapped to $\bot_V$ and the done set $D$ is empty.
\end{enumerate}
\end{definition}

Figure \ref{fig:cyclesemantics} shows the cycle-oriented transition
rules for \ssiplus\ form.  The left column consists of definitions and
the right column shows a precondition on top of the line, and a
transition below the line.  If the definition in the left column is
present in the \ssiplus\ form and the precondition on top of the line
is satisfied, then the transition shown below the line can be performed.

\textbf{EXPLAIN THE RATIONALE BEHIND THE RULES HERE.}

\subsubsection{Event-driven semantics}
\begin{figure}[t]\small
\begin{transitions}
t=c,\,c\:\mbox{a constant}:
& \trule{t\notin D}
  {\tuple{E,D,S} \to \tuple{E[t=c],D\cup\{t\},S} }\\

t=\mathbf{op}(t_1,\ldots,t_n):
& \tuple{E[t_1=v_1]\ldots[t_n=v_n],D,S} \to \tuple{E[t=\mathbf{op}(v_1,\ldots,v_n)],D,S}\\

t=\phi(t_1,\ldots,t_n):
& \tuple{E[t_i=v],D,S} \to \tuple{E[t=v],D,S}\\

\tuple{t_1,\ldots,t_n}=\sigma(P,t):
& \tuple{E[t=v][P=i],D,S} \to \tuple{E[t_i=v],D,S}\\

\xivec{t_1,\ldots,t_n}{t_{n+1},\ldots,t_m}=\xi_K(\xivec{t'_1,\ldots,t'_n}{t'_{n+1},\ldots,t'_m}):
& \myarray{r}{
  \tuple{E[t'_i=v],D,S} \to \hfill\\\quad\quad\quad\quad\quad
  \tuple{E[t_i=v],D,S[K\mapsto S[K]\cup\tuple{t_i,v}]}\\
   \mbox{where }1\le i \le n\\
%  \mbox{where }K\mbox{ is a unique constant corresponding to}\\
%  \mbox{this \ssiplus\ statement}\\
  }\\

\xivec{t_1,\ldots,t_n}{t_{n+1},\ldots,t_m}=\xi_K(\xivec{t'_1,\ldots,t'_n}{t'_{n+1},\ldots,t'_m}):
& \trule{S[K]=\left\{\tuple{t_1,v_1},\ldots,\tuple{t_n,v_n}\right\}}
  {\myarray{r}{
   \tuple{E[t'_{n+1}=v_{n+1}]\ldots[t'_m=v_m],D,S} \to\quad\quad\quad\quad\\
   \tuple{E[t_1=v_1]\ldots[t_m=v_m],D,S}\\
%   \mbox{where }S[K]=\bigcup_{i=1}^n \{\tuple{t_i,v_i}\}\\
%   \mbox{where }S[K]=\left\{\tuple{t_1,v_1},\ldots,\tuple{t_n,v_n}\right\}\\
  } }
\end{transitions}
\caption{Event-driven transition rules for \ssiplus.  Note the
unfortunate synchronization in the last rule. $K$ is a
statement-identifier constant which is unique for each source \xifunction.}
\label{fig:eventsemantics}
\end{figure}

In the event-driven semantics, configurations consist of an
\emph{event set}, a \emph{done set}, and an \emph{invariant store}.
The event set $E$ contains definitions of the form $t=c$, and the done
set $D$ is defined as for the cycle-oriented semantics.  The invariant
store is a mapping from numbered \xifunction{s} in the source
\ssiplus\ form to a set of tuples representing saved values for loop
invariants.

We define the following domains:
\begin{itemize}
\item $\mathit{Evt} = \mathit{Nam} \times V$ is the event domain.  An
event consists of a name-value pair.  The metavariable $e$ stands for
elements of $\mathit{Evt}$.
\item $\mathit{Xif} \subset \mathit{Int}$ is used to number
\xifunction{s} in the source \ssiplus\ form.  There is some mapping
function which relates \xifunction{s} to unique elements of
$\mathit{Xif}$.  The metavariable $K$ stands for an element in
$\mathit{Xif}$.
\end{itemize}

A formal definition of our configuration domain is now possible:
\begin{definition}~\\*[-1\baselineskip]
\begin{enumerate}
\item An \emph{event set} $E$ is a finite subset of $\mathit{Evt}$.
The notation $E[t=c]$ represents an event set identical to $E$ except
that it contains the event $\tuple{t,c}$.  We say a name $t$ is
\emph{defined} if $\tuple{t,v} \in E$ for some $v$.  For all
$\tuple{t_1,v_1},\tuple{t_2,v_2} \in E$, $t_1$ and $t_2$ differ.  This is
equivalent to saying that no name $t$ is multiply defined in an event
set.  This constraint is enforced by the transition rules, 
not by the definition of $E$.
\item An \emph{invariant store} $S: K \to E$ is a finite mapping from
\xifunction{s} to event sets.
\item A \emph{configuration} is a tuple $\tuple{E, D, S}$ consisting
of an event set, a done set, and an invariant store.  The initial
configuration is
$\tuple{\{\}_{\mathit{Evt}},
        \{\}_{\mathit{Nam}},
        \{\}_{\mathit{Xif} \times E}}$ ---
that is, it consists of an empty event set, an empty done set, and
an empty mapping for the invariant store.
\end{enumerate}
\end{definition}

Figure \ref{fig:eventsemantics} shows the event-driven transition
rules for \ssiplus\ form.  As before, the left column consists of
definitions and the right column shows an optional precondition above
a line, and a transition.  If the definition in the left column is
present in the \ssiplus\ form and the precondition (if any) above the
line is satisfied, then the transition can be performed.  Note that
most transitions remove some event from the event set $E$, replacing
it with a new event.  The done set $D$ ensures that only one event is
generated for constant assignments.  The invariant store $S$ stores
the values of loop invariants for regeneration at each loop iteration.

\textbf{MORE DESCRIPTION OF EVENT-DRIVEN SEMANTICS HERE.}

\subsection{Construction algorithms}
\textbf{Presentation of algorithms; complexity analysis.}
\footnote{Basic algorithm for generating \xifunction{s}: the top
tuple is found with a depth-first search of the source
control-flow graph, and the bottom tuple can be determined by a
traversal of the same SESE-region tree which generates \ssizero\
form.}
\footnote{Constant trigger is found via traversal of the SESE-region tree.}

\subsection{Uses and applications}
\subsubsection{Analysis and optimization.}
\subsubsection{Abstract Interpretation.}
\subsubsection{Hardware compilation.}\label{sec:hardware}
The observant reader may have noticed that the two 
operational semantics given in section \ref{sec:semantics} closely
resemble circuit implementations for the program according to
synchronous and asynchronous design methodologies.  In fact,
\ssiplus{} was designed specifically to facilitate rendering a
high-level program into hardware.  The two semantics differ primarily
on how cyclic dependencies (i.e. loops) are handled.

In this section, we will describe how to translate \ssiplus{} to
hardware, glossing over details of how the store is handled, which is
outside the scope of this work.  Upon concluding this section, it will
be obvious how to generate hardware for simple functional constructs
using \ssiplus.

\textbf{Er, at this point I will do what I just said I would do.  I'm
working on generating the figures to illustrate this section.}

\section{Results}
\textbf{Numbers, numbers, and more numbers.}

\section{Future Work}
\textbf{Get a PhD.}\footnote{This involves finishing section
\ref{sec:hardware} by describing how memory is handled in hardware
compilation.  Answer: using lots and lots of sophisticated analysis,
that's how.}

\section{Conclusions}
\textbf{Scott is done.}

\bibliography{harpoon}
\appendix
\end{document}
