% -*- latex -*- This is a LaTeX document.
% $Id: pldi04.tex,v 1.4 2003-11-09 20:59:02 cananian Exp $
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[preprint]{acmconf}
\usepackage{times}
\usepackage{epsfig}
\usepackage{supertech}
\usepackage{array}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{dcolumn}
\usepackage{varioref} % \vref command
\usepackage{support}

% work around fig2dev wanting to add colors to .figures.
\providecommand{\color}[2][rgb]{}

%setup varioref package
\renewcommand{\reftextbefore}{on the preceding page}\vrefwarning

\renewcommand{\floatpagefraction}{0.8}
\renewcommand{\topfraction}{0.25}

\title{Efficient Software Transactions for Object-Oriented Languages}

\gdef\and{\hspace*{4em}}%Tight title with one line for authors
\author{C.~Scott~Ananian \and Martin~Rinard \\
Computer Science and Artificial Intelligence Laboratory\\
Massachusetts Institute of Technology\\ 
Cambridge, MA 02139 \\
\texttt{\{cananian,rinard\}@csail.mit.edu}
}
\special{papersize=8.5in,11in}

%% Paper title: an efficient implementation of software transactions for
%% o-o lang.

%% Step-by-step justify our efficiency claims:

%% start: scales dsm paper.
%%   show that adding a check does not affect runtime too much, and
%%   that false hits are low.
%%     (possibly restructure all sub-word accesses as word-accesses at
%%      a high level to reduce false hits with sub-word data?
%%      do this only if these benchmarks show it is worthwhile)
%%   show that reading much more frequent than writing, so not alloc'ing
%%   extra memory for reads is a good idea.

%%   show that expanding object with transaction info is not too
%%   expensive.

%%   show that transaction mechanism, when invoked, is cheap.

%%   large objects?
%% ---------
%% concurrent updates?  we support concurrent *reads*.


%\renewcommand{\baselinestretch}{0.97}
\begin{document}
%
\maketitle
\support{This research was supported by DARPA/AFRL Contract F33615-00-C-1692.}
%
% abstract
\begin{abstract}
\ldots
\end{abstract}
%
\section{Introduction}

\subsection{Contributions}

\section{Example}

\section{Non-blocking synchronization}\label{sec:nb-sync}
%\subsection{History}

\begin{figure}
{\ttfamily
\begin{tabular}{l@{\hspace{1in}}l}
Writer:        &Reader:                 \\
~~v1++;        &~~do \{                 \\
~~{\it write}; &~~~~temp = v2;          \\
~~v2++;        &~~~~{\it read};         \\
               &~~\} while (temp != v1);\\
\end{tabular}
}
\caption{Lamport's single-writer multiple-reader non-blocking
    synchronization algoritm \cite{Lamport77}.}
% writes and reads to v1/v2 can be non-atomic if the subreads/subwrites
% are done in the correct order; see original reference for details.
\label{fig:lamport}
\end{figure}
Lamport presented the first alternative to synchronization via mutual
exclusion in \cite{Lamport77}, for a limited situation involving a single
writer and multiple readers.  Lamport's technique, presented in
Figure~\ref{fig:lamport}, relies on reading
guard elements $v_1$ and $v_2$ in an order opposite to that in which
they are written,
guaranteeing that a consistent data snapshot can be recognized.  The
writer always completes its part of the algorithm in a constant number
of steps; readers are guaranteed to complete only in the absence of
concurrent writes.

Herlihy formalized \emph{wait-free} implementations of
concurrent data objects in \cite{Herlihy88}.  A wait-free implementation
guarantees that any process can complete any operation in a finite
number of steps, regardless of the activities of other processes.
Lamport's algorithm, for example, is not wait-free
because readers can be delayed indefinitely.  Wait-free algorithms
typically involve ``recursive helping,'' whereby active processes can
complete operations on behalf of stalled processes, ensuring that all
operations are eventually completed.

Massalin and Pu introduced the term \emph{lock-free} to describe 
algorithms with weaker progress guarantees.
A lock-free implementation guarantees only that \emph{some}
process will complete in a finite number of steps
\cite{MassalinPu91}.  Unlike a wait-free implementation,
lock-freedom allows starvation.  Since other simple techniques can be
layered to prevent starvation (for example, exponential backoff),
simple lock-free implementations are usually seen as worthwhile practical
alternatives to more complex wait-free implementations.

An even weaker criterion, \emph{obstruction-freedom}, was introduced
by Herlihy, Luchangco, and Moir in \cite{HerlihyLuMo03}.
Obstruction-freedom only guarantees progress for threads executing in
isolation; that is, although other threads may have partially
completed operations, no other thread may take a step until the
isolated thread completes.  Obstruction-freedom not only allows
starvation of a particular thread, it allows contention among threads
to halt all progress in all threads
indefinitely.  External mechanisms are used to reduce contention
(thus, achieve progress) including backoff, queueing, or timestamping.

Revisiting Lamport's algorithm, we conclude it is neither lock-free
nor obstruction-free,
because halting the writer between the guard increments will prevent
readers from ever getting a consistent snapshot.

We will use the term \emph{non-blocking} to describe
generally any synchronization mechanism which doesn't rely on mutual
exclusion or locking, including wait-free, lock-free,
and obstruction-free implementations.
We will be concerned mainly with lock-free algorithms.%
\footnote{Note that some authors use ``non-blocking'' and
  ``lock-free'' as synonyms, usually meaning what we here call
  \emph{lock-free}.  Others exchange our definitions for ``lock-free''
  and ``non-blocking'', using lock-free as a generic term and non-blocking
  to describe a specific class of implementations.  As there is
  variation in the field, we choose to use the parallel construction
  \emph{wait-free}, \emph{lock-free}, and \emph{obstruction-free} for
  our three specific progress criteria, and the dissimilar
  \emph{non-blocking} for the general class.}

\subsection{Advantages over mutual exclusion}\label{sec:nb-adv}
Non-blocking synchronization offers a number of advantages over mutual
exclusion within critical regions.  Foremost for the concerns of this paper is
fault-tolerance:  a process which fails while holding a lock within a
critical region can prevent all other non-failing processes from
ever making progress.  In an operating system context, this means that
thread termination must be done very carefully to avoid inadvertently
killing a process within a critical region.  If a thread dies while
holding a user-mode lock, all other threads in its address space may
deadlock; if it dies while holding a kernel lock, the whole system may
crash.  Although one can use external means to recognize orphaned
locks and release them, it is in general not possible to restore the
locked data structures to a consistent state after such a failure.
Non-blocking synchronization offers a graceful means out of
these troubles, as non-progress or failure of any one thread will not
affect the progress or consistency of other threads or the system.
These fault-tolerant properties are even more relevant in
distributed systems where entire nodes may fail without warning.

Non-blocking synchronization offers performance benefits as well.
Even in a failure-free system, page faults, cache misses, context
switches, I/O, and other unpredictable events may result in delays to the
entire system when mutual exclusion is used; non-blocking
synchronization allows undelayed processes or processors to continue
to make progress.  In loosely coupled asynchronous systems such
unexpected delays are the norm, rather than the exception.
%On parallel systems critical regions may cause unexpected serialization.

Real-time systems have other problems with mutual exclusion.
A low-priority task which 
acquires a lock and is then delayed may hold up higher-priority tasks
which contain critical regions protected with the same lock.
This situation is called
\emph{priority inversion}, and is responsible for a number of
high-profile system failures, including whole-system resets during the Mars
Pathfinder mission \cite{Jones97}.  Non-blocking
synchronization can guarantee that the high-priority task makes
progress.\footnote{Note that the progress guarantees made are
  different for wait-free, lock-free, and obstruction-free
  algorithms.  For example, priority-inversion can still occur on
  obstruction-free implementations if a lower priority thread contends
  persistently for the resource.  On a uniprocessor, a valid solution
  in this case might be to simply not interleave executions of tasks
  with differing priorities; obstruction-freedom then guarantees that
  the high-priority task ``in isolation'' will make progress.}

\subsection{Efficiency}
Herlihy presented the first \emph{universal} method for wait-free
concurrent implementation of an arbitrary sequential object
\cite{Herlihy88,Herlihy91}.  This original method was based on
a \emph{fetch-and-cons} primitive, which atomically places
an item on the head of a list and returns the list of items following
it; all concurrent primitives capable of solving the
$n$-process consensus problem---\emph{universal} primitives---were
shown powerful enough to implement \emph{fetch-and-cons}.
In Herlihy's method, 
every sequential operation is translated into two steps.  In the first,
\emph{fetch-and-cons} is used to place the name and arguments of the
operation to be performed
at the head of a list, returning the other operations on the list.
Since the state
of a deterministic object is completely determined by the history of
operations performed on it, applying the operations returned
in order from last to first is sufficient to locally reconstruct the
object state 
prior to our operation.
We then use the prior state to compute the result of our operation
without requiring further synchronization with the other processes.

This first universal method was not very practical, a shortcoming
which Herlihy soon addressed \cite{Herlihy93}.  In addition, his revised universal
method can be made lock-free, rather than wait-free, resulting in
improved performance.  In the lock-free version of this method,
objects contain a shared variable
holding a pointer to their current state.  Processes begin by loading
the current state pointer and then copying the referenced state to a
local copy.  The sequential operation is performed on the
copy, and then if the object's shared state pointer is unchanged from
its initial load it is atomically swung to point at the updated state.

Herlihy called this the ``small object protocol'' because the object
copying overhead is prohibitive unless the object is small enough to
be copied efficiently (in, say, $O(1)$ time).  He also presented a
``large object protocol'' which requires the programmer to
manually break the object into small blocks, after which the small
object protocol can be employed.  This trouble with large objects is
common to many non-blocking implementations; our solution is presented
in Section~\ref{sec:proposal}.

Barnes provided the first universal non-blocking implementation
method which avoids object copying \cite{165265}.  He eliminates the
need to store ``old'' object
state in case of operation failure by having all threads cooperate to
apply operations.  For example, if the first processor begins an operation
and then halts, another processor will complete the first's operation
before applying its own.  Barnes proposes to accomplish the
cooperation by creating a parallel state machine for each operation,
so that each thread can independently try to advance the machine from state
to state and thus advance incomplete operations.%
\footnote{It is interesting to note that Barnes' cooperative method
  for non-blocking 
  situation plays out in a real-time system very similarly to priority
  inheritance for locking synchronization.}
Although this avoids
copying state, the lock-step cooperative process is extremely
cumbersome and does not appear to have ever been implemented.
Furthermore, it does not protect against errors in the implementation
of the operations, which could cause \emph{every} thread to fail in turn
as one by one they attempt to execute a buggy operation.

Alemany and Felten \cite{135446} identified two factors hindering the
performance of non-blocking algorithms to date: resources wasted by operations
that fail, and the cost of data copying.  Unfortunately, they
proceeded to
``solve'' these problems by ignoring short delays and failures and
using operating system support to handle delays caused by
context switches, page faults, and
I/O operations.  This works in some situations, but obviously suffers
from a bootstrapping problem as the means to implement an operation system.

Although lock-free implementations are usually assumed to be more
efficient that wait-free implementations, LaMarca \cite{197975}
showed experimental evidence that Herlihy's simple
wait-free protocol scales very well on parallel machines.
When more than about twenty threads are involved, the wait-free
protocol becomes
faster than Herlihy's lock-free small-object protocol, three OS-aided
protocols of LaMarca and Alemany and Felten, and a
\emph{test-and-Compare\&Swap} spin-lock.

% Afek et al have a somewhat complicated improved wait-free method.

% Transactional memories?

\section{Analyses and Implementation}

\section{Experimental Results}

\section{Related Work}

\section{Conclusions}

\renewcommand{\baselinestretch}{1}
\bibliographystyle{plain}
\bibliography{xaction}

%\appendix
%\input{pldi02-appendix}
\end{document}

% LocalWords:  csail mit edu AFRL
