% -*- latex -*- This is a LaTeX document.
% $Id: pldi04.tex,v 1.16 2003-11-11 06:24:42 cananian Exp $
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[preprint]{rdbacmconf}
\usepackage{times}
\usepackage{epsfig}
\usepackage{supertech}
\usepackage{array}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{dcolumn}

\usepackage{amstext}
\usepackage{varioref} % \vref command
\usepackage{support}
\usepackage{xspace}

% work around fig2dev wanting to add colors to .figures.
\providecommand{\color}[2][rgb]{}

%setup varioref package
\renewcommand{\reftextbefore}{on the preceding page}\vrefwarning

\renewcommand{\floatpagefraction}{0.8}
\renewcommand{\topfraction}{0.25}

\title{Efficient Software Transactions for Object-Oriented Languages}

\gdef\and{\hspace*{4em}}%Tight title with one line for authors
\author{C.~Scott~Ananian \and Martin~Rinard \\
Computer Science and Artificial Intelligence Laboratory\\
Massachusetts Institute of Technology\\ 
Cambridge, MA 02139 \\
\texttt{\{cananian,rinard\}@csail.mit.edu}
}
\special{papersize=8.5in,11in}

\date{\today \\ $ $Revision: 1.16 $ $}

%% Paper title: an efficient implementation of software transactions for
%% o-o lang.

%% Step-by-step justify our efficiency claims:

%% start: scales dsm paper.
%%   show that adding a check does not affect runtime too much, and
%%   that false hits are low.
%%     (possibly restructure all sub-word accesses as word-accesses at
%%      a high level to reduce false hits with sub-word data?
%%      do this only if these benchmarks show it is worthwhile)
%%   show that reading much more frequent than writing, so not alloc'ing
%%   extra memory for reads is a good idea.

%%   show that expanding object with transaction info is not too
%%   expensive.

%%   show that transaction mechanism, when invoked, is cheap.

%%   large objects?
%% ---------
%% concurrent updates?  we support concurrent *reads*.


%%% macros for this paper
\newcommand{\atomic}{\texttt{atomic}\xspace}
\newcommand{\funcname}[1]{\ensuremath{\text{\sc #1}}}
\newcommand{\var}[1]{\ensuremath{\text{\it #1}}}
\newcommand{\fref}[2]{\ensuremath{#1\text{\tt .#2}}}
\newcommand{\addr}[1]{\ensuremath{\text{\tt \&}(#1)}}
\newcommand{\tuple}[1]{\ensuremath{\left\langle #1 \right\rangle}}


%\renewcommand{\baselinestretch}{0.97}
\begin{document}
%
\maketitle
\support{This research was supported by DARPA/AFRL Contract F33615-00-C-1692.}
%
% abstract
\begin{abstract}
This paper proposes an efficient object-based implementation of
non-blocking software transactions.  We integrate transactions into Java
to provide race-safety for certain types of improperly synchronized
code, to provide fault-tolerance and backtracking, and to express
synchronization and optimistic concurrency.  We use ideas from
distributed shared memory implementations to efficiently implement
transactions with little overhead for non-transactional code.  We also
show how functional arrays can be used to overcome object size
limitations for non-blocking synchronization based on Herlihy's
``small object'' scheme.\note{Also, transactions are exception-handling?}

We present a detailed design of our proposal, justifying the choices
we make with empirical performance results.  We describe compiler
optimizations which greatly improve the common-case efficiency for
typical Java benchmarks.  We argue that our transaction mechanism
makes some racy existing code safe, is easier to use for new code,
and promises better scalability than the existing
mutual-exclusion--based synchronization methods.
\end{abstract}

%
\secput{intro}{Introduction}

\subsecput{contrib}{Contributions}
The contributions of this paper include:
\begin{itemize}
\item A way to ``transactify'' existing Java programs, and a
  description of the transaction properties of typical Java programs
  transformed in such a way.
\item A concrete language design to integrate transactions into Java
  providing fault-tolerance, backtracking, and non-blocking
  synchronization.
\note{This is \texttt{try \{\ldots\} else \{\ldots\}} syntax.}
\note{Omitting ``optimistic concurrency'' from this list for now.}
\item An efficient implementation of this design which extends
  race-protection to unsynchronized code.
\item Empirical data to justify the implementation choices.
\item Compiler optimizations to improve the speed of software
  transactions.
\item An application of fast functional arrays to solve the ``large
  object'' problem with Herlihy's universal non-blocking protocol.
  \note{Short section; cite my Area Exam as a Technical Report here
  for more details.}
\item A variation of the implementation which leverages hardware
  support for small transactions.  This allows hardware small
  transactions to be bootstrapped to provide efficient large
  transactions.
\end{itemize}

\secput{example}{Example and Motivation}
This section illustrates how transactions can ease
the programming of multithreaded applications.  In particular, we show
that implementing mutual exclusion with locks can be quite
complicated, even for simple code.  Moreover, the overheads of a
locking solution can be significant, especially if the locks enforce
mutual exclusion in a tight inner loop.

A transaction can be thought of as a sequence of loads and stores
performed as part of a program.  The intuition for transactions is
that they either \defn{commit} or they \defn{abort}.  If a transaction
commits, then all of the loads and stores appear to have run
atomically with respect to other transactions.  That is, the
transaction's operations are not interleaved with those of other
transactions.  If a transaction aborts, then none of its stores take
effect and the transaction must be restarted, typically using a
backoff algorithm to preclude livelock.


\subsecput{flow}{Network flow}
\note{Shows how we simplify locking discipline.}

Consider a serial program for computing network flow (see, for
example, \cite[Chapter 26]{CormenLeRi01}).  The inner loop of the code
pushes flow across an edge by increasing the ``excess flow'' on one
vertex and decreasing it by the same amount on another vertex.  One
might see the following Java code: \par {\footnotesize\samepage
\begin{verbatim}
void pushFlow(Vertex v1, Vertex v2, double flow) {
  v1.excess += flow; /* Move excess flow from v1 */
  v2.excess -= flow; /* to v2.                   */
}
\end{verbatim}
}

To parallelize this code, one must preclude multiple threads from
modifying the excess flow on those two vertices at the same time.
Locks provide one way to enforce this mutual exclusion: 
\par {\footnotesize\samepage
\begin{verbatim}
void pushFlow(Vertex v1, Vertex v2, double f) {
  Object lock1, lock2;
  if (v1.id < v2.id) {       /* Avoid deadlock. */
    lock1 = v1; lock2 = v2;
  } else {
    lock1 = v2; lock2 = v1;
  }
  synchronized(lock1) {
    synchronized(lock2) {
      v1.excess += f; /* Move excess flow from v1 */
      v2.excess -= f; /* to v2.                   */
    } /* unlock lock2 */
  } /* unlock lock1 */
}
\end{verbatim}
}

This code is surprisingly complicated and slow compared to the
original.  Space for each object's lock must be reserved.
To avoid deadlock, the code must acquire the locks in
a consistent linear order, resulting in an unpredictable branch in the
code.
In Java, we require the programmer to insert an \texttt{id} field into
each vertex to maintain this order.
Moreover, the time required to acquire the locks is
an order of magnitude larger on today's processors than the time to
modify the excess flow.
\note{Using FLEX, the locking code is over 11x
  slower than the no-locks code.  With Sun's JVM, this overhead falls
  to about 1.7x, because Sun is wicked smart about their lock
  implementations.}
What's more, all of this overhead is rarely
needed!  For a graph with thousands or millions of vertices, the
number of threads operating on the graph is likely to be less than a
hundred.  Consequently, the chances are quite small that two different
threads actually conflict.  Without the locks to implement mutual
exclusion, however, the program would fail intermittently.

Software transactions (and some language support) allow the
programmer to parallelize the original code using an \texttt{atomic}
keyword to indicate that the code block should appear to execute
atomically: 
\par {\footnotesize\samepage
\begin{verbatim}
void pushFlow(Vertex v1, Vertex v2, double flow) {
  atomic { /* Transaction begin. */
    v1.excess += flow; /* Move excess flow from v1 */
    v2.excess -= flow; /* to v2.                   */
  } /* Transaction end. */
}
\end{verbatim}
} 

This simple example is not meant to be a definitive defense of
transactional memory.  Rather, its aim is to show the reader how
convoluted locking protocol can be compared with transactions.  As we
shall see in \secref{eval},\note{Sure would be nice if we could make
good on this promise.} BTM can implement this code with essentially no
overhead, allowing excess flows to be updated safely by multiple
parallel threads. 


\subsecput{stringbuffer}{The \texttt{StringBuffer} class}
\note{Shows how we fix race-conditions.}
\par {\footnotesize
\samepage
\begin{verbatim}
public final class StringBuffer ... {
  private char value[];
  private int count;
  ... 
  public synchronized
  StringBuffer append(StringBuffer sb) {
    ...
    int len = sb.length(); // len may be stale.
    int newcount = count + len; 
    if (newcount > value.length)
      expandCapacity(newcount);
    // next statement may use stale len
    sb.getChars(0, len, value, count);
    count = newcount;
    return this;
  }
  public synchronized int length() { return count; }
  public synchronized void getChars(...) { ... }
}
\end{verbatim}
}

This\note{This text is mostly just to get the ideas down for now.}  
source code is from the \texttt{java.lang.StringBuffer} class in
version 1.4.2 of Sun's Java standard library.  \texttt{StringBuffer}
represents a mutable string of characters.  The documentation for the
class states:
\par{\footnotesize
\begin{quote}
 String buffers are safe for use by multiple threads. The methods 
 are synchronized where necessary so that all the operations on any 
 particular instance behave as if they occur in some serial order 
 that is consistent with the order of the method calls made by each of 
 the individual threads involved. 
\end{quote}
}

The \texttt{synchronized} modified on the \texttt{append()} method is
meant to ensure that this method operates atomically.  But it does
not: another thread can change the parameter \texttt{sb} between the
call to \texttt{sb.length()} and the call to \texttt{sb.getChars()},
causing incorrect data to be appended or a
\texttt{StringIndexOutOfBoundException} to be thrown.  The calls to
\texttt{sb.length()} and \texttt{sb.getChars()} are individually
atomic, but they do not compose to form an atomic implementation of
\texttt{append()}.  An \texttt{atomic} declaration (or running under
our transactions implementation) would fix this programmer error.
[This bug was found by \cite{FlanaganQa03}.]
\note{What about just saying \texttt{public synchronzied Append} ? -Bradley}

Further, if the synchronization in this method is implemented with
transactions, note that the call to \texttt{sb.getChars()} means that
the size of the transaction for this method will grow like the length
of the parameter~\texttt{sb}.  In other words, the transaction can be
made arbitrarily large by increasing the length of \texttt{sb}; or,
equivalently, there is no bound on transaction size without a bound on
the size of the string~\texttt{sb}.  This means that boundless
transactions must be supported if this method is operate on arbitrary
inputs.

\subsecput{destruct}{Destructive traversal}
\note{Shows transaction-as-exception-handling, fault-tolerance, and
back-tracking.}
\cite{SchorrWa67}

 \par {\footnotesize\samepage
\begin{verbatim}
void traverse(List l) {
  List last = null, t;
  
  /* zip through the list, reversing links */
  do {
    preVisit(l); // visit the node
    t = l.next;
    l.next = last;
    last = l;
    l = t;
  } while (l!=null);

  /* now zip backwards, fixing up the links. */
  l = last;
  do {
    t = l.next; // "previous"
    l.next = last; // "next"
    postVisit(l); // visit the node.
    last = l;
    l = t;
  } while (l!=null);
}
\end{verbatim}
}

 \par {\footnotesize\samepage
\begin{verbatim}
try {
  try {
    traverse(list);
  } catch (Throwable t) {
    fail; // explicit xaction abort
  }
} else { // try-else construct
  throw new Error();
}
\end{verbatim}
}

\subsecput{backtrack}{Backtracking}
Prolog-esque example?

%\subsecput{optcon}{Optimistic Concurrency} % not in this paper.

\secput{related}{Related work}
\subsecput{nb-sync}{Non-blocking synchronization}

\begin{figure}
{\ttfamily
\begin{tabular}{l@{\hspace{1in}}l}
Writer:        &Reader:                 \\
~~v1++;        &~~do \{                 \\
~~{\it write}; &~~~~temp = v2;          \\
~~v2++;        &~~~~{\it read};         \\
               &~~\} while (temp != v1);\\
\end{tabular}
}
\caption{Lamport's single-writer multiple-reader non-blocking
    synchronization algoritm \cite{Lamport77}.}
% writes and reads to v1/v2 can be non-atomic if the subreads/subwrites
% are done in the correct order; see original reference for details.
\label{fig:lamport}
\end{figure}
Lamport presented the first alternative to synchronization via mutual
exclusion in \cite{Lamport77}, for a limited situation involving a single
writer and multiple readers.  Lamport's technique, presented in
Figure~\ref{fig:lamport}, relies on reading
guard elements $v_1$ and $v_2$ in an order opposite to that in which
they are written,
guaranteeing that a consistent data snapshot can be recognized.  The
writer always completes its part of the algorithm in a constant number
of steps; readers are guaranteed to complete only in the absence of
concurrent writes.

Herlihy formalized \emph{wait-free} implementations of
concurrent data objects in \cite{Herlihy88}.  A wait-free implementation
guarantees that any process can complete any operation in a finite
number of steps, regardless of the activities of other processes.
Lamport's algorithm, for example, is not wait-free
because readers can be delayed indefinitely.  Wait-free algorithms
typically involve ``recursive helping,'' whereby active processes can
complete operations on behalf of stalled processes, ensuring that all
operations are eventually completed.

Massalin and Pu introduced the term \emph{lock-free} to describe 
algorithms with weaker progress guarantees.
A lock-free implementation guarantees only that \emph{some}
process will complete in a finite number of steps
\cite{MassalinPu91}.  Unlike a wait-free implementation,
lock-freedom allows starvation.  Since other simple techniques can be
layered to prevent starvation (for example, exponential backoff),
simple lock-free implementations are usually seen as worthwhile practical
alternatives to more complex wait-free implementations.

An even weaker criterion, \emph{obstruction-freedom}, was introduced
by Herlihy, Luchangco, and Moir in \cite{HerlihyLuMo03}.
Obstruction-freedom only guarantees progress for threads executing in
isolation; that is, although other threads may have partially
completed operations, no other thread may take a step until the
isolated thread completes.  Obstruction-freedom not only allows
starvation of a particular thread, it allows contention among threads
to halt all progress in all threads
indefinitely.  External mechanisms are used to reduce contention
(thus, achieve progress) including backoff, queueing, or timestamping.

Revisiting Lamport's algorithm, we conclude it is neither lock-free
nor obstruction-free,
because halting the writer between the guard increments will prevent
readers from ever getting a consistent snapshot.

We will use the term \emph{non-blocking} to describe
generally any synchronization mechanism which doesn't rely on mutual
exclusion or locking, including wait-free, lock-free,
and obstruction-free implementations.
We will be concerned mainly with lock-free algorithms.%
\footnote{Note that some authors use ``non-blocking'' and
  ``lock-free'' as synonyms, usually meaning what we here call
  \emph{lock-free}.  Others exchange our definitions for ``lock-free''
  and ``non-blocking'', using lock-free as a generic term and non-blocking
  to describe a specific class of implementations.  As there is
  variation in the field, we choose to use the parallel construction
  \emph{wait-free}, \emph{lock-free}, and \emph{obstruction-free} for
  our three specific progress criteria, and the dissimilar
  \emph{non-blocking} for the general class.}

\subsecput{nb-adv}{Advantages over mutual exclusion}
Non-blocking synchronization offers a number of advantages over mutual
exclusion within critical regions.  Foremost for the concerns of this paper is
fault-tolerance:  a process which fails while holding a lock within a
critical region can prevent all other non-failing processes from
ever making progress.  In an operating system context, this means that
thread termination must be done very carefully to avoid inadvertently
killing a process within a critical region.  If a thread dies while
holding a user-mode lock, all other threads in its address space may
deadlock; if it dies while holding a kernel lock, the whole system may
crash.  Although one can use external means to recognize orphaned
locks and release them, it is in general not possible to restore the
locked data structures to a consistent state after such a failure.
Non-blocking synchronization offers a graceful means out of
these troubles, as non-progress or failure of any one thread will not
affect the progress or consistency of other threads or the system.
These fault-tolerant properties are even more relevant in
distributed systems where entire nodes may fail without warning.

Non-blocking synchronization offers performance benefits as well.
Even in a failure-free system, page faults, cache misses, context
switches, I/O, and other unpredictable events may result in delays to the
entire system when mutual exclusion is used; non-blocking
synchronization allows undelayed processes or processors to continue
to make progress.  In loosely coupled asynchronous systems such
unexpected delays are the norm, rather than the exception.
%On parallel systems critical regions may cause unexpected serialization.

Real-time systems have other problems with mutual exclusion.
A low-priority task which 
acquires a lock and is then delayed may hold up higher-priority tasks
which contain critical regions protected with the same lock.
This situation is called
\emph{priority inversion}, and is responsible for a number of
high-profile system failures, including whole-system resets during the Mars
Pathfinder mission \cite{Jones97}.  Non-blocking
synchronization can guarantee that the high-priority task makes
progress.\footnote{Note that the progress guarantees made are
  different for wait-free, lock-free, and obstruction-free
  algorithms.  For example, priority-inversion can still occur on
  obstruction-free implementations if a lower priority thread contends
  persistently for the resource.  On a uniprocessor, a valid solution
  in this case might be to simply not interleave executions of tasks
  with differing priorities; obstruction-freedom then guarantees that
  the high-priority task ``in isolation'' will make progress.}

\subsecput{efficiency}{Efficiency}
Herlihy presented the first \emph{universal} method for wait-free
concurrent implementation of an arbitrary sequential object
\cite{Herlihy88,Herlihy91}.  This original method was based on
a \emph{fetch-and-cons} primitive, which atomically places
an item on the head of a list and returns the list of items following
it; all concurrent primitives capable of solving the
$n$-process consensus problem---\emph{universal} primitives---were
shown powerful enough to implement \emph{fetch-and-cons}.
In Herlihy's method, 
every sequential operation is translated into two steps.  In the first,
\emph{fetch-and-cons} is used to place the name and arguments of the
operation to be performed
at the head of a list, returning the other operations on the list.
Since the state
of a deterministic object is completely determined by the history of
operations performed on it, applying the operations returned
in order from last to first is sufficient to locally reconstruct the
object state 
prior to our operation.
We then use the prior state to compute the result of our operation
without requiring further synchronization with the other processes.

This first universal method was not very practical, a shortcoming
which Herlihy soon addressed \cite{Herlihy93}.  In addition, his revised universal
method can be made lock-free, rather than wait-free, resulting in
improved performance.  In the lock-free version of this method,
objects contain a shared variable
holding a pointer to their current state.  Processes begin by loading
the current state pointer and then copying the referenced state to a
local copy.  The sequential operation is performed on the
copy, and then if the object's shared state pointer is unchanged from
its initial load it is atomically swung to point at the updated state.

Herlihy called this the ``small object protocol'' because the object
copying overhead is prohibitive unless the object is small enough to
be copied efficiently (in, say, $O(1)$ time).  He also presented a
``large object protocol'' which requires the programmer to
manually break the object into small blocks, after which the small
object protocol can be employed.  This trouble with large objects is
common to many non-blocking implementations; our solution is presented
in \secref{proposal}.

Barnes provided the first universal non-blocking implementation
method which avoids object copying \cite{Barnes93}.  He eliminates the
need to store ``old'' object
state in case of operation failure by having all threads cooperate to
apply operations.  For example, if the first processor begins an operation
and then halts, another processor will complete the first's operation
before applying its own.  Barnes proposes to accomplish the
cooperation by creating a parallel state machine for each operation,
so that each thread can independently try to advance the machine from state
to state and thus advance incomplete operations.%
\footnote{It is interesting to note that Barnes' cooperative method
  for non-blocking 
  situation plays out in a real-time system very similarly to priority
  inheritance for locking synchronization.}
Although this avoids
copying state, the lock-step cooperative process is extremely
cumbersome and does not appear to have ever been implemented.
Furthermore, it does not protect against errors in the implementation
of the operations, which could cause \emph{every} thread to fail in turn
as one by one they attempt to execute a buggy operation.

Alemany and Felten \cite{AlemanyFe92} identified two factors hindering the
performance of non-blocking algorithms to date: resources wasted by operations
that fail, and the cost of data copying.  Unfortunately, they
proceeded to
``solve'' these problems by ignoring short delays and failures and
using operating system support to handle delays caused by
context switches, page faults, and
I/O operations.  This works in some situations, but obviously suffers
from a bootstrapping problem as the means to implement an operation system.

Although lock-free implementations are usually assumed to be more
efficient that wait-free implementations, LaMarca \cite{LaMarca94}
showed experimental evidence that Herlihy's simple
wait-free protocol scales very well on parallel machines.
When more than about twenty threads are involved, the wait-free
protocol becomes
faster than Herlihy's lock-free small-object protocol, three OS-aided
protocols of LaMarca and Alemany and Felten, and a
\emph{test-and-Compare\&Swap} spin-lock.

% Afek et al have a somewhat complicated improved wait-free method.

% Transactional memories?
\subsecput{tm}{Transactional Memory Systems}

Several software transaction systems have been proposed.  Some constrain the
programmer and make transactions difficult to use.  All have
relatively high overheads, which make transactions unattractive for
uniprocessor and small SMP systems. [Once the number of processors is
large enough, the increased parallelism which can be provided by
optimistic transactions may cancel out the performance penalty of
their use.]

The first proposal for software transactional memory was proposed by
Shavit and Touitou \cite{ShavitTo95}; their system requires that all
input and output locations touched by a transaction be known in
advance, which limits its application.  It performs at least 10
fetches and 4 stores per location accessed (not counting the loads and
stores directly required by the computation).  The benchmarks
presented were collected on a 64-processor machine; no performance
numbers were shown for less than 10 processors.

Rudys and Wallach \cite{RudysWa02} proposed a copying-based
transaction system to allow rollback of hostile codelets.
They shows an order of magnitude slowdown for field and array
accessed, and 6x to 23x slowdown on their benchmarks.

Herlihy, Luchango, Moss, and Scherer's scheme \cite{HerlihyLuMoSc03}
allows transactions to touch a dynamic set of memory locations;
however the user still has to explicitly \emph{open} every object touched
before it can be used in a transaction.  This implementation is based
on object copying, and so has poor performance for large objects and
arrays.  Not including work necessary to copy objects involved in
writes, they require $O(R(R+W))$ work to open R objects for reading
and W objects for writing, which is quadratic in the number of objects
involved in the transaction.   A list insertion benchmark which they
present shows 9x slowdown over a locking scheme, they beat the locking
implementation when more than 5-10 processors are active.  They
present benchmark data with up to 576 threads on 72 processors.

Harris and Fraser built a software transaction system on a flat
word-oriented transactional memory abstraction \cite{HarrisFr03},
roughly similar to simulating Herlihy's original hardware
transactional memory proposal in software.  This avoids problems with
large objects.  Performing $m$ memory operations touching $l$ distinct
locations costs at least $m+l$ extra reads and $l+1$ CAS operations, in
addition to the reads and writes required by the computation.
They appear to execute about twice as slowly as a locking
implementation on some microbenchmarks.  They benchmark on a
4-processor as well as a 106-processor machine; their crossover point
(at which the blocking overhead of locks matches the software
transaction overhead) is around 4 processors.

Programmers will be loathe to use transactions to synchronize their
code when it results in their code running more slowly on the uniprocessor
and small-SMP systems which are most common today.

Programming with small non-blocking synchronization primitives alone
is like using overlays instead of virtual memory.

Our SPECjvm98 and Linux case studies indicate that
transactional memory operations can be extremely common when
transactions are made easy to use and natural to express.  Programmers
can be expected to use both very small and very large transactions
when they are able, and transactions are used by the operating system
and programming libraries to provide synchronization and safety
properties.  Implementing transactional memory in hardware allows the
overheads to be low (\textbf{cite figures here}), which is appropriate
when transactional operations are common.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\punt{
Ideas:
 - virtual memory/overlays comparison
 - what is expensive in software versions XBEGIN?  XLD?
 - basic ideas of software impls
   also, how they handle overflow & other limits.
 - honest about what we're showing.
   here's what happens if you explore the hardware avenue.
   sw technology problems become solved hw problems.
 - k-compare-and-swap insufficient?
 - say when we couldn't find performance numbers.
   performance #s in terms of reads/writes.
 - now that we know what transactions look like,
   we want to use BTM\@.
 - use categories?
 - STM systems always use ridiculously large # processors for benchmarks.

The software just ain't good enough!
\begin{itemize}
\item Argue that SW isn't good
\item Also, lots of transactional loads and stores
\end{itemize}

Software transactional memory implementations:
% FOR EACH: PERFORMANCE.  IDEA.  LIMITS.
\begin{itemize}
\item Shavit and Touitou \cite{ShavitTo95}: static transactions only;
  that is, input and output locations must be known in advance.
  Performance?
  Non-blocking, based on LL/SC. k-word compare\&swap operation.
  ``outperforms Herlihy's translation method for sufficiently large
  numbers of processors.''  Alleges Herlihy/Moss is blocking?
  I think this is actually the case.  Our scheme is not.
  Elsewhere, ``but not non-blocking, since a single process which is
  repeatedly swapped during the execution of a transaction will never
  terminate successfully''.  They define non-blocking as ``repeated
  execution of some transaction by a process implies that some process
  (not necessarily the same one and with a possibly different
  transaction) will terminate successfully after a finite number of
  attempts in the whole system.''
  ``static transactions, that is, transactions which access a
  pre-determined sequence of locations.  This class includes most of
  the known and proposed synchronization primitives in the
  literature.''
  Requires one ownership record for every location in the memory.
  no limits to size of trans, since we allocate one ownership record
  per memory location.

  Performance: *at least* 5 fetches, 5 LL, and 4SC per location
  accessed.

  64 processors; no numbers presented for less than 10 processors.

  acquire/release/agree are expensive.

\item Rudys and Wallach \cite{RudysWa02}: transactions for security.
  6x to 23x slowdown.  Creates 'backup' objects.  Predictably bad
  for large objects.  10x slowdown for field/array accesses.
  only uniprocessor.

\item Herlihy et al \cite{HerlihyLuMoSc03}: dynamic transactions, but
  copies entire object and so works for small objects only.  Basically
  Herlihy's method.  You must explicitly \emph{open} every object
  accessed, at which time conflict-checking is done.
  O((R+W)R) work to open R objects for reading and W objects for
  writing, plus cost to copy W objects once.  Concretely: for simple
  list insert/delete, locks perform 768 ops/ms, and best DSTM scheme
  is 80 ops/ms.   So over 9x slowdown.
  Up to 576 threads on 72 processors.

  open and commit are the expensive operations.
\item Harris and Fraser \cite{HarrisFr03}: atomic sections, word-based
  scheme.  no safety for non-transactional operations which might
  touch transactional locations.  transactional operation at least 2x
  slower than normal op.
  XBEGIN = CAS + STORE
  XFAIL = STORE
  Tread = 2 FETCH (best case)
  Twrite = 1 FETCH + (1 FETCH and 2 STOREs, on same cache line)
  XEND = > (1 FETCH + 1 CAS) for each distinct word read or written.

  overhead of a committing transaction performing r reads and w writes
  to 'l' distinct locations.
  is at least (2r+2w+(l)) reads (2w+1) writes, and (l+1) CAS
  operations.  2(r+w)+l reads, 2w+1 writes, and (l+1) CAS operations.

m memory operations to l locations does m+l extra reads and l+1 CAS operations.

  4 proc; also 48 proc of 106-processor machine.  I think the
  crossover point is around 4 proc.

  COMMIT is the most expensive operation.
\end{itemize}
}


\secput{efficient}{Designing Efficient Transactions}
\subsection{Scales}
\subsection{Object expansion}
\epsfigput{bloat}{Application slowdown with increasing object bloat.}
\subsection{Reads vs. Writes}
\subsection{Large objects}
\subsection{Subsumption?}

\secput{algo}{Algorithms}

\secput{opt}{Optimizations}
\subsection{Hoisting}
\subsection{Check-combining}
\subsection{Global field analysis}
\subsection{Escape analysis}

\secput{integrate}{Integration}
% JNI tricks, etc.

\secput{eval}{Evaluation}

\begin{figure*}
\footnotesize
\begin{center}
\begin{tabular}{lr|rr|rr}
Benchmark                 &  Base       & Locks         & Trans              & Time                     & Overflow       \\
                          &             & time          & time               & in trans              & overhead       \\
                          &  (cycles)   & \multicolumn{2}{c|}{(\% of Base)}   & \multicolumn{2}{c}{(\% of Trans time)}                 \\ \hline
\texttt{200\_check}       &   8,062,716 & 124.0\%       &  101.7\%           & 32.9\%           & 0.00366\%      \\
\texttt{202\_jess}        &  75,012,964 & 140.9\%       &  107.1\%           & 59.3\%           & 0.00761\%      \\
\texttt{209\_db}          &  11,784,220 & 142.4\%       &  105.1\%           & 53.9\%           & 0              \\
\texttt{213\_javac}       &  30,688,574 & 169.9\%       & 1469.5\%           & 99.0\%           & 93.0\%         \\
\texttt{222\_mpegaudio}&  98,961,244    & 100.3\%       &   99.6\%           & 0.817\%          & 0              \\
\texttt{228\_jack}        & 261,441,564 & 175.3\%       &  104.4\%           & 32.2\%           & 0.00260\%      \\
\end{tabular}
\end{center}
\caption{SPECjvm98 performance and overheads on a 1-processor UVSIM simulation. The {\em Base }
 time uses no synchronization.  The {\em Locks time} is the time using
 locks as a percentage of the base.  The {\em Trans time} is the time using BTM
 transactions as a perdentage of the base. The
 {\em time in Trans} is the fraction of the {\em Trans time} actualy
 spent running a transaction.  The {\em
 Overflow overhead} column is fraction of the {\em Trans time}
 spent handling overflows.}
\label{fig:specperf}
\end{figure*}

We found that using BTM transactions in the SPECjvm98 benchmarks
results in very little overhead compared to using locks. We compiled
the benchmarks to use BTM transactions using our modified Java
compiler.  The \texttt{201\_compress} and \texttt{205\_raytrace}
benchmarks were omitted due to unresolved bugs in the base UVSIM
simulator, and similarly \texttt{libpthread} deficiencies prevented
the execution of \texttt{227\_mtrt}.
The remaining benchmarks were run on one processor in UVSIM to compare the
performance overheads associated with BTM transactions compared to
locks.  As shown in \figref{specperf}, there is a significant amount
of overhead associated with locking (more than 75\% over the base case
in {\texttt{228\_jack})}. %Put that {} around the texttt to prevent the jack from being separated from the )
For BTM, there are two main sources of
overhead: The transactions may abort, wasting the cycles spent
computing, and also executing the branch to the abort handler.  For
these java programs, the transactions never abort because they are
single-threaded programs.  The second overhead is the cost of handling
overflows.

The only application that has a significant transaction overhead is
\texttt{javac}, which runs 14 times slower under BTM than without any
mutual exclusion.  Virtually all of the overhead comes from handling
overflows, which is not surprising since the entire compilation is
enclosed in one large transaction.  The transaction touches 253,547
cache lines, and there are only 16K cache lines in the L2 cache,
meaning that the overflow memory contains at least 237K cache lines.
So even though only 0.5\% of the xops miss, those few misses incur
huge search costs, as the finite state machine looks through the
linear array of overflowed cache lines.  We intend to look into using
a better data structure, such as a hash table.  Though there is a
significant performance decrease in this case, BTM still ensures the
application runs correctly. Namely, the programmer wanted the entire
compile process to occur as one atomic action so BTM will ensure that
is the case.  It may be the case that \texttt{javac} really cannot be
parallelized, in which case locks may be the best choice.  But if the
programmer wishes to parallelize \texttt{javac}, she may be find it
easier to break up the large transaction than to break up a locked
block.

\secput{concl}{Conclusions}

\renewcommand{\baselinestretch}{1}
\bibliographystyle{plain}
\bibliography{xaction}

\appendix
\secput{extra}{Text with nowhere else to go}

\begin{figure*}
\footnotesize
\begin{center}
\begin{tabular}{l@{}rrrrrrrrr}
program  & input& total      & \% cache  & xactions    & oversized & xops        & xmiss & overflow & biggest \\ 
         & size & memory ops & misses &             & xactions  &   \%        &   \%    & \% & xaction\\ \hline
\texttt{make\_linux}  &  & 315,776,028 & 0.56\%  &  6,964,277  & 3368     & 41.0\%  &  0.017\%  &            &         8144   \\
\texttt{dbench}      &  & 100,928,220 & 0.43\%  &  1,863,426  &   88     & 49.5\%  &  0.001\%  &            &         7047   \\

 \hline \texttt{201\_compress } &   1 & 229,332,212 &  0.10\% &        524 &   0 &  0.0\% &     0   &      0   &     1,335 \\
                                &  10 & 264,389,092 &  0.10\% &        662 &   0 &  0.0\% &     0   &      0   &     1,335 \\
                               & 100 & 2,981,777,890 &  0.10\% &      2,272 &   0 &  0.0\% &     0   &      0   &     1,352 \\
 \hline \texttt{202\_jess     } &   1 &   1,972,479 &  3.13\% &     82,103 &   0 & 43.3\% &     0   &      0   &     3,586 \\
                                &  10 &  27,371,450 &  0.73\% &    104,636 &   0 &  3.4\% &     0   &      0   &     4,624 \\
                                & 100 & 405,153,255 &  2.71\% &  4,892,829 &   0 &  9.1\% &     0   &      0   &     4,826 \\
 \hline \texttt{205\_raytrace } &   1 &  14,535,905 &  1.83\% &      1,125 &   1 & 49.6\% & 0.648\% & 0.0889\% &   875,925 \\
                                &  10 &  36,655,846 &  1.33\% &      1,922 &   1 & 19.7\% & 0.257\% & 0.0520\% &   875,925 \\
                                & 100 & 420,005,763 &  1.65\% &      4,177 &   1 &  1.7\% & 0.022\% & 0.0239\% &   875,925 \\
 \hline \texttt{209\_db       } &   1 &     393,455 &  2.01\% &     14,191 &   0 & 45.8\% &     0   &      0   &     5,112 \\
                                &  10 &  15,814,285 &  5.66\% &    874,319 &   2 & 23.3\% & 0.060\% & 0.0001\% &    51,292 \\
                                & 100 & 848,082,597 & 10.14\% & 45,222,742 & 288 & 23.0\% & 0.350\% & 0.0005\% &   148,374 \\
 \hline \texttt{213\_javac    } &   1 &   1,605,330 &  1.88\% &        460 &   1 & 89.5\% & 0.517\% & 0.2087\% &   253,547 \\
                                &  10 &  14,968,130 &  1.65\% &        476 &   1 & 98.8\% & 1.403\% & 0.2101\% & 1,547,922 \\
                                & 100 & 472,416,129 &  1.78\% &        668 &   4 & 99.9\% & 1.652\% & 0.5988\% & 8,928,356 \\
 \hline \texttt{222\_mpegaudio} &   1 &  26,551,440 &  0.03\% &      1,049 &   0 &  0.1\% &     0   &      0   &     1,340 \\
                                &  10 & 282,263,132 &  0.00\% &      1,675 &   0 &  0.0\% &     0   &      0   &     1,339 \\
                                & 100 & \multicolumn{5}{c}{(\texttt{222\_mpegaudio}/100 simulation not yet complete)} \\
\end{tabular}
\end{center}
\caption{Experimental results for transactifying the Linux kernel and
Java.  For Java the \textit{input size} can be run in three ``sizes'':
1\%, 10\%, and 100\% of the full input size.    For the percentages,
we write ``0'' for numbers that are exactly zero, and a ``zero''
percentage, such as ``0.0\%,'' for small nonzero values.}
\label{fig:perfnums}
\end{figure*}

\section{Analyses and Implementation}

\secput{proposal}{A proposal for a non-blocking Java OS}
\begin{figure}[t]
{\it
\begin{tabular}{l}
{\bf const} myDirectory == {\bf object} oneEntryDirectory\\
~~{\bf export} Store, Lookup\\
~~{\bf monitor}\\
~~~~{\bf var} name : String\\
~~~~{\bf var} AnObject : Any\\
\\
~~~~{\bf operation} Store [ n : String, o : Any ]\\
~~~~~~name $\gets$ n\\
~~~~~~AnObject $\gets$ o\\
~~~~{\bf end} Store
\\
~~~~{\bf function} Lookup [ n : String ] $\to$ [ o : Any ]\\
~~~~~~{\bf if} n = name\\
~~~~~~~~{\bf then} o $\gets$ AnObject\\
~~~~~~~~{\bf else} o $\gets$ {\bf nil}\\
~~~~~~{\bf end if}\\
~~~~{\bf end} Lookup\\
\\
~~~~{\bf initially}\\
~~~~~~name $\gets$ {\bf nil}\\
~~~~~~AnObject $\gets$ {\bf nil}\\
~~~~{\bf end initially}\\
\\
~~{\bf end monitor}\\
{\bf end} oneEntryDirectory
\end{tabular}
}
\caption{A directory object in Emerald, from \cite{BlackHuJuLe86},
  illustrating the use of monitor synchronization.}
\label{fig:emerald-dir}
\end{figure}

\begin{figure}[t]
{\ttfamily\small\vspace{.25in}
\begin{tabular}{l}
class Account \{\\
\\
~~int balance = 0;\\
\\
~~{\bf atomic} int deposit(int amt) \{\\
~~~~int t = this.balance;\\
~~~~t = t + amt;\\
~~~~this.balance = t;\\
~~~~return t;\\
~~\}\\
\\
~~{\bf atomic} int readBalance() \{\\
~~~~return this.balance;\\
~~\}\\
\\
~~{\bf atomic} int withdraw(int amt) \{\\
~~~~int t = this.balance;\\
~~~~t = t - amt;\\
~~~~this.balance = t;\\
~~~~return t;\\
~~\}\\
\\
\}\\
\end{tabular}
}\vspace{.2in}
\caption{A simple bank account object, adapted from \cite{FlanaganQa03},
  illustrating the use of the \atomic modifier.}
\label{fig:atomic}
\end{figure}
Language-based operating systems promise to allow even further
reduction in micro-kernel design by removing even address spaces from the
kernel.  Further, by providing correct and universal synchronization
in the language, we hope to eliminate the difficulty of writing
specific correct lock-free data structures, illustrated by Massalin
and Pu's errors in Section~\ref{sec:bugs}.

To that end, we propose some slight modifications to the Java
language to better support its use for writing operating systems.
We concentrate on synchronization in the language, eliding as
orthogonal issues related to direct access to hardware.  We expect
that slight extensions similar to those in Lisaac \cite{SonntagCo02} and
JEPES \cite{SchultzBuChKn03} will be sufficient to support interrupt handlers
and memory-mapped I/O.  We then present an efficient implementation
technique for the synchronization primitives in the language, showing
first how to synchronize single encapsulated objects, then operations
on multiple objects, and finally present the lock-free functional
array implementation which allows efficient large object
synchronization.

Our implementation will use the same DCAS primitive used in the Cache
Kernel and in Synthesis.  This primitive can be emulated using
architecture extensions to the popular LL/SC operations,
or in software with OS support using Bershad's technique
\cite{GreenwaldCh96,Bershad93}.

\subsection{Synchronization in the language}
The Emerald system \cite{BlackHuJuLe86,JulSt91} introduced
\emph{monitored objects} for synchronization.  Emerald code to
implement a simple directory object is shown in
Figure~\ref{fig:emerald-dir}.  Each object is associated with
Hoare-style monitor, which provides mutual exclusion and process
signalling.  Each Emerald object is divided into a monitored part and
a non-monitored part.  Variables declared in the monitored part are
shared, and access to them from methods in the non-monitored part is
prohibited---although non-monitored methods may call monitored methods
to effect the access.  Methods in the monitored part acquire the monitor lock
associated with the receiver object before entry and release it on
exit, providing for mutual exclusion and safe update of the shared
variables.  Monitored objects naturally integrate synchronization into
the object model.

Unlike Emerald monitored objects, where methods can only acquire the
monitor of their receiver and where restricted access to shared
variables is enforced by the compiler, Java implements a loose
variant where any monitor may be explicitly acquired and no shared
variable protection exists.  As a default, however, Java methods
declared with the {\tt synchronized} keyword behave like Emerald
monitored methods,
ensuring that the monitor lock of their receiver is held during execution.

Java's synchronization primitives arguably allow for more efficient
concurrent code than Emerald's---for example, Java objects can use
multiple locks to
protect disjoint sets of fields, and coarse-grain locks can be used
which protect multiple objects---but Java is also more prone to programmer
error.  However, even Emerald's restrictive
monitored objects are not sufficient to prevent data races.  As a
simple example, imagine that an object provided two monitored methods
{\tt read} and {\tt write} which accessed a shared variable.
Non-monitored code can call {\tt read}, increment the value returned,
and then call {\tt write}, creating a classic race condition scenario.
The atomicity of the parts is not sufficient to guarantee atomicity of
the whole \cite{FlanaganQa03}.

This suggests that a better model for synchronization in
object-oriented systems is \emph{atomicity}.  Figure~\ref{fig:atomic}
shows Java extended with an \atomic keyword to implement an
object representing a bank account.  Rather than explicitly
synchronizing on locks, we simply require that the methods marked
\atomic execute atomically with respect to other threads in the
system; that is, that every execution of the program computes the same
result as some execution where all atomic methods were run \emph{in
  isolation} at a certain point in time between their invocation and return.
This point is called the \emph{linearization point}.  Note that
atomic methods invoked directly or indirectly from an atomic
method are subsumed by it: if the outermost method appears atomic,
then by definition all inner method invocations will also appear atomic.
Flanagan and Qadeer provide a more formal semantics in \cite{FlanaganQa03}.
Atomic methods can be analyzed using sequential reasoning techniques, which
significantly simplifies reasoning about program correctness.

Atomic methods can be implemented using locks.  A simple if deadlock-prone 
implementation would simply acquire a single global lock during
the execution of every atomic method.  Flanagan and Qadeer
\cite{FlanaganQa03} present a more sophisticated technique which proves that
a given implementation using standard Java monitors correctly
guarantees method atomicity.
We will use non-blocking synchronization to implement atomic methods.

\subsection{A simple implementation of functional arrays}
Our atomic method implementation will use \emph{functional arrays} as
a building block.  Functional arrays are \emph{persistent}; that is,
after an element is updated both the new and the old contents of the
array are available for use.  Since arrays are simply maps from
integers (indexes) to values; any functional map datatype (for
example, a functional balanced tree) can be used to implement
functional arrays.

However, the distinguishing characteristic of an imperative array is its
theoretical complexity: $O(1)$ access or update of any element.
Implementing functional arrays with a functional balanced tree yields
$O(\lg n)$ worst-case access or update.
% We will address this
% discrepancy in Section~\ref{sec:large-obj}.

For concreteness, functional arrays have the following three
operations defined:
\begin{itemize}
\item $\funcname{FA-Create}(n)$: Return an array of size $n$.  The contents of
  the array are initialized to zero.
\item $\funcname{FA-Update}(A_j, i, v)$: Return an array $A_{j'}$ which is
  functionally identical to array $A_j$ except that $A_{j'}(i)=v$.
  Array $A_j$ is not destroyed and can be accessed further.
\item $\funcname{FA-Read}(A_j, i)$: Return $A_j(i)$.
\end{itemize}
We allow any of these operations to \emph{fail}.  Failed operations
can be safely retried, as all operations are idempotent by definition.

For the moment, consider the following na{\"\i}ve implementation:
\begin{itemize}
\item $\funcname{FA-Create}(n)$: Return an ordinary imperative array of size
  $n$.
\item $\funcname{FA-Update}(A_j, i, v)$: Create a new imperative array
  $A_{j'}$ and copy the contents of $A_j$ to $A_{j'}$.  Return $A_{j'}$.
\item $\funcname{FA-Read}(A_j, i)$: Return $A_j[i]$.
\end{itemize}
This implementation has $O(1)$ read and $O(n)$ update, so it matches
the performance of imperative arrays only when $n=O(1)$.  We will
therefore call these \emph{small object functional arrays}.  Operations
in this implementation never fail.  Every operation is non-blocking
and no synchronization is necessary, since the imperative arrays are
never mutated after they are created.

\subsection{A single-object protocol}
\begin{figure}\centering
\includegraphics[width=3.25in,clip=true]{Figures/nb-single-obj.eps}
\caption{Implementing non-blocking single-object concurrent operations
  with functional arrays.}
\label{fig:single-o}
\end{figure}
Given a non-blocking implementation of functional arrays, we can
construct an implementation of \atomic for single objects.  In
this implementation, fields of at most one object may be referenced
during the execution of the atomic method.

We will consider the following two operations on objects:
\begin{itemize}
\item $\funcname{Read}(o, f)$: Read field $f$ of $o$.  We will assume that
  there is a constant mapping function which given a field name
  returns an integer index.  We will write the result of mapping $f$
  as \fref{f}{index}.  For simplicity, and without loss of generality,
  we will assume all fields are of equal size.
\item $\funcname{Write}(o, f, v)$: Write value $v$ to field $f$ of $o$.
\end{itemize}
All other operations on Java objects, such as method dispatch and type
interrogation, can be performed using the immutable {\tt type}
field in the object.  Because the {\tt type} field is never changed
after object creation, non-blocking implementations of operations on
the {\tt type} field are trivial.

As Figure~\ref{fig:single-o} shows, our single-object implementation
of \atomic represents objects as a pair, combining {\tt type} and a
reference to a functional array.  When not inside an atomic
method, object reads and writes are implemented using the
corresponding functional array operation, with the array reference in
the object being updated appropriately:
\begin{itemize}
\item $\funcname{Read}(o, f)$:
  Return $\funcname{FA-Read}(\fref{o}{fields}, \fref{f}{index})$.
\item $\funcname{Write}(o, f, v)$: Replace \fref{o}{fields} with the
  result of $\funcname{FA-Update}(\fref{o}{fields}, \fref{f}{index}, v)$.
\end{itemize}

The interesting cases are reads and writes inside an atomic method.
At entry to our atomic method which will access (only) object $o$, we
store \fref{o}{fields} in a local variable $u$.  We create another
local variable $u'$ which we initialize to $u$.  Then our read and
write operations are implemented as:
\begin{itemize}
\item $\funcname{ReadAtomic}(o, f)$:
  Return $\funcname{FA-Read}(u', \fref{f}{index})$.
\item $\funcname{WriteAtomic}(o, f, v)$:
  Update variable $u'$ to the result of
  $\funcname{FA-Update}(u', \fref{f}{index}, v)$.
\end{itemize}

At the end of the atomic method, we use Compare-And-Swap to atomically
set \fref{o}{fields} to $u'$ iff it contained $u$.  If the CAS fails,
we back-off and retry.

With our na{\"\i}ve ``small object'' functional arrays, this implementation is
exactly the ``small object protocol'' of Herlihy \cite{Herlihy93}.
Herlihy's protocol is rightly criticized for an excessive amount of
copying.  We will address this with a better implementation of
functional arrays in Section~\ref{sec:large-obj}.
However, the restriction that only one object
may be referenced within an atomic method is overly limiting.

\subsection{Extension to multiple objects}
\begin{figure}[t]\centering
\includegraphics[width=3.25in,clip=true]{Figures/nb-multi-obj.eps}
\caption{Data structures to support non-blocking multi-object
  concurrent operations.  Objects point to a linked list of versions,
  which reference operation identifiers.  Versions created within the
  same execution of an atomic method share the same operation
  identifier.  Version structure also contain pointers to functional
  arrays, which record the values for the fields of the object.
  If no modifications have been made to the object, multiple versions
  in the list may share the same functional array.}
\label{fig:multi-o}
\end{figure}
\begin{figure}[p]
\renewcommand{\>}{~~}
\newcommand{\com}[1]{\hfill [{\sl #1}]}
\begin{tabular}{l}
$\funcname{Read}(o, f)$:\\
begin\\
retry:\\
\>$u \gets \fref{o}{versions}$ \\
\>$u' \gets \fref{u}{next}$ \\
\>$s  \gets \fref{\fref{u}{owner}}{status}$ \\
\>if ($s=\text{\sl DISCARDED}$) \com{Delete DISCARDED?}\\
\>\>CAS$(u, u', \addr{\fref{o}{versions}})$\\
\>\>goto retry \\
\>else if ($s=\text{\sl COMPLETE}$)\\
\>\>$a \gets \fref{u}{fields}$ \com{$u$ is COMPLETE}\\
\>\>$\fref{u}{next} \gets \text{\bf null}$ \com{Trim version list}\\
\>else\\
\>\>$a \gets \fref{u'}{fields}$ \com{$u'$ is COMPLETE}\\
\>return $\funcname{FA-Read}(a, \fref{f}{index})$ \com{Do the read}\\
end\\
\\
$\funcname{ReadAtomic}(o, f)$:\\
begin\\
\>$u \gets \fref{o}{versions}$\\
\>if ($\var{oid} = \fref{u}{owner}$) \com{My OID should be first}\\
\>\>return $\funcname{FA-Read}(\fref{u}{fields}, \fref{f}{index})$
\com{Do the read}\\
\>else \com{Make me first!}\\
\>\>$u' \gets \fref{u}{next}$\\
\>\>$s  \gets \fref{\fref{u}{owner}}{status}$\\
\>\>if ($s=\text{\sl DISCARDED}$) \com{Delete DISCARDED?}\\
\>\>\>CAS$(u, u', \addr{\fref{o}{versions}})$\\
\>\>else if ($\fref{\var{oid}}{status}=\text{\sl DISCARDED}$)
\com{Am I alive?}\\
\>\>\>fail\\
\>\>else if ($s=\text{\sl IN-PROGRESS}$) \com{Abort IN-PROGRESS?}\\
\>\>\>CAS$(s, \text{\sl DISCARDED}, \addr{\fref{\fref{u}{owner}}{status}})$\\
\>\>else \com{Link new version in:} \\
\>\>\>$\fref{u}{next} \gets \text{\bf null}$ \com{Trim version list}\\
\>\>\>$u' \gets \text{new \tt Version}(\var{oid}, u, \text{\bf null})$
\com{Create new version}\\
\>\>\>if (CAS$(u, u', \addr{\fref{o}{versions}}) \neq \text{\sl FAIL}$)\\
\>\>\>\>$\fref{u'}{fields} \gets \fref{u}{fields}$ \com{Copy old fields}\\
\>\>goto retry\\
end\\
\end{tabular}
\caption{\funcname{Read} and \funcname{ReadAtomic} implementations for the
  multi-object protocol.}\label{fig:reads}
\end{figure}

\begin{figure}[p]
\renewcommand{\>}{~~}
\newcommand{\com}[1]{\hfill [{\sl #1}]}
\begin{tabular}{l}
$\funcname{Write}(o, f, v)$:\\
begin\\
retry:\\
\>$u  \gets \fref{o}{versions}$\\
\>$u' \gets \fref{u}{next}$\\
\>$s  \gets \fref{\fref{u}{owner}}{status}$\\
\>if ($s=\text{\sl DISCARDED}$) \com{Delete DISCARDED?}\\
\>\>CAS$(u, u', \addr{\fref{o}{versions}})$\\
\>else if ($s=\text{\sl IN-PROGRESS}$) \com{Abort IN-PROGRESS?}\\
\>\>CAS$(s, \text{\sl DISCARDED}, \addr{\fref{\fref{u}{owner}}{status}})$\\
\>else \com{$u$ is COMPLETE}\\
\>\>$\fref{u}{next} \gets \text{\bf null}$ \com{Trim version list}\\
\>\>$a \gets \fref{u}{fields}$\\
\>\>$a' \gets \funcname{FA-Update}(a, \fref{f}{index}, v)$\\
\>\>if (CAS$(a, a', \addr{\fref{u}{fields}}) \neq \text{\sl FAIL}$)
\com{Do the write}\\
\>\>\>return \com{Success!}\\
\>goto retry\\
end\\
\\
$\funcname{WriteAtomic}(o, f, v)$:\\
begin\\
\>$u  \gets \fref{o}{versions}$\\
\>if ($oid = \fref{u}{owner}$) \com{My OID should be first}\\
\>\>$\fref{u}{fields} \gets \funcname{FA-Update}(\fref{u}{fields}, \fref{f}{index}, v)$\com{Do write}\\
\>else \com{Make me first!}\\
\>\>$u' \gets \fref{u}{next}$\\
\>\>$s  \gets \fref{\fref{u}{owner}}{status}$\\
\>\>if ($s=\text{\sl DISCARDED}$) \com{Delete DISCARDED?}\\
\>\>\>CAS$(u, u', \addr{\fref{o}{versions}})$\\
\>\>else if ($\fref{\var{oid}}{status}=\text{\sl DISCARDED}$)
\com{Am I alive?}\\
\>\>\>{\it fail}\\
\>\>else if ($s=\text{\sl IN-PROGRESS}$) \com{Abort IN-PROGRESS?}\\
\>\>\>CAS$(s, \text{\sl DISCARDED}, \addr{\fref{\fref{u}{owner}}{status}})$\\
\>\>else \com{Link new version in:} \\
\>\>\>$\fref{u}{next} \gets \text{\bf null}$ \com{Trim version list}\\
\>\>\>$u' \gets \text{new \tt Version}(\var{oid}, u, \text{\bf null})$
\com{Create new version}\\
\>\>\>if (CAS$(u, u', \addr{\fref{o}{versions}}) \neq \text{\sl FAIL}$)\\
\>\>\>\>$\fref{u'}{fields} \gets \fref{u}{fields}$ \com{Copy old fields}\\
\>\>goto retry\\
end\\
\end{tabular}
\caption{\funcname{Write} and \funcname{WriteAtomic} implementations for the
  multi-object protocol.}\label{fig:writes}
\end{figure}

We now extend the implementation to allow the fields of any number of
objects to be accessed during the atomic method.
Figure~\ref{fig:multi-o} shows our new object representation.
Objects consist of two slots, and the first represents the immutable
{\tt type}, as before.  The second field, {\tt versions}, points to a
linked list of {\tt Version} structures.  The {\tt Version} structures
contain a pointer {\tt fields} to a functional array, and a pointer
{\tt owner} to an \emph{operation identifier}.  The operation
identifier contains a single field, {\tt status}, which can be set to
one of three values: \textsl{COMPLETE}, \textsl{IN-PROGRESS}, or
\textsl{DISCARDED}.  When the operation identifier is created, the
status field is initialized to \textsl{IN-PROGRESS}, and it will be
updated exactly once thereafter, to either \textsl{COMPLETE} or
\textsl{DISCARDED}.  A \textsl{COMPLETE} operation identifier never
later becomes \textsl{IN-PROGRESS} or \textsl{DISCARDED}, and
a \textsl{DISCARDED} operation identifier never becomes
\textsl{COMPLETE} or \textsl{IN-PROGRESS}.

We create an operation identifier when we begin or restart an atomic
method and place it in a local variable \emph{oid}.  At the end of the
atomic method, we use CAS to set \fref{\var{oid}}{status} to
{\sl COMPLETE} iff it was {\sl IN-PROGRESS}.  If the CAS is successful,
the atomic method has also executed successfully; otherwise
$\fref{\var{oid}}{status}=\text{\sl DISCARDED}$ and we must
back-off and retry the atomic method.  All {\tt Version} structures
created while in the atomic method will reference \emph{oid} in
in their {\tt owner} field.

Semantically, the current field values for the object will be given by
the first version in 
the versions list whose operation identifier is {\sl COMPLETE}.
This allows us to link {\sl IN-PROGRESS} versions in at the head of
multiple objects' versions lists and atomically change the values of
all these objects by setting the one common operation identifier to
{\sl COMPLETE}.  We only allow one {\sl IN-PROGRESS} version on the
versions list, and it must be at the head, so
Therefore, before we can link a new version at the head, we
must ensure that every other version on the list is {\sl DISCARDED} or
{\sl COMPLETE}.

Since we will never look past the first {\sl COMPLETE} version in the
versions list, we can free all versions past that point.  In our
presentation of the algorithm, we do this by explicitly setting the
{\tt next} field of every {\sl COMPLETE} version we see to {\bf null};
this allows the versions past that point to be garbage collected.
An optimization would be to have the garbage collector do the list
trimming for us when it does a collection.
% always must read u.next before u.owner.status to ensure we don't
% get caught with a null pointer from a version that just committed.

We don't want to inadvertently chase the null {\tt next} pointer
of a {\sl COMPLETE} version, so we always load the {\tt next}
field of a version \emph{before} we load {\tt owner.status}.  Since
the writes occur in the reverse order ({\sl COMPLETE} to
{\tt owner.status}, then {\bf null} to {\tt next}) we have ensured that
our {\tt next} pointer is valid whenever the status is not {\sl COMPLETE}.

We begin an atomic method with \funcname{AtomicEntry} and attempt to
complete an atomic method with \funcname{AtomicExit}.  They are defined as
follows:
\begin{itemize}
\item $\funcname{AtomicEntry}$: create a new operation identifier, with
  its status initialized to {\sl IN-PROGRESS}.  Assign it to the
  thread-local variable \var{oid}.
\item $\funcname{AtomicExit}$:
  If
 $$\text{CAS}(\text{\sl IN-PROGRESS}, \text{\sl COMPLETE},
             \addr{\fref{\var{oid}}{status}})$$
  is successful, the atomic method as a whole has completed successfully,
  and can be linearized at the location of the CAS.
  Otherwise, the method has failed.  Back-off and retry from
  \funcname{AtomicEntry}.
\end{itemize}
Pseudo-code describing \funcname{Read}, \funcname{Write}, \funcname{AtomicRead},
and \funcname{AtomicWrite} is presented in Figures~\ref{fig:reads} and
\ref{fig:writes}.  In the absence of contention, all operations take
constant time plus an invocation of \funcname{FA-Read} or
\funcname{FA-Update}.

\subsection{Lock-free functional arrays}\label{sec:large-obj}
\begin{figure*}\centering
\includegraphics[width=7in,clip=true]{Figures/chuang.eps}
\caption{Shallow binding scheme for functional arrays, from
  \cite[Figure~1]{Chuang94}.}
\label{fig:chuang}
\end{figure*}
In this section we will present a lock-free implementation of functional
arrays with $O(1)$ performance in the absence of contention.  This
will complete our implementation of non-blocking \atomic methods for Java.

There have been a number of proposed implementations of functional
arrays, starting from the ``classical'' functional binary tree
implementation.  O'Neill and Burton \cite{ONeillBu97} give a fairly
inclusive overview.  Functional array implementations fall generally
into one of three categories: \emph{tree-based}, \emph{fat-elements},
or \emph{shallow-binding}.

Tree-based implementations typically have a logarithmic term in their
complexity.  The simplest is the persistent binary tree with $O(\ln
n)$ look-up time; Chris Okasaki 
\cite{Okasaki95} has implemented a purely-functional random-access list
with $O(\ln i)$ expected lookup time, where $i$ is the index of the
desired element.

Fat-elements implementations have per-element data structures indexed
by a master array. Cohen \cite{Cohen84} hangs a list of
versions from each element in the master array.
O'Neill and Burton \cite{ONeillBu97}, in a more sophisticated
technique, hang a splay tree off each element and achieve $O(1)$
operations for single-threaded use, $O(1)$ amortized cost when
accesses to the array are ``uniform'', and $O(\ln n)$ amortized worst
case time. 

Shallow binding was introduced by Baker \cite{Baker78} as a method to
achieve fast variable lookup in Lisp environments.  Baker clarified
the relationship to functional arrays in \cite{Baker91}.  Shallow
binding is also called \emph{version tree arrays}, \emph{trailer
  arrays}, or \emph{reversible differential lists}.  A typical
drawback of shallow binding is that reads may take $O(u)$ worst-case
time, where $u$ is the number of updates made to the array.  Tyng-Ruey
Chuang \cite{Chuang94} uses randomized cuts to the version tree to limit
the cost of a read to $O(n)$ in the worst case.  Single-threaded
accesses are $O(1)$.

Our use of functional arrays is single-threaded in the common case,
when transactions do not abort.  Chuang's scheme is attractive because
it limits the worst-case cost of an abort, with very little added
complexity.   In this section we will present a lock-free version of
Chuang's randomized algorithm.

In shallow binding, only one version of the functional array (the
\emph{root}) keeps its contents in an imperative array (the
\emph{cache}).   Each of the other versions is represented as a path
of \emph{differential nodes}, where each node describes the
differences between the current array and the previous array.  The
difference is represented as a pair \tuple{\text{\it index},\text{\it value}},
representing the new value to be stored at the specified index.
All paths lead to the root.  An update to the functional array is
simply implemented by adding a differential node pointing to the array it is
updating.

The key to constant-time access for single-threaded use is provided by the read
operation.  A read to the root simply reads the appropriate value from
the cache.  However, a read to a differential node triggers a series
of rotations which swap the direction of differential nodes and result
in the current array acquiring the cache and becoming the new root.
This sequence of rotations is called \emph{re-rooting}, and is
illustrated in Figure~\ref{fig:chuang}.  Each rotation
exchanges the root nodes for a differential node pointing to it, after
which the differential node becomes the new root and the root becomes
a differential node pointing to the new root. The cost of a read is
proportional to its re-rooting length, but after the first read
accesses to the same version are $O(1)$ until the array is re-rooted again.

Shallow binding performs badly if read operations ping-pong between two
widely separated versions of the array, as we will continually
re-root the array from one version to the other.
Chuang's contribution is to provide for \emph{cuts} to the chain of
differential nodes: once in a while we clone the cache and create a
new root instead of performing a rotation.  This operation takes
$O(n)$ time, so we amortize it over $n$ operations by randomly
choosing to perform a cut with probability $1/n$.

\begin{figure*}\centering
\includegraphics[width=7in,clip=true]{Figures/funarr.eps}
\caption{Atomic steps in $\funcname{FA-Rotate}(B)$.  Time proceeds top-to-bottom
  on the left hand side, and then top-to-bottom on the right.
  Array $A$ is a root node, and $\funcname{FA-Read}(A, x)=z$.
  Array $B$ has the almost the same contents as $A$, but
  $\funcname{FA-Read}(B, x)=y$.}
\label{fig:funarr}
\end{figure*}

\begin{figure*}\centering
\renewcommand{\>}{~~}
\newcommand{\com}[1]{\hfill [{\sl #1}]}
\begin{tabular}{l}
$\funcname{FA-Update}(A, i, v)$:\\
begin\\
\>$d \gets \text{new DiffNode}(i, v, A)$\\
\>$A'\gets \text{new Array}(\fref{A}{size}, d)$\\
\>return $A'$\\
end\\
\\
$\funcname{FA-Read}(A, i)$:\\
begin\\
retry:\\
\>$d_C \gets \fref{A}{node}$\\
\>if $d_C$ is a cache, then\\
\>\>$v \gets \fref{A}{node}[i]$\\
\>\>if $(\fref{A}{node} \neq d_C)$\com{consistency check}\\
\>\>\>goto retry\\
\>\>return $v$\\
\>else\\
\>\>\funcname{FA-Rotate}(A)\\
\>\>goto retry\\
end\\
\\
$\funcname{FA-Rotate}(B)$:\\
begin\\
retry:\\
\>$d_B \gets \fref{B}{node}$\com{step (1): assign names as per Figure~\ref{fig:funarr}.}\\
\>$A \gets \fref{d_B}{array}$\\
\>$x \gets \fref{d_B}{index}$\\
\>$y \gets \fref{d_B}{value}$\\
\>$z \gets \funcname{FA-Read}(A, x)$\com{rotates A as side effect}\\
\\
\>$d_C \gets \fref{A}{node}$\\
\>if $d_C$ is not a cache, then \\
\>\>goto retry\\
\\
\>if $(0 = (\text{random} \bmod \fref{A}{size}))$\com{random cut}\\
\>\>$d_C' \gets \text{copy of }d_C$\\
\>\>$d_C'[x] \gets y$\\
\>\>$s\gets\text{DCAS}(d_C, d_C, \addr{\fref{A}{node}}, d_B, d_C', \addr{\fref{B}{node}})$\\
\>\>if $(s \neq \text{\sl SUCCESS})$ goto retry\\
\>\>else return\\
\\
\>$C \gets \text{new Array}(\fref{A}{size}, d_C)$\\
\>$d_A \gets \text{new DiffNode}(x, z, C)$\\
\\
\>$s \gets \text{CAS}(d_C, d_A, \addr{\fref{A}{node}})$\com{step (2)}\\
\>if $(s\neq \text{\sl SUCCESS})$ goto retry\\
\\
\>$s\gets\text{CAS}(A, C, \addr{\fref{d_B}{array}})$\com{step (3)}\\
\>if $(s\neq \text{\sl SUCCESS})$ goto retry\\
\\
\>$s \gets\text{CAS}(C, B, \addr{\fref{d_A}{array}})$\com{step (4)}\\
\>if $(s\neq \text{\sl SUCCESS})$ goto retry\\
\\
\>$s \gets \text{DCAS}(z, y, \addr{d_C[x]},  d_C, d_C, \addr{\fref{C}{node}})$\com{step (5)}\\
\>if $(s\neq \text{\sl SUCCESS})$ goto retry\\
\\
\>$s \gets \text{DCAS}(d_B, d_C, \addr{\fref{B}{node}}, d_C, {\bf nil}, \addr{\fref{C}{node}})$\com{step (6)}\\
\>if $(s\neq \text{\sl SUCCESS})$ goto retry\\
end\\
\end{tabular}
\caption{Implementation of lock-free functional array using shallow
  binding and randomized cuts.}
\label{fig:fun-impl}
\end{figure*}

Figure~\ref{fig:funarr} shows the data structures used for the
functional array implementation, and the series of atomic steps used
to implement a rotation.  The {\tt Array} class represents a
functional array; it consists of a {\tt size} for the array and a
pointer to a {\tt Node}.  There are two types of nodes: a {\tt
  CacheNode} stores a value for every index in the array, and a {\tt
  DiffNode} stores a single change to an array.  {\tt Array} objects
which point to {\tt CacheNode}s are roots.

In step 1 of the figure, we have a root array $A$ and an
array $B$ whose differential node $d_B$ points to $A$.  The functional
arrays $A$ and $B$ differ in one element: element $x$ of $A$ is $z$,
while element $x$ of $B$ is $y$.  We are about to rotate $B$ to give
it the cache, while linking a differential node to $A$.

Step 2 shows our first atomic action.  We have created a new {\tt
  DiffNode} $d_A$ and a new {\tt Array} $C$ and linked them between
$A$ and its cache.  The {\tt DiffNode} $d_A$ contains the value for
element $x$ contained in the cache, $z$, so there is no change in
the value of $A$.

We continue swinging pointers until step 5, when can finally set
the element $x$ in the cache to $y$.  We perform this operation with a
DCAS operation which checks that $\fref{C}{node}$ is still pointing to
the cache as we expect.  Note that a concurrent rotation would swing
$\fref{C}{node}$ in its step 1.  In general, therefore, the location
pointing to the cache serves as a reservation on the cache.

Thus in step 6 we need to again use DCAS to simultaneously swing
$\fref{C}{node}$ away 
from the cache as we swing $\fref{B}{node}$ to point to the cache.

Figure~\ref{fig:fun-impl} presents pseudocode for \funcname{FA-Rotate},
\funcname{FA-Read}, and \funcname{FA-Update}.  Note that \funcname{FA-Read} also
uses the cache pointer as a reservation,
double-checking the cache pointer after it finishes its read to ensure that the
cache hasn't been stolen from it.

Let us now consider cuts, where \funcname{FA-Read} clones the cache
instead of performing a rotation.   Cuts also check the cache pointer
to protect against concurrent rotations.  But what if the cut occurs
while a rotation is mutating the cache in step 5?  In this case the
only array adjacent to the root is $B$, so the cut must be occurring
during an invocation of $\funcname{FA-Rotate}(B)$.  But then the
differential node $d_B$ will be applied after the cache is copied,
which will safely overwrite the mutation we were concerned about.

Note that with hardware support for small transactions \cite{HerlihyMo93}
we could cheaply perform the entire rotation atomically, instead of
using this six-step approach.


%% \subsection{Optimizations}
%% Re-rooting is the most complicated part of the functional array
%% algorithm.  It can be optimized in a number of ways.  For example,

%% %unsync rotate for transaction-local data.
%% The first is to recognize that some array versions can only be seen by
%% a single thread.  In particular, when we are working on an {\sl
%%   IN-PROGRESS} operation, all array versions which it creates are
%% unreachable from other threads until the operation is committed.
%% We can add a field {\tt creator} to the {\tt Array} object which records what
%% operation created that version.  If the {\tt creator} field of both
%% $A$ and $B$ contains our own \var{oid} when we begin a rotate, we know
%% that these versions are both thread local

%% .. uh, no.  This doesn't work.

%% % scales method of tagging fields.


\subsecput{os}{Operating systems}

Perhaps the most promising single application for boundless
transactions is providing atomicity within an operating system.
Locking is difficult to reason about For example, we counted at the
comments in the linux filesystem layer, and found that about 15\% of
the comments relate to locking protocols.  Those comments often
describe global invariants of the program which are difficult to
verify.  And many kernel bugs involve races and deadlocks.

[Dealing with nested transactions: modularity, semantics.]

\subsecput{htm}{HTM versus STM}

\subsecput{performance}{Performance improvements}

\note{This text may very well belong in some other section.}  Herlihy
and Moss' HTM scheme is not non-blocking.  In their proposal, a
transactional load by a processor ends with an XABORT and an XCOMMIT
tag on the read cache line.  Later, while that processor's transaction
is still outstanding, a transactional store by a different processor
to that same cache line will result in a T\_RFO cycle on the bus, to
which the first processor's cache will result BUSY.  The second
processor commits suicide (aborts itself) in response.

[A processor which does transactional reads holds an irrevocable lock
on those cache lines until it commits or aborts itself, and any
other processor which attempts a write on those lines will block,
waiting.]

This means that a processor which transactionally reads many locations
and then loops forever without committing may prevent all other
processors from ever making progress on their transactions.  This is a
simple protocol error: the intervening processor should be able to
abort the first processor instead of blocking for its completion.
Loosely, processors must be able to kill others; if a processor can
only commit suicide, than the system may block.  When transactions are
exposed to user code, Herlihy's system is vulnerable to this simple
denial-of-service attack.  BTM does not suffer from this
problem. [although starvation may still occur.]

\end{document}

% LocalWords:  csail mit edu AFRL
