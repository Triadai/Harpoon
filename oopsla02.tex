% -*- latex -*- This is a LaTeX document.
% $Id: oopsla02.tex,v 1.18 2002-03-23 04:39:15 cananian Exp $
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\documentclass[preprint]{acmconf}
\documentclass{acmconf}
% don't forget to turn off 'preprint' before submission!
%\usepackage[section,plain]{algorithm}
%\usepackage{amsthm} % proof environment
%\usepackage{amstext} % the \text command for math mode (replaces \mbox)
\usepackage{varioref} % \vref command
\usepackage{graphicx} % for eps figures
\usepackage{color}
\usepackage{comdef}
\newcommand{\figscale}{1.0}

%setup varioref package
\renewcommand{\reftextbefore}{on the preceding page}\vrefwarning

\newcommand{\mycomment}[1]{}
\newcommand{\meet}{\ensuremath{\sqcap}}

\renewcommand{\floatpagefraction}{0.8}

\title{\bf Data Size Optimizations for Java Programs}

\author{C.~Scott~Ananian and Martin~Rinard\\
        Laboratory for Computer Science\\
        Massachusetts Institute of Technology\\ 
        Cambridge, MA 02139 \\ 
        {\tt \{cananian, rinard\}@lcs.mit.edu} }

\begin{document}
% in preprint mode, tag pages with a revision identifier.
%\pagestyle{myheadings}\markboth{$ $Revision: 1.18 $ $}{$ $Revision: 1.18 $ $}
\bibliographystyle{plain}

\maketitle

% abstract
\begin{abstract}
We present a set of techniques for reducing the memory consumption of
object-oriented programs. These techniques 
include optimizations that eliminate fields with
constant values, reduce the sizes of fields based on the range
of values that can appear in each field, and use a variety of
transformations to eliminate fields with common default values
or usage patterns from the majority of objects that originally
contained these fields. We can apply these optimizations both 
to fields declared by the programmer and to fields in
object headers. We use a variety of program analysis 
algorithms to extract the information required to apply these
optimizations. 

We have implemented these techniques in the MIT Flex compiler
system and applied them to the programs in the SPECjvm98 
benchmark suite. Our experimental results show that 
our combined techniques can significantly reduce the amount
of memory required to store the objects in our benchmark
suite.  Some of the optimizations reduce the overall execution time;
others impose modest performance penalties.

\end{abstract}

\section{Introduction}

This paper presents a set of techniques for reducing the
amount of data space required to represent objects
in object-oriented programs. Our techniques optimize
the representation of both the programmer-defined fields
within each object and the header information used by the
run-time system:
\begin{itemize}
\item {\bf Field Reduction:} 
Our a flow-sensitive, interprocedural bitwidth analysis
analysis computes the range of values that the program
may assign to each field. The compiler then transforms the program
to reduce the size of the field to the smallest size 
capable of storing that range of values. 
\item {\bf Constant Field Elimination:} 
If the bitwidth analysis finds that field always holds
the same constant value, the compiler eliminates the field. 
It removes each write to the field, and replaces each read
with the constant value.
\item {\bf Static Specialization:} Our analysis finds 
classes with fields whose values do not change after initialization,
even though objects allocated at different allocation sites may
have different values for these fields. It then generates 
a specialized version of each class for each allocation site;
this version omits the fields and instead provides accessor
methods that return the corresponding values. 
\item {\bf Field Externalization:} Our analysis uses profiling
to find fields that almost always have the same default value. 
It then removes these fields from their enclosing class, 
using a hash table to store values of the field that differ
from the default value. It replaces writes to the field with
an insertion into the hash table (if the written value is not the
default value) or a removal from the hash table (if the written value
is the default value). It replaces reads with hash table lookups; 
if the object is not present in the hash table, the lookup simply
returns the default value. 
\item {\bf Unused Field Analysis:} If the program does not
use a field, the compiler eliminates the field. 
\item {\bf Claz Compression:} Our class hierarchy analysis
computes an upper bound on the number of classes that the
program may instantiate. Objects in standard 
Java implementations have a header that contains a pointer
to the class data (such as the method dispatch table) for that object. 
Our compiler uses the results of the class
hierarchy analysis to replace the reference with a smaller
offset into a table of pointers to the class data. 
\item {\bf Byte Packing:} All of the above transformations may
reduce or eliminate the amount of space required to store each
field in the object or object header. Our byte packing algorithm
arranges the fields in the object to minimize the amount of 
storage. 
\end{itemize}
All of these transformations reduce the space required to store
objects, but potentially increase the running time of the program.
Our experimental results show that, for our set of benchmark
programs, all of our combined techniques can reduce the peak amount of memory
required to run the program by as much as XXX\% and never reduce the
running time by more than YYY\%.

\subsection{Contributions}

This paper makes the following contributions:
\begin{itemize}
\item {\bf Space Reduction Transformations:} It presents a set
of novel transformations for reducing the memory required to 
represent objects in object-oriented programs.

\item {\bf Analysis Algorithms:} It presents a set of 
analysis algorithms that automatically extract the 
information required to apply the space reduction 
transformations.

\item {\bf Implementation:} We have fully 
implemented all of the analyses and techniques 
presented in the paper. Our experience with this
implementation enables us to discuss the pragmatic
details of developing an effective implementation 
of our techniques. 

\item {\bf Experimental Results:} It presents a set
of experimental results that characterize the impact
of our transformations. 
\end{itemize}

\section{Example}

We next present several examples that illustrate the kinds of 
analyses and transformations that our compiler performs.

\subsection{Field Reduction and Constant Field Elimination}

Figure~\ref{fig:value} presents the {\tt Value} class, which is 
a wrapper around either an {\tt Integer} object or a {\tt Float}
object. The {\tt type} field indicates which kind of object
is stored in the {\tt value} field of the class, 
which essentially implements a tagged 
union.\footnote{This class is a simplfied version of similar
classes that appear in some of our benchmarks.} 
The class also maintains the {\tt positive} field, which is
{\tt 1} if the wrapped number is positive and {\tt 0} otherwise. 

Our bitwidth analysis uses an interprocedural
value-flow algorithm to compute upper and lower bounds for the
values that can appear in each variable. This analysis tracks
the flow of values across procedure boundaries via parameters,
into and out of the heap via instance variables of classes, and through
intermediate temporaries and local variables in the program.
It also reasons about the semantics of arithmetic operators such
as {\tt +} and {\tt *} to obtain bounds for the values computed
by arithmetic expressions. 
This analysis discovers the following facts about 
how the program uses this class: 1) the {\tt integerType} 
field always has the value {\tt 0}, 2) the {\tt floatType} 
field always has the value {\tt 1}, 3) the {\tt type} 
field always has a value between {\tt 0} and {\tt 1} (inclusive),
and 4) the {\tt positive} field always has a value between 
{\tt 0} and {\tt 1} (also inclusive).

\begin{figure}
\begin{verbatim}
public class Value { 
  int integerType = 0;
  int floatType = 1;
  int type;
  int positive;
  Object value;
  void setInteger(Integer i) { 
    type = integerType;
    value = i;
    if (i.intValue() > 0) positive = 0;
    else positive = 1;
  }
  void setFloat(Float f) { 
    type = floatType;
    value = f;
    if (f.floatValue() > 0.0) positive = 0;
    else positive = 1;
  }
  void setValue(int t, int p, Object v) { 
    type = t;
    positive = p;
    value = v;
  }
}

public class main { 
  public static void main() { 
    Value v = new Value();
    Integer i = new Integer(5);
    v.setValue(v.integerType, 1, i);
  }
}
\end{verbatim}
\caption{\label{fig:value} Value Class}
\end{figure}

Our compiler uses this information to remove all occurrences
of the {\tt integerType} and {\tt floatType} fields from the
program. It replaces each read of the {\tt integerType} field
with the constant {\tt 0}, and each read of the {\tt floatType}
field with the constant {\tt 1}. It also uses the bounds on the 
values of the {\tt type} and {\tt positive} variables to reduce the size of the 
corresponding fields. Our currently implemented compiler rounds
field sizes to the nearest byte required to hold the range
of values that can occur. Our byte packing algorithm then 
generates a dense packing of the values, attempting to preserve
the alignment of the variables if possible. In this case, the
algorithm can reduce the field sizes by six bytes and the overall
size of the object by one four-byte word. 

\subsection{Static Specialization} 

Figure~\ref{fig:string} presents portions of the implementation
of the Java {\tt String} class. The {\tt value} field in this
class refers to a character array that holds the characters
in the string; the {\tt count} field holds the length of the
string. In some cases, instances of the {\tt String} class
are derived substrings of other instances 
(see the {\tt substring} method in Figure~\ref{}), in which case the
{\tt offset} field provides the offset of the starting 
point of the string within the {\tt value} character array. 
Note that the {\tt value}, {\tt offset}, and {\tt count} 
fields are all initialized when the string is constructed
and do not change during the lifetime of the string.

\begin{figure}[tp]
\begin{samplecode}
public final class String \{\\
\>private final char value[];\\
\>private final int offset;\\
\>private final int count;\\
\>\ldots\\
\>public char charAt(int i) \{\\
\>\>return value[offset+i];\\
\>\}\\
\>public String substring(int start) \{\\
\>\>int noff = offset + start;\\
\>\>int ncnt = count - start;\\
\>\>return new String(value, noff, ncnt);\\
\>\}\\
\}\\
\end{samplecode}
\caption{Portions of the {\tt java.lang.String} class.}
\label{fig:string-fields}
\end{figure}

In practice, most strings are not substrings of other strings. 
The {\tt offset} field in most strings is therefore zero.
Moreover, it is possible to tell at the object creation site
whether the {\tt offset} will be zero or some other number.
In fact, all of the public {\tt String} constructors create
strings with {\tt offset} zero; only the {\tt substring} method
creates strings with a non-zero offset. And even at 
calls to the private {\tt String(int, int, char[])} constructor
inside the {\tt substring} method, it is possible to dynamically
test the values of the parameters to determine if the newly
constructed string will have a zero or non-zero offset.

Our analysis exploits this fact by splitting the 
{\tt String} class into two classes: a superclass {\tt SmallString}
that omits the {\tt offset} field, and a subclass {\tt BigString} that
extends {\tt SmallString} and includes the {\tt offset} field. 
Each of these two new classes implements a {\tt getOffset} method
that returns the value of the offset. The {\tt getOffset} method
in the {\tt SmallString} class simply returns zero; the {\tt
getOffset} method in the {\tt BigString} method returns the 
value of the offset field. 

At every allocation site except the one inside the {\tt substring}
method, the transformed program allocates a {\tt SmallString} 
object. Inside the {\tt substring} method, the program generates
code that dynamically tests if the offset in the substring
will be zero. If so, it allocates a {\tt SmallString} object;
if not, it allocates a {\tt BigString} object. This transformation
therefore eliminates the {\tt offset} field in the majority
of strings. 

\begin{figure}[tp]
\begin{samplecode}
public final class SmallString \{\\
\>private final char value[];\\
\>private final int count;\\
\>int getOffset() \{ return 0; \}\\
\>\ldots\\
\>public char charAt(int i) \{\\
\>\>return value[getOffset()+i];\\
\>\}\\
\}\\
public final class BigString\\
\>\>extends SmallString \{\\
\>private final int offset;\\
\>int getOffset() \{ return offset; \}\\
\}\\
\end{samplecode}
\caption{Static specialization of {\tt java.lang.String}.}
\label{fig:big-small}
\end{figure}

\begin{figure}[tp]
\begin{samplecode}
public SmallString substring(int start) \{\\
\>int noff = offset + start;\\
\>int ncnt = count - start;\\
\>if (noff==0)\\
\>\>return new SmallString(value, noff, ncnt);\\
\>else\\
\>\>return new BigString(value, noff, ncnt);\\
\}\\
\end{samplecode}
\caption{Dynamic selection among specialized classes in a method
  from {\tt java.lang.String}.}
\label{fig:dyn-select}
\end{figure}

The analysis required to support this transformation takes place
in two phases. The first phase scans the program
to identify fields that
are constant after initialization. In our example, the analysis
determines that the {\tt offset} field is never written after
it is initialized. The next phase determines if the value
of this field is determined either by the constructor that
initialized it or if it is a simple function of the parameters
of the constructor. In our example, the analysis determines
that the {\tt offset} field is zero for all constructors
except the private constructor invoked within the {\tt substring}
method. It also determines that, for objects initialized by 
this constructor, the value of the {\tt offset} field is simply
the value of the {\tt noff} parameter to this constructor. 

This analysis identifies a set of candidate fields. 
The analysis then chooses one of the candidate fields, then 
splits the class along the possible values
that can appear in the field. Our current implementation uses
profiling to select the field that will provide the largest
space savings; our policy takes both the size of the field
and the percentage of objects that have the same value for 
that field. In our example, the analysis identifies the 
{\tt offset} field as the best candidate and splits the class
on that field. We can apply this idea recursively to the 
new program to obtain the benefits of splitting on multiple
fields. 

\subsection{Field Externalization}

In the string example discussed above, it was possible to determine
which version of the specialized class to use at object allocation
time. In some cases, however, a given field may almost always have
a given value, even though it is not possible to statically determine
what the value is or which objects will contain fields of that 
value. In such cases we apply another optimization, 
{\em field externalization}. This optimization removes the field
from the class, replacing fields whose values differ from the default 
value with hash table entries that map objects to values. If an object/value
mapping is present in the hash table, that entry provides the 
value of the removed field. If there is no mapping for a given object,
the field is assumed to have the default value. 
In our current implementation, we use profiling 
to identify the default value. 

In this scheme, writes to the field are converted into a check to see
if the new value of the field is the default value. If so, the 
generated code simply removes any old mappings for that 
object from the hash table.
If not, the generated code removes any old mapping and inserts a new
mapping to record the new value. 

\subsection{Hashlock Externalization}

Our currently implemented system applies field externalization
in a general way to any field in the object. We would, however,
like to highlight an especially useful extension of the basic
technique. Java implementations typically store an object
hash code and lock information in the object header. For many
objects, however, the program never actually uses the hash code
or lock information. Our implemented system therefore uses
a variant of field externalization called {\em hashlock 
externalization}. This variant allocates all objects 
without the hash code and lock information fields in the header,
then lazily reconstructs the values when necessary. 
Specifically, if the program ever uses the hash code or lock information, 
the generated code generates the hash code or lock information
for the object, then stores this information in a hash table
that maps objects to their hash code or lock information. 

Note that, in general, this transformation (as well as field
externalization) may actually increase space usage. But in
practice, we have found that our set of benchmark programs
rarely uses these fields. The overall result is a substantial
space savings. Note that the combination of claz compression 
and hashlock elimination produces a common-case object header
size of one byte --- one byte for the claz offset and no
space at all for hash code or lock information. 

\subsection{Required Analysis Information}

\section{Analysis Algorithms}

% class hierarchy analysis & callgraph
% bitwidth analysis for field reducer.
% profiling implementation.
% definite initialization. (does zero escape?)
%  used in profiling (for non-zero N)
%  used in bitwidth. (determines whether zero is a visible value)
% constructor classifier in static spec. determines 'good' fields.
%  uses MustParamOracle, ConstMap

\subsection{Class Hierarchy Analysis and Call-Graph Construction}
We start with a static class hierarchy analysis to collect the set of
instantiated classes and callable methods.  This allows us to generate
a conservative call graph for the program, using the known receiver
type at the call-site and its instantiated subclasses in the
hierarchy.  Based on the class hierarchy, we can also tag all leaf
classes as {\tt final}, regardless of whether the source code contained
this modifier.  Methods which are not overridden, based on
the hierarchy, are also marked {\tt final}, and calls with a single
receiver method are devirtualized.  We also remove uncallable methods
and assign non-conflicting slots to interface methods using a
graph-coloring algorithm.  The results of some class casts and {\tt
  instanceof} operations can also be determined statically using
these results.

\subsection{Bitwidth Analysis}
% this allows us to reduce fields and remove unused/const fields.
We use a flow-sensitive interprocedural bitwidth analysis to
find constants values, unused and constant fields, and to reduce
field sizes where possible.  We use a dataflow framework with
Wegman and Zadeck's Sparse Conditional Constant (SCC) propagation
algorithm \cite{wegman91:scc} as a basis.  We then extend their
analysis interprocedurally, add coverage of Java language features,
and extend the value lattice to handle bitwidth.
Since almost all types in Java are signed (with the exception of the
16-bit {\tt char}), we must be able to describe bitwidths of both
negative and positive numbers, which we do by splitting the set of
values into negative, zero, and positive parts, and describing the
bitwidth of each individually.

\subsubsection{Value lattice}
First consider extending the basic three level value lattice of Wegman and
Zadeck (figure~\ref{fig:wzlat} to allow the classification of
negative, positive, or zero
values, as illustrated in figure~\ref{fig:scclat6}.
A join on two negative numbers yields the entry \code{(M--)},
indicating the set of all negative non-zero numbers; a
join on a negative and a positive number yields \code{(M-P)}, and so on.
\begin{figure}
\centering\renewcommand{\figscale}{0.6}\input{Figures/THlat1b}
\caption{Wegman and Zadeck's SCC value lattice.}
\label{fig:wzlat}
\end{figure}
\begin{figure}
\centering\renewcommand{\figscale}{0.6}\input{Figures/THlat6b}
\caption[An integer lattice for signed integers.]
{An integer lattice for signed integers. A classification into
negative (M), positive (P), or zero (Z) is grafted onto the standard
flat integer constant domain.}
\label{fig:scclat6}
\end{figure}

Now substitute integers $M$ and $P$ into the tuples, to represent the
widths of the absolute value of the negative and positive portion of
the number.  Merging the constants $2$ and $4$ would result in the lattice
value \code{(--3)},\footnote{Read this as \tuple{-,-,3} if you think
  ``double-negation'' when seeing two dashes in a row.}
for example, and merging $-4$ with $4$ would result in
\code{(3-3)}.  Some combination rules for arithmetic operations are
shown in figure~\ref{fig:bitrules}.  The rules for simple arithmetic
operators should be obvious upon examination (adding two $N$ bit
integers yields at most an $N+1$-bit integer, for example) although
care must be taken to ensure that combinations of negative and
positive integers are handled correctly.  Our implementation contains
in addition rules for common special cases, such as multiplication by
a one-bit quantity, division by constants, or bitfield operations on
two positive numbers.
\begin{figure}
\begin{eqnarray*}
-\tuple{M,P} &=& \tuple{P,M}\\
\tuple{M_l,P_l} + \tuple{M_r,P_r} &=& \tuple{1+\max(M_l,M_r),1+\max(P_l,P_r)}\\
%\tuple{M_l,P_l} \times \tuple{M_r,P_r} &=& \langle\max(M_l+P_r,P_l+M_r),\\
%                                       &&  \max(M_l+M_r,P_l+P_r)\rangle\\
\tuple{M_l,P_l} \times \tuple{M_r,P_r} &=&
\tuple{\begin{array}{l}\max(M_l+P_r,P_l+M_r),\\
                       \max(M_l+M_r,P_l+P_r)\end{array}}\\
\tuple{0,P_l} \wedge \tuple{0,P_r} &=& \tuple{0,\min(P_l,P_r)}\\
\tuple{M_l,P_l}\wedge \tuple{M_r,P_r} &=& \tuple{\max(M_l,M_r),\max(P_l,P_r)}
\end{eqnarray*}%
\caption{Some combination rules for bit-width analysis.  The \code{Z}
  element indicating whether zero is a possible value has been omitted
  for clarity.}\label{fig:bitrules}
\end{figure}

\subsubsection{Treatment of fields}
Dataflow on this bitwidth lattice is performed on the entire Java
program interprocedurally.  The analysis is what Heintze and Tardieu
\cite{heintze01}
would call {\it field-based}; that is, given a field $f$ defined in
class $X$, and an instance of $X$ named $x$, we consider an assignment
to $x.f$ to be an assignment to the field $X.f$ and ignore the base
object $x$.\footnote{An obvious extension is to use pointer
analysis to discriminate between fields allocated at different sites
in the program.}  The result of the analysis is a bitwidth
specification for each variable and field in the program.  As the
analysis is based on SCC, we also identify constant variables and
fields; reads of constant fields are replaced with their constant
value and the field is eliminated.  Fields which we do not discover
any reads of during our analysis are also removed as unused.

\subsubsection{Other details}
Our analysis handles method calls by merging the lattice values of the
method parameters at the call-site with the formal parameters of the
method.  Similarly, the return value of the method is propagated back
to all call-sites.  Our compiler's intermediate representation handles
thrown exceptions by treating the method return value as a tuple, and
the call site as a conditional branch.  The ``return value'' is
assigned and the first branch taken on a normal method return, and the
``exception value'' is assigned and the second branch taken when an
exception is thrown from the method.

Our implementation of this analysis is actually context-sensitive,
with a user-defined context length.  All results presented here were
obtained with the context set to zero.

Space does not permit us to describe the full extension of the value
lattice to handle all Java types, the class hierarchy, {\tt
  null} and {\tt String} constants, and fixed-length arrays.
We refer the interested reader to \cite{ananian99:tech} for an
exhaustive description.

\subsection{Definite Initialization Analysis}
The Java field semantics dictate that uninitialized fields must have
the value zero (or {\tt null}, for pointer fields).  It may seem,
then, that the starting lattice value for every integer field should
be {\tt 0}.  This, however, prevents us from finding non-zero field
constants in the program: a simple initialization statement like {\tt
  x=5} will assign {\tt x} the value $\mathbf{0}\meet\mathbf{5}$,
which is not equal to $\mathbf{5}$!\footnote{On the lattice of
  figure~\ref{fig:scclat6}, $\mathbf{0}\meet\mathbf{5}=\text{\tt
    (-ZP)}$.}

We perform a {\it definite initialization} analysis to remedy this
problem and restore precision to our analysis.
Figure~\ref{fig:definit-example} shows an example of a simple
constant field.
With only constructor {\tt A$_1$}, field {\tt f} will get the
lattice value {\bf 5}.  Without constructor {\tt A$_2$} in the class,
we say that field {\tt f} is {\it definitely initialized} because
every constructor of {\tt A} assigns a value to {\tt f} before
control-flow leaves it.  
Adding constructor {\tt A$_2$} allows the
default {\bf 0} value of {\tt f} to be seen by not {\it definitely
  assigning} a value to {\tt f}; {\tt f} is thus no longer
definitely-initialized.  We construct a mapping from methods to all
fields which they may read (in a flow-insensitive manner) and
transitively close over the call-graph to determine a ``safe set'' of
methods which the constructor may call before a definite
initialization of {\tt f}.  If control flow may pass to a method not
in the safe set before {\tt f} is initialized, then {\tt f} is not
definitely initialized.

Definitely-initialized fields are allowed to start at $\bot$ in the
bitwidth analysis lattice.  All other fields must start at value
{\bf 0}, which will make it impossible for the field to represent a
non-zero constant value.  The results of the definite initialization
analysis are also used to profile mostly-constant fields, as described
in the next section.

\begin{figure}
\begin{samplecode}
public class A \{\\
\>int f;\\
\>A$_1$(\ldots) \{ f = 5; \}\\
\>A$_2$(\ldots) \{\\
\>\>// no assignment to f\\
\>\}\\
\}\\
\end{samplecode}
\caption{Use example for the definite initialization analysis.
The field {\tt f} is {\it definitely initialized} if only constructor
{\tt A$_1$} is in the class; with constructor {\tt A$_2$} present it
is not.}
\label{fig:definit-example}
\end{figure}

\subsection{Profiling Mostly-Constant Fields}
To aid the static specialization and field externalization
transformations, we instrument a ``profiling build'' of the code
to determine which fields are mostly-constant.  Our implementation
builds one binary per examined constant, that is, one binary to look
for ``mostly-zero'' fields, a separate binary to look for fields which
are usually ``one'', a third binary to look for fields commonly
``two'', and so forth.  We built ten binaries, looking for
``mostly-$N$'' fields, where $N$ was in the interval $[-5,5]$.
Only the ``mostly-zero'' build was relevant for pointer fields, where
we looked for mostly-{\tt null} fields.  We could easily have built
one combined binary instead of eleven, but for simplicity the following
discussion will follow our implementation and assume there is only one
target value $N$ which we are looking for.

We add one counter per class which is incremented at allocation sites
to record the number of times each exact class type is instantiated.
We also add per-field counters which are incremented the {\tt first}
time a non-$N$ value is stored into a certain field.  By comparing the
number of times the class (thus field) is instantiated and the number
of times its value it set to a non-$N$ value, we can determine the
amount of memory we could save by applying a ``mostly-$N$''
transformation (static specialization or field externalization) to the
field.  Static specialization will split first on the field
demonstrating the largest potential savings, with the most common $N$
value seen.  Externalization will only apply if the ratio of
always-$N$-valued fields to total allocated fields exceeds its
breakeven point (see section {\bf\Large FIXME} for a discussion).

When looking for non-zero $N$ values, the default zero value for
uninitialized fields becomes a problem.  For these cases, we use the
definite-initialization analysis in the previous section to increment the
``non-$N$'' counter on any path where the field in question is not
definitely initialized.

\subsection{Finding Subclass-Final Fields}

Our static specialization transformation can only be applied to what
we call {\it subclass-final} fields.  Subclass-finality is a looser
but similar constraint to Java's {\tt final} modifier.  We do a
single-pass analysis to determine subclass-finality, using the results
from the bitwidth analysis to improve our results.  In particular we
wish to note that we do {\it not} rely on the programmer's manual
specification of the {\it final} modifier on fields.

The first generalization past Java's {\tt final} semantics is what we
will call {\it this-final} fields.  This final fields can be written
multiple times (or not at all), but all writes must occur within
constructors of the field's declaring class.
Fields marked {\tt final} in Java, by
contrast, must be written to {\it exactly once} in each constructor of
the declaring class.
As in Java, the object containing the field must match the receiver of
the constructor in which it can be written; you cannot write a final
field in another instance of the class from your instance's
constructor.

We then loosen the restriction further, and define
{\it subclass-final} fields which can be written in {\it any method of
  a subclass}, in addition to within the constructor of its declaring
class.  As before, you can only write to subclass-final fields of {\tt
  this} inside a constructor of the fields declaring field, but you
can write to the field zero or more times.

Subclass-finality matches the requirements of the static
specialization transformation.  Since we always insert a ``Big''
version of the original split class as parent to any subclasses,
subclasses can write to the split field without restriction.
We need only restrict writes which occur in the class proper.

Our analysis constructs the set of subclass-final fields by finding
its dual, the set of {\it non}-subclass-final fields.  We scan every
method and pull out all writes to fields which belong to
a {\it superclass} of the method's declaring class---writes to fields
in the method's declaring class are not removed, only fields in
superclasses.  If the method is a constructor, we additionally remove
all writes to fields of the {\tt this} parameter.  All remaining
fields are added to the set of non-subclass-final fields.

\subsection{Constructor Classification}
% includes MustParamOracle and ConstMap
As a final step to enable the static specialization transformation, we
must identify constructors which blah blah blah.

in constructor:
  merge all SET ops on fields of this: is must param?  const? (use bw)
  treat 'this' constructor as inlined.
  all other calls: we know that there are no bad writes in non-constr,
   because field is subclass-final.

if merged value at end is param or constant, then this constructor
can be specialized on that field.  we split all subclass-final fields
which are 'good' for at least one callable constructor.  order of
splitting is by magnitude of expected gain, from profiling.

\section{Implementation Issues}

% header optimizations.
% java-vs-byte-vs-bit packing.
% byte-packing strategy.
% garbage collection; dynamic methods.  conservative gc. ole aggeson's blah.
% efficiency of field virtualization.
% single-inheritance when splitting.
% efficiency of external hashtable
% distribution of mostly-constants.
% array allocation?
% pointer size improvements.
% parameter widening; reflection; interface to native code. stoplist

cp \section{Extensions} % ???
% header optimizations.
% pointer compression?

\section{Experimental Results}
\label{sec:results}

We have implemented all of the analyses and transformations described
in this paper in the MIT Flex compiler infrastructure 
(available at {\tt www.flexc.lcs.mit.edu}). We measure the effectiveness
of our optimizations by compiling the SPECjvm98 benchmarks with 
this compiler, then measuring the resulting space savings and
performance. 

\subsection{Memory Savings}

To evaluate the effectiveness of our technique at reducing the
amount of memory required to execute the program, 
we first ran an instrumented version of each
application without any space optimizations. We used this
instrumented version to compute the total amount of memory 
allocated during the entire execution of the program. 
We then ran an instrumented version of our program after optimization.
This version enabled us to calculate the amount by which each 
technique reduced the size of the allocated data. 
Figure~\ref{} presents the total space savings numbers. This
figure contains a bar for each application, with the bar broken
down into categories that indicate the percentage of memory from 
the original unoptimized execution that we were able to eliminate
with each optimization. The black section of each bar indicates the
amount of memory remaining after all optimizations. 
(Discussion here...)

We used then instrumented version to compute the amount of memory required
to store the maximum amount of data live at any point in the
execution, in both the optimized and unoptimized versions. 
Figure~\ref{} presents the space savings according to this
metric. 

\subsection{Objects Versus Arrays}

The majority of our optimizations are designed to optimize
object fields rather than arrays. We next present numbers 
that characterize the space reductions for objects only,
rather than for both objects and arrays. Figure~\ref{}
presents space savings numbers for objects alone, omitting
any storage required for arrays. 

\subsection{Execution Times} 

We next evaluate the execution time impact of applying our space
optimizations. Figure~\ref{} presents the normalized execution 
times of each application after the application of our sequence
of optimizations. These numbers show that the first several
optimizations (Claz Compression, Field Reduction, and Byte Packing)
typically reduce the execution times, while the
remainder (Static Specialization, Field Externalization, and  Hashlock
externalization) generate modest increases in the execution times. 


\begin{figure}
\includegraphics[scale=0.32,clip=true]{Figures/spaceopt.eps}
\caption{Cumulative reduction in dynamic allocation achieved with
  our transformations.}
\label{fig:total}
\end{figure}
%\begin{figure*}
%\includegraphics[scale=0.65]{Figures/spaceopt-bit.eps}
%\caption{results w/ bit alignment}
%\end{figure*}
\begin{figure}
\includegraphics[scale=0.32,clip=true]{Figures/spec-space.eps}
\caption{Total allocation in spec benchmarks.}
\label{fig:space}
\end{figure}

\begin{figure}
\includegraphics[scale=0.32,clip=true]{Figures/oopsla-speed.eps}
\caption{Runtime performance of space optimizations.}
\label{fig:space}
\end{figure}

\begin{table}
\begin{tabular}{lcccr@{.}l}
&\bf total&&&\multicolumn{2}{c}{\bf\% alloc'ed}\\
\bf Benchmark &\bf fields &\bf unread &\bf constant &
\multicolumn{2}{c}{\bf space saved} \\\hline
200\_check      & 279 &   79   &   35   &  2&6\% \\
201\_compress   & 298 &   75   &   31   &  2&5\% \\
202\_jess       & 485 &   91   &   43   &  9&9\% \\
205\_raytrace   & 341 &   75   &   30   &  0&0\% \\
209\_db         & 286 &   75   &   35   &  0&0\% \\
213\_javac      & 531 &   85   &   34   &  0&6\% \\
222\_mpegaudio  & 286 &   75   &   35   &  1&4\% \\
227\_mtrt       & 341 &   75   &   30   &  0&0\% \\
228\_jack       & 378 &   77   &   31   & 10&2\% \\
\end{tabular}
\caption{Number of unused and constant fields in SPEC benchmarks,
  and the savings realized (in \% of total dynamic allocated bytes) by
  removing them.}
\label{tab:const-unused}
\end{table}


\section{Related work}

Many researchers have focused on the problem of reducing the amount of
header space required to represent Java
locks~\cite{BKMS98,OK99,ADGKRW99}. The basic issue is that the vast
majority of programs do not use the lock associated with every object
in its full generality, so it is possible to develop improved
algorithms optimized for the common case rather than the most general
case. The basic idea is to represent the lock with the minimum amount
of state (typically a bit) required to support the common usage
pattern of a thread first acquiring, then releasing the lock in the
object and to back off to a more elaborate scheme only when the thread
exhibits a more complex pattern such as nested lock acquires. The
primary focus has been on improving performance rather than on
reducing space; however, many of the algorithms also eliminate the
need to store the complicated locking objects required to support the
most general lock usage pattern possible in a Java program. These
techniques typically reduce the lock space overhead to 24 header bits.

Research in escape analysis and related analyses can enable the
compiler to find objects whose locks are never 
acquired~\cite{ACSE99,BH99,WR99:OOPSLA99,CGSSM99,Ruf00:PLDI00,SR01:PPOPP01}.
This information can enable the compiler to remove the space
reserved for synchronization support in these objects. 
Our hashlock removal algorithm uses a totally dynamic approach
based on our field externalization mechanism. 

Several researchers have used bitwidth analysis to reduce the size
of the generated circuits for compilers that generate hardware
implementations of programs written in C or similar programming 
languages~\cite{ananian:siliconc,RR00:PLDI00,SBA00:PLDI00,BGSW00}.

Dieckmann and Hoelzle have performed an in-depth analysis of the
memory allocation behavior of Java programs~\cite{DH99}. Although 
space is not their primary focus, their study does quantify 
the space overhead associated the use of a two-word header
and of 8-byte alignment. In general, our measurements of the 
memory system behavior of Java programs broadly agree with their
measurements. 

Sweeny and Tip \cite{SweeneyTip98DeadDataMembers} did a study of dead
members of C++ programs, which is similar to the unread field
elimination done by our bitwidth analysis.  However, they
fail to identify {\it constant} members, which our SCC-based algorithm
does easily.  Further, our results show that unread and constant field
elimination is very dependent on the coding style of a particular
application.  The collection of techniques we have presented here
gives much more consistent savings over a wide range of benchmarks.

%Aggarwal and Randall \cite{aggarwal01} described a array bounds check
%removal method using {\it related fields}.  This work attempted to
%discover fields, such as {\tt Vector.size}, which are guaranteed to be
%less than or equal to the length of some array, for example, the
%backing array stored in {\tt Vector.data}.  Tests against the related
%field could then provide information about bounds checks on accesses
%to the array.  A similar technique could be used in this work to
%extend the utility of bitwidth information discovered on related fields.


\section{Conclusions}

\bibliography{harpoon}

%\appendix
%\input{pldi02-appendix}
\end{document}
